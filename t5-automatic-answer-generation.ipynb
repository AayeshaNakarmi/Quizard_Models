{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9988427,"sourceType":"datasetVersion","datasetId":6147037}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install evaluate rouge rouge_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T12:38:56.248839Z","iopub.execute_input":"2024-12-05T12:38:56.24912Z","iopub.status.idle":"2024-12-05T12:39:08.982605Z","shell.execute_reply.started":"2024-12-05T12:38:56.249091Z","shell.execute_reply":"2024-12-05T12:39:08.98163Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\n# Retrieve the Hugging Face API token from Kaggle secrets\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T12:39:08.984193Z","iopub.execute_input":"2024-12-05T12:39:08.984483Z","iopub.status.idle":"2024-12-05T12:39:09.08182Z","shell.execute_reply.started":"2024-12-05T12:39:08.984455Z","shell.execute_reply":"2024-12-05T12:39:09.081251Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch,nltk,spacy,string,transformers,json,evaluate,warnings\nimport torch.nn as nn\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom torch.optim import Adam\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler\nfrom sklearn.model_selection import train_test_split\nfrom transformers import T5Tokenizer, T5Model, T5ForConditionalGeneration, T5TokenizerFast\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom sklearn.metrics import f1_score\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T12:39:09.0826Z","iopub.execute_input":"2024-12-05T12:39:09.082812Z","iopub.status.idle":"2024-12-05T12:39:28.443775Z","shell.execute_reply.started":"2024-12-05T12:39:09.082791Z","shell.execute_reply":"2024-12-05T12:39:28.442987Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# nltk.download('punkt')          # Tokenizer models\n# nltk.download('wordnet')        # WordNet lexical database\n# nltk.download('omw-1.4')        # Open Multilingual WordNet\n# nltk.download('averaged_perceptron_tagger')  # POS tagger\n# nltk.download('stopwords')      # Common stop words\n# nltk.download('vader_lexicon')  # Sentiment analysis lexicon","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T12:39:28.446126Z","iopub.execute_input":"2024-12-05T12:39:28.447051Z","iopub.status.idle":"2024-12-05T12:39:28.450772Z","shell.execute_reply.started":"2024-12-05T12:39:28.447008Z","shell.execute_reply":"2024-12-05T12:39:28.449775Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_name = \"aayeshanakarmi/T5-QG-finetuned-squad\"\n\nTOKENIZER = T5Tokenizer.from_pretrained(model_name, use_auth_token=hf_token)\nDEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n# Load the model from the Hugging Face model hub\nMODEL = T5ForConditionalGeneration.from_pretrained(model_name, use_auth_token=hf_token, return_dict=True)\n\n# MODEL = T5ForConditionalGeneration.from_pretrained(\"t5-small\", return_dict=True)\nMODEL.to(DEVICE)\nOPTIMIZER = Adam(MODEL.parameters(), lr=0.00001)\nQ_LEN = 256   # Question Length\nT_LEN = 32    # Target Length\nBATCH_SIZE = 4\nEPOCHS = 5\nOUTPUT_DIR = '/kaggle/tmp/'\nOUTPUT_MODEL_NAME = 'AQG-finetuned-squad-lite'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T12:39:28.451889Z","iopub.execute_input":"2024-12-05T12:39:28.452238Z","iopub.status.idle":"2024-12-05T12:39:35.980834Z","shell.execute_reply.started":"2024-12-05T12:39:28.452201Z","shell.execute_reply":"2024-12-05T12:39:35.979859Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loading the data\n\nwith open('/kaggle/input/squad-20/train-v2.0.json') as f:\n    data = json.load(f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T12:39:35.981971Z","iopub.execute_input":"2024-12-05T12:39:35.98229Z","iopub.status.idle":"2024-12-05T12:39:37.369129Z","shell.execute_reply.started":"2024-12-05T12:39:35.982263Z","shell.execute_reply":"2024-12-05T12:39:37.368411Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extracting context, question, and answers from the dataset\n\ndef prepare_data(data):\n    articles = []\n    \n    for article in data[\"data\"]:\n        for paragraph in article[\"paragraphs\"]:\n            for qa in paragraph[\"qas\"]:\n                question = qa[\"question\"]\n\n                if not qa[\"is_impossible\"]:\n                    answer = qa[\"answers\"][0][\"text\"]\n                \n                inputs = {\"context\": paragraph[\"context\"], \"question\": question, \"answer\": answer}\n\n            \n                articles.append(inputs)\n\n    return articles","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T12:39:37.37017Z","iopub.execute_input":"2024-12-05T12:39:37.370433Z","iopub.status.idle":"2024-12-05T12:39:37.375886Z","shell.execute_reply.started":"2024-12-05T12:39:37.370405Z","shell.execute_reply":"2024-12-05T12:39:37.374886Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = prepare_data(data)\n\n# Create a Dataframe\ndata = pd.DataFrame(data)\ndata","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T12:39:37.377063Z","iopub.execute_input":"2024-12-05T12:39:37.377301Z","iopub.status.idle":"2024-12-05T12:39:37.776301Z","shell.execute_reply.started":"2024-12-05T12:39:37.377278Z","shell.execute_reply":"2024-12-05T12:39:37.775412Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# data = data.sample(n=1000, random_state=42)\n# data = data.reset_index(drop=True)\ndata, test_data = train_test_split(data, test_size=0.1, random_state=42)\n\ndata = data.reset_index(drop=True)\ntest_data = test_data.reset_index(drop=True)\n\nprint(f\"Data size: {len(data)}\")\nprint(f\"Testing data size: {len(test_data)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T12:39:37.777165Z","iopub.execute_input":"2024-12-05T12:39:37.777392Z","iopub.status.idle":"2024-12-05T12:39:37.812381Z","shell.execute_reply.started":"2024-12-05T12:39:37.77737Z","shell.execute_reply":"2024-12-05T12:39:37.811538Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class QA_Dataset(Dataset):\n    def __init__(self, tokenizer, dataframe, q_len, t_len):\n        self.tokenizer = tokenizer\n        self.q_len = q_len\n        self.t_len = t_len\n        self.data = dataframe\n        self.questions = self.data[\"question\"]\n        self.context = self.data[\"context\"]\n        self.answer = self.data['answer']\n        \n    def __len__(self):\n        return len(self.questions)\n    \n    def __getitem__(self, idx):\n        question = self.questions[idx]\n        context = self.context[idx]\n        answer = self.answer[idx]\n        \n        # Tokenizing the question and context pair with truncation\n        question_tokenized = self.tokenizer(question, context, max_length=self.q_len, \n                                            padding=\"max_length\", truncation=True, \n                                            add_special_tokens=True)\n        \n        # Tokenizing the answer with truncation\n        answer_tokenized = self.tokenizer(answer, max_length=self.t_len, \n                                          padding=\"max_length\", truncation=True, \n                                          add_special_tokens=True)\n        \n        # Preparing the labels\n        labels = torch.tensor(answer_tokenized[\"input_ids\"], dtype=torch.long)\n        labels[labels == 0] = -100  # Masking padding tokens in the labels\n        \n        return {\n            \"input_ids\": torch.tensor(question_tokenized[\"input_ids\"], dtype=torch.long),\n            \"attention_mask\": torch.tensor(question_tokenized[\"attention_mask\"], dtype=torch.long),\n            \"labels\": labels,\n            \"decoder_attention_mask\": torch.tensor(answer_tokenized[\"attention_mask\"], dtype=torch.long)\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T12:39:37.814553Z","iopub.execute_input":"2024-12-05T12:39:37.814843Z","iopub.status.idle":"2024-12-05T12:39:37.822234Z","shell.execute_reply.started":"2024-12-05T12:39:37.814818Z","shell.execute_reply":"2024-12-05T12:39:37.821353Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dataloader\ntrain_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n\ntrain_sampler = RandomSampler(train_data.index)\nval_sampler = RandomSampler(val_data.index)\n\nqa_dataset = QA_Dataset(TOKENIZER, data, Q_LEN, T_LEN)\n\ntrain_loader = DataLoader(qa_dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\nval_loader = DataLoader(qa_dataset, batch_size=BATCH_SIZE, sampler=val_sampler)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T12:39:37.823052Z","iopub.execute_input":"2024-12-05T12:39:37.823275Z","iopub.status.idle":"2024-12-05T12:39:37.850207Z","shell.execute_reply.started":"2024-12-05T12:39:37.823253Z","shell.execute_reply":"2024-12-05T12:39:37.849553Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(train_loader),len(val_loader))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T12:39:37.851185Z","iopub.execute_input":"2024-12-05T12:39:37.851497Z","iopub.status.idle":"2024-12-05T12:39:37.855934Z","shell.execute_reply.started":"2024-12-05T12:39:37.85146Z","shell.execute_reply":"2024-12-05T12:39:37.855077Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import logging\nlogging.disable(logging.WARNING)\n\n# Lists to store loss values for each epoch\ntrain_losses = []\nval_losses = []\n\nfor epoch in range(EPOCHS):\n    # Training phase\n    MODEL.train()\n    train_loss = 0\n    train_batch_count = 0\n    \n    # Loop through training data\n    for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{EPOCHS}\"):\n        input_ids = batch[\"input_ids\"].to(DEVICE)\n        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n        labels = batch[\"labels\"].to(DEVICE)\n        decoder_attention_mask = batch[\"decoder_attention_mask\"].to(DEVICE)\n\n        # Forward pass\n        outputs = MODEL(\n                          input_ids=input_ids,\n                          attention_mask=attention_mask,\n                          labels=labels,\n                          decoder_attention_mask=decoder_attention_mask\n                        )\n\n        # Backpropagation\n        OPTIMIZER.zero_grad()\n        outputs.loss.backward()\n        OPTIMIZER.step()\n\n        # Accumulate training loss\n        train_loss += outputs.loss.item()\n        train_batch_count += 1\n\n    # Compute average training loss for the epoch\n    avg_train_loss = train_loss / train_batch_count\n    train_losses.append(avg_train_loss)\n\n    # Validation phase\n    MODEL.eval()\n    val_loss = 0\n    val_batch_count = 0\n\n    with torch.no_grad():  # Disable gradient calculation during validation\n        for batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}/{EPOCHS}\"):\n            input_ids = batch[\"input_ids\"].to(DEVICE)\n            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n            labels = batch[\"labels\"].to(DEVICE)\n            decoder_attention_mask = batch[\"decoder_attention_mask\"].to(DEVICE)\n\n            # Forward pass\n            outputs = MODEL(\n                              input_ids=input_ids,\n                              attention_mask=attention_mask,\n                              labels=labels,\n                              decoder_attention_mask=decoder_attention_mask\n                            )\n\n            # Accumulate validation loss\n            val_loss += outputs.loss.item()\n            val_batch_count += 1\n\n    # Compute average validation loss for the epoch\n    avg_val_loss = val_loss / val_batch_count\n    val_losses.append(avg_val_loss)\n\n    # Print out losses after each epoch (not each batch)\n    print(f\"Epoch {epoch+1}/{EPOCHS} -> Train loss: {avg_train_loss:.4f}\\tValidation loss: {avg_val_loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T12:39:37.856904Z","iopub.execute_input":"2024-12-05T12:39:37.857132Z","iopub.status.idle":"2024-12-05T15:50:47.771769Z","shell.execute_reply.started":"2024-12-05T12:39:37.857109Z","shell.execute_reply":"2024-12-05T15:50:47.770941Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plotting the loss\nplt.figure(figsize=(10, 5))\nplt.plot(range(1, EPOCHS+1), train_losses, marker='o', label='Train Loss')\nplt.plot(range(1, EPOCHS+1), val_losses, marker='o', label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss Over Epochs')\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T15:50:47.772783Z","iopub.execute_input":"2024-12-05T15:50:47.772969Z","iopub.status.idle":"2024-12-05T15:50:48.077513Z","shell.execute_reply.started":"2024-12-05T15:50:47.772944Z","shell.execute_reply":"2024-12-05T15:50:48.076821Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MODEL.save_pretrained(f'{OUTPUT_DIR}{OUTPUT_MODEL_NAME}')\nTOKENIZER.save_pretrained(f'{OUTPUT_DIR}{OUTPUT_MODEL_NAME}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T15:50:48.07901Z","iopub.execute_input":"2024-12-05T15:50:48.07926Z","iopub.status.idle":"2024-12-05T15:50:49.017915Z","shell.execute_reply.started":"2024-12-05T15:50:48.079221Z","shell.execute_reply":"2024-12-05T15:50:49.017109Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T16:19:50.718094Z","iopub.execute_input":"2024-12-05T16:19:50.718673Z","iopub.status.idle":"2024-12-05T16:19:50.835618Z","shell.execute_reply.started":"2024-12-05T16:19:50.718641Z","shell.execute_reply":"2024-12-05T16:19:50.834922Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import login\n\n# Replace 'your_token' with your actual Hugging Face token\nlogin(token=hf_token)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T16:21:40.773014Z","iopub.execute_input":"2024-12-05T16:21:40.773258Z","iopub.status.idle":"2024-12-05T16:21:40.829895Z","shell.execute_reply.started":"2024-12-05T16:21:40.77323Z","shell.execute_reply":"2024-12-05T16:21:40.829131Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the fine-tuned model and tokenizer locally\nMODEL.save_pretrained(\"T5-QuestionAnswering-squad-10\")\nTOKENIZER.save_pretrained(\"T5-QuestionAnswering-squad-10\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T16:21:43.17Z","iopub.execute_input":"2024-12-05T16:21:43.170252Z","iopub.status.idle":"2024-12-05T16:21:43.936252Z","shell.execute_reply.started":"2024-12-05T16:21:43.170221Z","shell.execute_reply":"2024-12-05T16:21:43.93535Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Replace 'your_token' with your actual Hugging Face token\nMODEL.push_to_hub(\"T5-QuestionAnswering-squad-10\", use_auth_token=hf_token, use_temp_dir=False)\nTOKENIZER.push_to_hub(\"T5-QuestionAnswering-squad-10\", use_auth_token=hf_token, use_temp_dir=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T16:22:10.568369Z","iopub.execute_input":"2024-12-05T16:22:10.568928Z","iopub.status.idle":"2024-12-05T16:22:21.684707Z","shell.execute_reply.started":"2024-12-05T16:22:10.568892Z","shell.execute_reply":"2024-12-05T16:22:21.683896Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom torch.optim import Adam\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler\nfrom sklearn.model_selection import train_test_split\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nfrom torch.optim.lr_scheduler import StepLR\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Model Configuration\nmodel_name = \"t5-small\"  # You can change this to any T5 variant\nTOKENIZER = T5Tokenizer.from_pretrained(model_name)\nDEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nMODEL = T5ForConditionalGeneration.from_pretrained(model_name, return_dict=True)\nMODEL.to(DEVICE)\n\n# Training Parameters\nQ_LEN = 512  # Increased for context + question\nT_LEN = 128  # Increased for potential longer answers\nBATCH_SIZE = 4\nEPOCHS = 5\nLEARNING_RATE = 1e-4\nOUTPUT_DIR = './models/'\nOUTPUT_MODEL_NAME = 'T5-answer-generation'\n\n# Data Preparation\ndef prepare_data(csv_path):\n    df = pd.read_csv(csv_path, encoding='latin-1')  # Adjust encoding as needed\n    df['input_text'] = 'Instruction: Generate a detailed answer to the question using the provided context. Provide the answer only without including any additional content. ' + df['Question'] + ' [SEP] ' + df['Context']\n    df['target_text'] = df['Answer']\n    return df[['input_text', 'target_text']]\n\n# Custom Dataset class\nclass AnswerGenerationDataset(Dataset):\n    def __init__(self, tokenizer, dataframe, q_len, t_len):\n        self.tokenizer = tokenizer\n        self.q_len = q_len\n        self.t_len = t_len\n        self.inputs = dataframe['input_text'].values\n        self.targets = dataframe['target_text'].values\n    \n    def __len__(self):\n        return len(self.inputs)\n    \n    def __getitem__(self, idx):\n        input_text = str(self.inputs[idx])\n        target_text = str(self.targets[idx])\n        \n        # Tokenize inputs\n        input_encodings = self.tokenizer(input_text, \n                                       max_length=self.q_len,\n                                       padding='max_length',\n                                       truncation=True,\n                                       return_tensors=\"pt\")\n        \n        # Tokenize targets\n        target_encodings = self.tokenizer(target_text,\n                                        max_length=self.t_len,\n                                        padding='max_length',\n                                        truncation=True,\n                                        return_tensors=\"pt\")\n        \n        # Prepare labels by masking padding tokens\n        labels = target_encodings['input_ids'].clone()\n        labels[labels == self.tokenizer.pad_token_id] = -100\n        \n        return {\n            'input_ids': input_encodings['input_ids'].squeeze(),\n            'attention_mask': input_encodings['attention_mask'].squeeze(),\n            'labels': labels.squeeze(),\n            'decoder_attention_mask': target_encodings['attention_mask'].squeeze()\n        }\n\n# Training Loop with Dynamic Learning Rate Adjustment\ndef train_model(train_loader, val_loader, model, optimizer, scheduler, num_epochs):\n    train_losses = []\n    val_losses = []\n    \n    for epoch in range(num_epochs):\n        # Training phase\n        model.train()\n        train_loss = 0\n        train_batch_count = 0\n        \n        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n            input_ids = batch['input_ids'].to(DEVICE)\n            attention_mask = batch['attention_mask'].to(DEVICE)\n            labels = batch['labels'].to(DEVICE)\n            decoder_attention_mask = batch['decoder_attention_mask'].to(DEVICE)\n            \n            outputs = model(input_ids=input_ids,\n                          attention_mask=attention_mask,\n                          labels=labels,\n                          decoder_attention_mask=decoder_attention_mask)\n            \n            loss = outputs.loss\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n            train_batch_count += 1\n        \n        avg_train_loss = train_loss / train_batch_count\n        train_losses.append(avg_train_loss)\n        \n        # Validation phase\n        model.eval()\n        val_loss = 0\n        val_batch_count = 0\n        \n        with torch.no_grad():\n            for batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}/{num_epochs}\"):\n                input_ids = batch['input_ids'].to(DEVICE)\n                attention_mask = batch['attention_mask'].to(DEVICE)\n                labels = batch['labels'].to(DEVICE)\n                decoder_attention_mask = batch['decoder_attention_mask'].to(DEVICE)\n                \n                outputs = model(input_ids=input_ids,\n                              attention_mask=attention_mask,\n                              labels=labels,\n                              decoder_attention_mask=decoder_attention_mask)\n                \n                val_loss += outputs.loss.item()\n                val_batch_count += 1\n        \n        avg_val_loss = val_loss / val_batch_count\n        val_losses.append(avg_val_loss)\n        \n        # Step scheduler at the end of each epoch\n        scheduler.step()\n        \n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        print(f\"Average training loss: {avg_train_loss:.4f}\")\n        print(f\"Average validation loss: {avg_val_loss:.4f}\")\n        \n    return train_losses, val_losses\n\n# Answer Generation Function\ndef generate_answer(model, tokenizer, question, context, max_length=128):\n    input_text = f\"generate answer: {question} [SEP] {context}\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n    inputs = inputs.to(DEVICE)\n    \n    outputs = model.generate(\n        input_ids=inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"],\n        max_length=max_length,\n        num_beams=4,\n        length_penalty=2.0,\n        early_stopping=True\n    )\n    \n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Main Function\ndef main():\n    # Load and prepare data\n    data = prepare_data('/kaggle/input/quizard-dataset-3000/Quizard_custom_dataset.csv')\n    \n    # Split data\n    train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n    \n    # Create datasets\n    train_dataset = AnswerGenerationDataset(TOKENIZER, train_data, Q_LEN, T_LEN)\n    val_dataset = AnswerGenerationDataset(TOKENIZER, val_data, Q_LEN, T_LEN)\n    \n    # Create dataloaders\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    # Initialize optimizer and scheduler\n    optimizer = Adam(MODEL.parameters(), lr=LEARNING_RATE)\n    scheduler = StepLR(optimizer, step_size=1, gamma=0.9)  # StepLR scheduler for dynamic LR adjustment\n    \n    # Train the model\n    train_losses, val_losses = train_model(train_loader, val_loader, MODEL, optimizer, scheduler, EPOCHS)\n    \n    # Plot training results\n    plt.figure(figsize=(10, 5))\n    plt.plot(range(1, EPOCHS+1), train_losses, marker='o', label='Train Loss')\n    plt.plot(range(1, EPOCHS+1), val_losses, marker='o', label='Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Training and Validation Loss Over Epochs')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n    \n    # Save the model\n    MODEL.save_pretrained(f'{OUTPUT_DIR}{OUTPUT_MODEL_NAME}')\n    TOKENIZER.save_pretrained(f'{OUTPUT_DIR}{OUTPUT_MODEL_NAME}')\n    \n    # Test the model with a sample\n    sample_question = \"What happens during photosynthesis?\"\n    sample_context = \"The process of photosynthesis occurs in the chloroplasts of plant cells. During photosynthesis, light energy is converted into chemical energy, stored as glucose, and oxygen is released.\"\n    \n    generated_answer = generate_answer(MODEL, TOKENIZER, sample_question, sample_context)\n    print(\"\\nSample Generation:\")\n    print(f\"Question: {sample_question}\")\n    print(f\"Context: {sample_context}\")\n    print(f\"Generated Answer: {generated_answer}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T05:16:45.924473Z","iopub.execute_input":"2025-02-08T05:16:45.924840Z","iopub.status.idle":"2025-02-08T05:27:26.149425Z","shell.execute_reply.started":"2025-02-08T05:16:45.924800Z","shell.execute_reply":"2025-02-08T05:27:26.148430Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T05:32:20.653048Z","iopub.execute_input":"2025-02-08T05:32:20.653766Z","iopub.status.idle":"2025-02-08T05:32:20.750750Z","shell.execute_reply.started":"2025-02-08T05:32:20.653731Z","shell.execute_reply":"2025-02-08T05:32:20.750142Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the fine-tuned model and tokenizer locally\nMODEL.save_pretrained(\"T5-AnswerGeneration-Quizard-5\")\nTOKENIZER.save_pretrained(\"T5-AnswerGeneration-Quizard-5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T05:32:32.964293Z","iopub.execute_input":"2025-02-08T05:32:32.964930Z","iopub.status.idle":"2025-02-08T05:32:33.529519Z","shell.execute_reply.started":"2025-02-08T05:32:32.964899Z","shell.execute_reply":"2025-02-08T05:32:33.528679Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Replace 'your_token' with your actual Hugging Face token\nMODEL.push_to_hub(\"T5-AnswerGeneration-Quizard-5\", use_auth_token=hf_token, use_temp_dir=False)\nTOKENIZER.push_to_hub(\"T5-AnswerGeneration-Quizard-5\", use_auth_token=hf_token, use_temp_dir=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T05:32:36.250572Z","iopub.execute_input":"2025-02-08T05:32:36.251429Z","iopub.status.idle":"2025-02-08T05:32:47.392135Z","shell.execute_reply.started":"2025-02-08T05:32:36.251393Z","shell.execute_reply":"2025-02-08T05:32:47.391293Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#     Context=\"The Great Barrier Reef is the world's largest coral reef system, located in the Coral Sea, off the coast of Queensland, Australia. It is composed of over 2,900 individual reefs and 900 islands stretching over 2,300 kilometers. The reef is known for its biodiversity, hosting countless marine species, and is a popular destination for snorkeling and diving enthusiasts. However, it faces threats from climate change, overfishing, and pollution.\",\n#     Question=\"What are some of the major threats faced by the Great Barrier Reef?\",\n#     Answer=\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T05:59:21.470443Z","iopub.execute_input":"2025-02-08T05:59:21.471285Z","iopub.status.idle":"2025-02-08T05:59:21.558994Z","shell.execute_reply.started":"2025-02-08T05:59:21.471251Z","shell.execute_reply":"2025-02-08T05:59:21.558361Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import login\n\n# Replace 'your_token' with your actual Hugging Face token\nlogin(token=hf_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T05:59:23.598170Z","iopub.execute_input":"2025-02-08T05:59:23.598858Z","iopub.status.idle":"2025-02-08T05:59:23.690550Z","shell.execute_reply.started":"2025-02-08T05:59:23.598826Z","shell.execute_reply":"2025-02-08T05:59:23.689699Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration\nimport torch\n\n# Load the model and tokenizer from Hugging Face Hub\nmodel_name = \"aayeshanakarmi/T5-AnswerGeneration-Quizard-5\"  # Replace with your repo name\ntokenizer = T5Tokenizer.from_pretrained(model_name, use_auth_token=True)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name, use_auth_token=True)\n\n# Move model to GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Function for generating the answer\ndef generate_answer(question, context, max_length=128):\n    input_text = f\"generate answer: {question} [SEP] {context}\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True, padding=\"max_length\")\n    inputs = inputs.to(device)\n    \n    outputs = model.generate(\n        input_ids=inputs[\"input_ids\"],\n        attention_mask=inputs[\"attention_mask\"],\n        max_length=max_length,\n        num_beams=4,\n        length_penalty=2.0,\n        early_stopping=True\n    )\n    \n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Test with an example\nsample_question = \"What are some of the major threats faced by the Great Barrier Reef?\"\nsample_context = \"The Great Barrier Reef is the world's largest coral reef system, located in the Coral Sea, off the coast of Queensland, Australia. It is composed of over 2,900 individual reefs and 900 islands stretching over 2,300 kilometers. The reef is known for its biodiversity, hosting countless marine species, and is a popular destination for snorkeling and diving enthusiasts. However, it faces threats from climate change, overfishing, and pollution.\"\n\ngenerated_answer = generate_answer(sample_question, sample_context)\nprint(\"Generated Answer:\", generated_answer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T05:59:27.007955Z","iopub.execute_input":"2025-02-08T05:59:27.008321Z","iopub.status.idle":"2025-02-08T05:59:34.904173Z","shell.execute_reply.started":"2025-02-08T05:59:27.008288Z","shell.execute_reply":"2025-02-08T05:59:34.903296Z"}},"outputs":[],"execution_count":null}]}