{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import torch\n# from datasets import load_dataset, Dataset\n# from transformers import (\n#     T5ForConditionalGeneration,\n#     T5Tokenizer,\n#     Trainer,\n#     TrainingArguments,\n#     DataCollatorForSeq2Seq\n# )\n# import numpy as np\n# from sklearn.model_selection import train_test_split\n\n# # 1. Load dataset\n# dataset = load_dataset(\"zjsd/RedStone-QA-mcq\")\n\n# # 2. Define preprocessing functions\n# def preprocess_for_answer_generation(examples):\n#     \"\"\"Preprocess data for training the model to generate the correct answer letter.\"\"\"\n#     inputs = []\n#     for text, question in zip(examples[\"text\"], examples[\"question\"]):\n#         combined = f\"generate answer: {text} question: {question}\"\n#         inputs.append(combined)\n    \n#     # Extract just the letter from \"Answer:X\" format\n#     labels = [answer.replace(\"Answer:\", \"\") for answer in examples[\"answer\"]]\n    \n#     return {\n#         \"input\": inputs,\n#         \"output\": labels\n#     }\n\n# def preprocess_for_distractor_generation(examples):\n#     \"\"\"Preprocess data for training the model to generate distractors.\"\"\"\n#     inputs = []\n#     outputs = []\n    \n#     for text, question, answer, choices in zip(\n#         examples[\"text\"], \n#         examples[\"question\"], \n#         examples[\"answer\"], \n#         examples[\"choices\"]\n#     ):\n#         # Extract the letter from \"Answer:X\"\n#         answer_letter = answer.replace(\"Answer:\", \"\")\n        \n#         # Find the index of the correct answer\n#         answer_idx = ord(answer_letter) - ord('A')\n        \n#         # Get the correct answer text\n#         if 0 <= answer_idx < len(choices):\n#             correct_answer_text = choices[answer_idx].split('. ', 1)[1] if '. ' in choices[answer_idx] else choices[answer_idx]\n            \n#             # Create input for distractor generation\n#             input_text = f\"generate distractors: {text} question: {question} correct answer: {correct_answer_text}\"\n            \n#             # Combine all distractors as output\n#             distractors = [choice.split('. ', 1)[1] if '. ' in choice else choice \n#                           for i, choice in enumerate(choices) if i != answer_idx]\n#             output_text = \" [SEP] \".join(distractors)\n            \n#             inputs.append(input_text)\n#             outputs.append(output_text)\n    \n#     return {\n#         \"input\": inputs,\n#         \"output\": outputs\n#     }\n\n# # 3. Process datasets for both tasks\n# answer_dataset = dataset[\"train\"].map(\n#     preprocess_for_answer_generation,\n#     batched=True,\n#     remove_columns=dataset[\"train\"].column_names\n# )\n\n# distractor_dataset = dataset[\"train\"].map(\n#     preprocess_for_distractor_generation,\n#     batched=True,\n#     remove_columns=dataset[\"train\"].column_names\n# )\n\n# # 4. Split datasets into train and validation\n# # Convert to pandas DataFrame for easier splitting\n# answer_df = answer_dataset.to_pandas()\n# distractor_df = distractor_dataset.to_pandas()\n\n# # Split using pandas DataFrame\n# answer_train_df, answer_val_df = train_test_split(answer_df, test_size=0.1, random_state=42)\n# distractor_train_df, distractor_val_df = train_test_split(distractor_df, test_size=0.1, random_state=42)\n\n# # Convert back to HuggingFace datasets\n# answer_train_dataset = Dataset.from_pandas(answer_train_df)\n# answer_val_dataset = Dataset.from_pandas(answer_val_df)\n# distractor_train_dataset = Dataset.from_pandas(distractor_train_df)\n# distractor_val_dataset = Dataset.from_pandas(distractor_val_df)\n\n# # 5. Load tokenizer and model\n# model_name = \"google/flan-t5-small\"  # You can use other sizes like small, large, xl\n# tokenizer = T5Tokenizer.from_pretrained(model_name)\n# model = T5ForConditionalGeneration.from_pretrained(model_name)\n\n# # 6. Define tokenization function\n# def tokenize_function(examples):\n#     model_inputs = tokenizer(\n#         examples[\"input\"],\n#         max_length=512,\n#         padding=\"max_length\",\n#         truncation=True\n#     )\n    \n#     # Tokenize targets\n#     with tokenizer.as_target_tokenizer():\n#         labels = tokenizer(\n#             examples[\"output\"],\n#             max_length=128,\n#             padding=\"max_length\",\n#             truncation=True\n#         )\n    \n#     model_inputs[\"labels\"] = labels[\"input_ids\"]\n#     return model_inputs\n\n# # 7. Tokenize datasets\n# answer_train_tokenized = answer_train_dataset.map(\n#     tokenize_function, \n#     batched=True,\n#     remove_columns=[\"input\", \"output\"]\n# )\n# answer_val_tokenized = answer_val_dataset.map(\n#     tokenize_function, \n#     batched=True,\n#     remove_columns=[\"input\", \"output\"]\n# )\n\n# distractor_train_tokenized = distractor_train_dataset.map(\n#     tokenize_function, \n#     batched=True,\n#     remove_columns=[\"input\", \"output\"]\n# )\n# distractor_val_tokenized = distractor_val_dataset.map(\n#     tokenize_function, \n#     batched=True,\n#     remove_columns=[\"input\", \"output\"]\n# )\n\n# # # 8. Define training arguments for answer generation\n# # answer_training_args = TrainingArguments(\n# #     output_dir=\"./results/answer_generation\",\n# #     evaluation_strategy=\"epoch\",\n# #     learning_rate=5e-5,\n# #     per_device_train_batch_size=8,\n# #     per_device_eval_batch_size=8,\n# #     num_train_epochs=3,\n# #     weight_decay=0.01,\n# #     save_total_limit=3,\n# #     load_best_model_at_end=True,\n# # )\n\n\n\n# answer_training_args = TrainingArguments(\n#     output_dir=\"./results/answer_generation\",\n#     evaluation_strategy=\"epoch\",  # Make sure this matches save_strategy\n#     save_strategy=\"epoch\",  # Change this to \"epoch\"\n#     save_total_limit=2,\n#     learning_rate=2e-5,\n#     per_device_train_batch_size=4,\n#     per_device_eval_batch_size=4,\n#     num_train_epochs=3,\n#     weight_decay=0.01,\n#     load_best_model_at_end=True,  # Keep this since you want the best model at the end\n#     metric_for_best_model=\"eval_loss\",  # Specify a metric to track\n#     greater_is_better=False,  # Set False since lower loss is better\n# )\n\n\n\n# # # 9. Define training arguments for distractor generation\n# # distractor_training_args = TrainingArguments(\n# #     output_dir=\"./results/distractor_generation\",\n# #     evaluation_strategy=\"epoch\",\n# #     learning_rate=5e-5,\n# #     per_device_train_batch_size=8,\n# #     per_device_eval_batch_size=8,\n# #     num_train_epochs=3,\n# #     weight_decay=0.01,\n# #     save_total_limit=3,\n# #     load_best_model_at_end=True,\n# # )\n# distractor_training_args = TrainingArguments(\n#     output_dir=\"./results/distractor_generation\",\n#     evaluation_strategy=\"epoch\",  # Ensure this matches save_strategy\n#     save_strategy=\"epoch\",  # Change this to \"epoch\"\n#     learning_rate=5e-5,\n#     per_device_train_batch_size=8,\n#     per_device_eval_batch_size=8,\n#     num_train_epochs=3,\n#     weight_decay=0.01,\n#     save_total_limit=3,\n#     load_best_model_at_end=True,  # Keep this since we want the best model at the end\n#     metric_for_best_model=\"eval_loss\",  # Track validation loss\n#     greater_is_better=False,  # Lower eval loss is better\n# )\n\n# # 10. Define data collator\n# data_collator = DataCollatorForSeq2Seq(\n#     tokenizer=tokenizer,\n#     model=model\n# )\n\n# # 11. Train answer generation model\n# answer_trainer = Trainer(\n#     model=model,\n#     args=answer_training_args,\n#     train_dataset=answer_train_tokenized,\n#     eval_dataset=answer_val_tokenized,\n#     tokenizer=tokenizer,\n#     data_collator=data_collator,\n# )\n\n# print(\"Training answer generation model...\")\n# answer_trainer.train()\n# answer_trainer.save_model(\"./results/answer_generation/final_model\")\n\n# # 12. Load a fresh model for distractor generation\n# distractor_model = T5ForConditionalGeneration.from_pretrained(model_name)\n\n# # 13. Train distractor generation model\n# distractor_trainer = Trainer(\n#     model=distractor_model,\n#     args=distractor_training_args,\n#     train_dataset=distractor_train_tokenized,\n#     eval_dataset=distractor_val_tokenized,\n#     tokenizer=tokenizer,\n#     data_collator=data_collator,\n# )\n\n# print(\"Training distractor generation model...\")\n# distractor_trainer.train()\n# distractor_trainer.save_model(\"./results/distractor_generation/final_model\")\n\n# # 14. Inference function for generating MCQs\n# def generate_complete_mcq(text, question, answer_model, distractor_model, tokenizer):\n#     # Make sure models are in evaluation mode\n#     answer_model.eval()\n#     distractor_model.eval()\n    \n#     # Get the device\n#     device = next(answer_model.parameters()).device\n    \n#     # Step 1: Generate the answer letter\n#     answer_input = f\"generate answer: {text} question: {question}\"\n#     answer_input_ids = tokenizer(answer_input, return_tensors=\"pt\").input_ids.to(device)\n#     answer_outputs = answer_model.generate(answer_input_ids, max_length=10)\n#     answer_letter = tokenizer.decode(answer_outputs[0], skip_special_tokens=True)\n    \n#     # We need the correct answer text for the next step, but in real inference we wouldn't have it\n#     # You would need an additional model to generate the correct answer content\n#     # For demonstration, let's assume we already have it\n#     correct_answer_text = \"This is the correct answer content\"  # Placeholder\n    \n#     # Step 2: Generate distractors\n#     distractor_input = f\"generate distractors: {text} question: {question} correct answer: {correct_answer_text}\"\n#     distractor_input_ids = tokenizer(distractor_input, return_tensors=\"pt\").input_ids.to(device)\n#     distractor_outputs = distractor_model.generate(distractor_input_ids, max_length=256)\n#     distractors_text = tokenizer.decode(distractor_outputs[0], skip_special_tokens=True)\n    \n#     # Split distractors\n#     distractors = distractors_text.split(\" [SEP] \")\n    \n#     # Format as MCQ\n#     choices = []\n#     answer_idx = ord(answer_letter) - ord('A')\n    \n#     for i in range(len(distractors) + 1):\n#         if i == answer_idx:\n#             choices.append(f\"{chr(65 + i)}. {correct_answer_text}\")\n#         else:\n#             distractor_idx = i if i < answer_idx else i - 1\n#             if distractor_idx < len(distractors):\n#                 choices.append(f\"{chr(65 + i)}. {distractors[distractor_idx]}\")\n    \n#     return {\n#         \"question\": question,\n#         \"answer\": f\"Answer:{answer_letter}\",\n#         \"choices\": choices\n#     }\n\n# print(\"Training complete! You can now use the trained models to generate MCQs.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-25T11:47:29.336090Z","iopub.execute_input":"2025-02-25T11:47:29.336384Z","execution_failed":"2025-02-25T12:32:46.379Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1495442 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc2927505f424f6394eda4378a6ac664"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/166161 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f928c6526c34238ba0bc9109622a098"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1495442 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad31579bf71a4322a8173482e12a179c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/166161 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9914713619d0488d93e268368970f2d5"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-2-debb5b6f5e79>:212: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  answer_trainer = Trainer(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"name":"stdout","text":"Training answer generation model...\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    "},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset, Dataset\nfrom transformers import (\n    T5ForConditionalGeneration,\n    T5Tokenizer,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForSeq2Seq\n)\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.auto import tqdm\nimport re\nfrom collections import Counter\n\n# 1. Load dataset\nprint(\"Loading dataset...\")\ndataset = load_dataset(\"zjsd/RedStone-QA-mcq\")\nprint(f\"Dataset loaded with {len(dataset['train'])} examples\")\n\n# 2. Define preprocessing functions\ndef preprocess_for_answer_generation(examples):\n    \"\"\"Preprocess data for training the model to generate the correct answer letter.\"\"\"\n    inputs = []\n    for text, question in zip(examples[\"text\"], examples[\"question\"]):\n        combined = f\"generate answer: {text} question: {question}\"\n        inputs.append(combined)\n    \n    # Extract just the letter from \"Answer:X\" format\n    labels = [answer.replace(\"Answer:\", \"\").strip() for answer in examples[\"answer\"]]\n    \n    return {\n        \"input\": inputs,\n        \"output\": labels\n    }\n\ndef preprocess_for_distractor_generation(examples):\n    \"\"\"Preprocess data for training the model to generate distractors.\"\"\"\n    inputs = []\n    outputs = []\n    \n    for text, question, answer, choices in zip(\n        examples[\"text\"], \n        examples[\"question\"], \n        examples[\"answer\"], \n        examples[\"choices\"]\n    ):\n        # Extract the letter from \"Answer:X\"\n        answer_letter = answer.replace(\"Answer:\", \"\").strip()\n        \n        # Find the index of the correct answer\n        answer_idx = ord(answer_letter) - ord('A')\n        \n        # Get the correct answer text\n        if 0 <= answer_idx < len(choices):\n            correct_answer_text = choices[answer_idx].split('. ', 1)[1] if '. ' in choices[answer_idx] else choices[answer_idx]\n            \n            # Create input for distractor generation\n            input_text = f\"generate distractors: {text} question: {question} correct answer: {correct_answer_text}\"\n            \n            # Combine all distractors as output\n            distractors = [choice.split('. ', 1)[1] if '. ' in choice else choice \n                          for i, choice in enumerate(choices) if i != answer_idx]\n            output_text = \" [SEP] \".join(distractors)\n            \n            inputs.append(input_text)\n            outputs.append(output_text)\n    \n    return {\n        \"input\": inputs,\n        \"output\": outputs\n    }\n\n# 3. Process datasets for both tasks\nprint(\"Preprocessing data for answer generation...\")\nanswer_dataset = dataset[\"train\"].map(\n    preprocess_for_answer_generation,\n    batched=True,\n    remove_columns=dataset[\"train\"].column_names\n)\n\nprint(\"Preprocessing data for distractor generation...\")\ndistractor_dataset = dataset[\"train\"].map(\n    preprocess_for_distractor_generation,\n    batched=True,\n    remove_columns=dataset[\"train\"].column_names\n)\n\n# 4. Split datasets into train and validation\nprint(\"Splitting datasets into train and validation sets...\")\n# Convert to pandas DataFrame for easier splitting\nanswer_df = answer_dataset.to_pandas()\ndistractor_df = distractor_dataset.to_pandas()\n\n# Split using pandas DataFrame\nanswer_train_df, answer_val_df = train_test_split(answer_df, test_size=0.1, random_state=42)\ndistractor_train_df, distractor_val_df = train_test_split(distractor_df, test_size=0.1, random_state=42)\n\nprint(f\"Answer generation: {len(answer_train_df)} training examples, {len(answer_val_df)} validation examples\")\nprint(f\"Distractor generation: {len(distractor_train_df)} training examples, {len(distractor_val_df)} validation examples\")\n\n# Convert back to HuggingFace datasets\nanswer_train_dataset = Dataset.from_pandas(answer_train_df)\nanswer_val_dataset = Dataset.from_pandas(answer_val_df)\ndistractor_train_dataset = Dataset.from_pandas(distractor_train_df)\ndistractor_val_dataset = Dataset.from_pandas(distractor_val_df)\n\n# 5. Load tokenizer and model\nprint(\"Loading tokenizer and model...\")\nmodel_name = \"google/flan-t5-small\"  # You can use other sizes like small, large, xl\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\n\n# 6. Define tokenization function\ndef tokenize_function(examples):\n    model_inputs = tokenizer(\n        examples[\"input\"],\n        max_length=512,\n        padding=\"max_length\",\n        truncation=True\n    )\n    \n    # Tokenize targets\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            examples[\"output\"],\n            max_length=128,\n            padding=\"max_length\",\n            truncation=True\n        )\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# 7. Tokenize datasets\nprint(\"Tokenizing datasets...\")\nanswer_train_tokenized = answer_train_dataset.map(\n    tokenize_function, \n    batched=True,\n    remove_columns=[\"input\", \"output\"]\n)\nanswer_val_tokenized = answer_val_dataset.map(\n    tokenize_function, \n    batched=True,\n    remove_columns=[\"input\", \"output\"]\n)\n\ndistractor_train_tokenized = distractor_train_dataset.map(\n    tokenize_function, \n    batched=True,\n    remove_columns=[\"input\", \"output\"]\n)\ndistractor_val_tokenized = distractor_val_dataset.map(\n    tokenize_function, \n    batched=True,\n    remove_columns=[\"input\", \"output\"]\n)\n\n# 8. Define custom metrics computation functions\n\n# Helper function to compute custom ROUGE-like metrics\ndef compute_ngram_overlap(pred, ref, n=1):\n    \"\"\"Compute n-gram overlap between prediction and reference.\"\"\"\n    def get_ngrams(text, n):\n        tokens = text.lower().split()\n        ngrams = [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n        return Counter(ngrams)\n    \n    pred_ngrams = get_ngrams(pred, n)\n    ref_ngrams = get_ngrams(ref, n)\n    \n    # Find overlap\n    overlap = sum((pred_ngrams & ref_ngrams).values())\n    \n    # Compute precision and recall\n    if sum(pred_ngrams.values()) == 0:\n        precision = 0\n    else:\n        precision = overlap / sum(pred_ngrams.values())\n        \n    if sum(ref_ngrams.values()) == 0:\n        recall = 0\n    else:\n        recall = overlap / sum(ref_ngrams.values())\n    \n    # Compute F1\n    if precision + recall == 0:\n        f1 = 0\n    else:\n        f1 = 2 * precision * recall / (precision + recall)\n    \n    return precision, recall, f1\n\ndef compute_answer_metrics(eval_pred):\n    predictions, labels = eval_pred\n    # Replace -100 with the pad_token_id\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    \n    # Get the predicted tokens\n    predicted_tokens = np.argmax(predictions, axis=-1)\n    \n    # Decode to get the actual letters\n    decoded_preds = [tokenizer.decode(pred, skip_special_tokens=True) for pred in predicted_tokens]\n    decoded_labels = [tokenizer.decode(label, skip_special_tokens=True) for label in labels]\n    \n    # Calculate accuracy\n    correct = sum(1 for pred, label in zip(decoded_preds, decoded_labels) if pred.strip() == label.strip())\n    accuracy = correct / len(decoded_labels) if len(decoded_labels) > 0 else 0\n    \n    # Print some examples for debugging\n    print(\"\\nAnswer Generation Examples (Prediction, Reference):\")\n    for i in range(min(5, len(decoded_preds))):\n        print(f\"  {decoded_preds[i]} | {decoded_labels[i]}\")\n    \n    # Return metrics\n    return {\n        \"accuracy\": accuracy,\n        \"exact_match_ratio\": correct / len(decoded_labels) if len(decoded_labels) > 0 else 0,\n    }\n\ndef compute_distractor_metrics(eval_pred):\n    predictions, labels = eval_pred\n    # Replace -100 with the pad_token_id\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    \n    # Get the predicted tokens\n    predicted_tokens = np.argmax(predictions, axis=-1)\n    \n    # Decode to get the actual distractors\n    decoded_preds = [tokenizer.decode(pred, skip_special_tokens=True) for pred in predicted_tokens]\n    decoded_labels = [tokenizer.decode(label, skip_special_tokens=True) for label in labels]\n    \n    # Calculate our custom ROUGE-like scores\n    rouge1_scores = [compute_ngram_overlap(pred, ref, n=1) for pred, ref in zip(decoded_preds, decoded_labels)]\n    rouge2_scores = [compute_ngram_overlap(pred, ref, n=2) for pred, ref in zip(decoded_preds, decoded_labels)]\n    \n    # Average the scores\n    rouge1 = np.mean([score[2] for score in rouge1_scores]) if rouge1_scores else 0  # Use F1\n    rouge2 = np.mean([score[2] for score in rouge2_scores]) if rouge2_scores else 0  # Use F1\n    \n    # Print some examples for debugging\n    print(\"\\nDistractor Generation Examples (First 100 chars):\")\n    for i in range(min(3, len(decoded_preds))):\n        print(f\"  Pred: {decoded_preds[i][:100]}...\")\n        print(f\"  Ref:  {decoded_labels[i][:100]}...\")\n        print(f\"  ROUGE-1 F1: {rouge1_scores[i][2]:.4f}\")\n        print()\n    \n    # Return metrics\n    return {\n        \"rouge1\": rouge1,\n        \"rouge2\": rouge2,\n        \"avg_rouge\": (rouge1 + rouge2) / 2,\n    }\n\n# 9. Define training arguments for answer generation\nanswer_training_args = TrainingArguments(\n    output_dir=\"./results/answer_generation\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    greater_is_better=True,  # For accuracy, higher is better\n    logging_dir=\"./logs/answer_generation\",\n    logging_steps=100,  # Log every 100 steps\n    report_to=[\"tensorboard\"],  # Report metrics to TensorBoard\n)\n\n# 10. Define training arguments for distractor generation\ndistractor_training_args = TrainingArguments(\n    output_dir=\"./results/distractor_generation\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=5e-5,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    save_total_limit=3,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"avg_rouge\",\n    greater_is_better=True,  # For ROUGE, higher is better\n    logging_dir=\"./logs/distractor_generation\",\n    logging_steps=100,  # Log every 100 steps\n    report_to=[\"tensorboard\"],  # Report metrics to TensorBoard\n)\n\n# 11. Define data collator\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer,\n    model=model\n)\n\n# 12. Train answer generation model\nprint(\"Initializing answer generation trainer...\")\nanswer_trainer = Trainer(\n    model=model,\n    args=answer_training_args,\n    train_dataset=answer_train_tokenized,\n    eval_dataset=answer_val_tokenized,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_answer_metrics,\n)\n\nprint(\"Training answer generation model...\")\nanswer_trainer.train()\n\n# Get validation results\nprint(\"Evaluating answer generation model...\")\nanswer_eval_results = answer_trainer.evaluate()\nprint(\"\\n\" + \"=\"*50)\nprint(\"ANSWER GENERATION EVALUATION RESULTS\")\nprint(\"=\"*50)\nprint(f\"Accuracy: {answer_eval_results['eval_accuracy']:.4f}\")\nprint(f\"Exact Match: {answer_eval_results['eval_exact_match_ratio']:.4f}\")\nprint(\"=\"*50 + \"\\n\")\n\nanswer_trainer.save_model(\"./results/answer_generation/final_model\")\n\n# 13. Load a fresh model for distractor generation\nprint(\"Loading fresh model for distractor generation...\")\ndistractor_model = T5ForConditionalGeneration.from_pretrained(model_name)\n\n# 14. Train distractor generation model\nprint(\"Initializing distractor generation trainer...\")\ndistractor_trainer = Trainer(\n    model=distractor_model,\n    args=distractor_training_args,\n    train_dataset=distractor_train_tokenized,\n    eval_dataset=distractor_val_tokenized,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_distractor_metrics,\n)\n\nprint(\"Training distractor generation model...\")\ndistractor_trainer.train()\n\n# Get validation results\nprint(\"Evaluating distractor generation model...\")\ndistractor_eval_results = distractor_trainer.evaluate()\nprint(\"\\n\" + \"=\"*50)\nprint(\"DISTRACTOR GENERATION EVALUATION RESULTS\")\nprint(\"=\"*50)\nprint(f\"ROUGE-1 F1: {distractor_eval_results['eval_rouge1']:.4f}\")\nprint(f\"ROUGE-2 F1: {distractor_eval_results['eval_rouge2']:.4f}\")\nprint(f\"Average ROUGE: {distractor_eval_results['eval_avg_rouge']:.4f}\")\nprint(\"=\"*50 + \"\\n\")\n\ndistractor_trainer.save_model(\"./results/distractor_generation/final_model\")\n\n# 15. Test the models with a simple example\ndef generate_complete_mcq(context, question, answer_model, distractor_model, tokenizer, num_choices=4):\n    # Set models to evaluation mode\n    answer_model.eval()\n    distractor_model.eval()\n    \n    # Get the device\n    device = next(answer_model.parameters()).device\n    \n    # Step 1: Generate the answer letter\n    answer_input = f\"generate answer: {context} question: {question}\"\n    answer_input_ids = tokenizer(answer_input, return_tensors=\"pt\").input_ids.to(device)\n    \n    print(\"Generating answer letter...\")\n    answer_outputs = answer_model.generate(\n        answer_input_ids, \n        max_length=10,\n        num_beams=4,\n        early_stopping=True\n    )\n    answer_letter = tokenizer.decode(answer_outputs[0], skip_special_tokens=True).strip()\n    print(f\"Generated answer letter: {answer_letter}\")\n    \n    # For real inference, you would need another model to generate the correct answer text\n    # For this example, we'll use a placeholder\n    correct_answer_text = \"This is a placeholder for the correct answer content\"\n    \n    # Step 2: Generate distractors\n    distractor_input = f\"generate distractors: {context} question: {question} correct answer: {correct_answer_text}\"\n    distractor_input_ids = tokenizer(distractor_input, return_tensors=\"pt\").input_ids.to(device)\n    \n    print(\"Generating distractors...\")\n    distractor_outputs = distractor_model.generate(\n        distractor_input_ids, \n        max_length=256,\n        num_beams=4,\n        early_stopping=True\n    )\n    distractors_text = tokenizer.decode(distractor_outputs[0], skip_special_tokens=True)\n    distractors = distractors_text.split(\" [SEP] \")\n    print(f\"Generated {len(distractors)} distractors\")\n    \n    # Format as MCQ\n    choices = []\n    answer_idx = ord(answer_letter) - ord('A') if answer_letter.isalpha() else 0\n    \n    # Ensure we have the right number of choices\n    for i in range(num_choices):\n        if i == answer_idx:\n            choices.append(f\"{chr(65 + i)}. {correct_answer_text}\")\n        else:\n            distractor_idx = i if i < answer_idx else i - 1\n            if distractor_idx < len(distractors):\n                choices.append(f\"{chr(65 + i)}. {distractors[distractor_idx]}\")\n            else:\n                choices.append(f\"{chr(65 + i)}. Alternative distractor {i+1}\")\n    \n    return {\n        \"question\": question,\n        \"answer\": f\"Answer:{answer_letter}\",\n        \"choices\": choices\n    }\n\n# 16. Final summary and demo\nprint(\"\\n\" + \"=\"*70)\nprint(\" TRAINING SUMMARY \".center(70, \"=\"))\nprint(\"=\"*70)\nprint(f\"Answer Generation Model - Accuracy: {answer_eval_results['eval_accuracy']:.4f}\")\nprint(f\"Distractor Generation Model - Avg ROUGE: {distractor_eval_results['eval_avg_rouge']:.4f}\")\nprint(\"\\nModels saved to:\")\nprint(f\"  - Answer Generation: ./results/answer_generation/final_model\")\nprint(f\"  - Distractor Generation: ./results/distractor_generation/final_model\")\nprint(\"=\"*70)\n\n# Demo function to test the models (uncomment to run)\ndef demo_mcq_generation():\n    # Load trained models\n    answer_model_path = \"./results/answer_generation/final_model\"\n    distractor_model_path = \"./results/distractor_generation/final_model\"\n    \n    print(f\"Loading trained models from {answer_model_path} and {distractor_model_path}\")\n    \n    try:\n        answer_model = T5ForConditionalGeneration.from_pretrained(answer_model_path)\n        distractor_model = T5ForConditionalGeneration.from_pretrained(distractor_model_path)\n        \n        # Sample context and question\n        context = \"The Python programming language was created by Guido van Rossum and first released in 1991. It emphasizes code readability with its notable use of significant whitespace.\"\n        question = \"Who created the Python programming language?\"\n        \n        # Generate MCQ\n        mcq = generate_complete_mcq(context, question, answer_model, distractor_model, tokenizer)\n        \n        print(\"\\n===== GENERATED MCQ =====\")\n        print(f\"Question: {mcq['question']}\")\n        print(\"Choices:\")\n        for choice in mcq[\"choices\"]:\n            print(f\"  {choice}\")\n        print(f\"Correct Answer: {mcq['answer']}\")\n        \n    except Exception as e:\n        print(f\"Error during demo: {e}\")\n        print(\"Note: You need to run the training first to generate the model files.\")\n\nprint(\"\\nTraining complete! The trained models can now be used to generate MCQs.\")\n\n# Uncomment to run the demo after training\n# demo_mcq_generation()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T12:51:08.380636Z","iopub.execute_input":"2025-02-25T12:51:08.380936Z","execution_failed":"2025-02-25T16:51:59.781Z"}},"outputs":[{"name":"stdout","text":"Loading dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/813 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97b20c78cfbc4250b4ad3747a7d66b9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"02f2eecfda5642699cec7306db349dd7.parquet:   0%|          | 0.00/41.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e110c44d03ef4c54b9d92d864779ef00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"23afc2c9c15646b5b42c3c1fd8191bb3.parquet:   0%|          | 0.00/41.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c32eb9c2658f4dc3a791d6dea6b67ce5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"3e76396d09c3438c82927f7147f79e4e.parquet:   0%|          | 0.00/43.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18f3a12c798948d6849dc3af70afa2d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"4c027dcad52241f596b09eb8c6c893a9.parquet:   0%|          | 0.00/45.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc40af84c7a249909842c4a119a3fe01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"69e90546dac941528cdd07d49b93b140.parquet:   0%|          | 0.00/41.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aabe5b690c1548bea3f5430c57b43f7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"a2687c7645db40708ea8db9371a0e703.parquet:   0%|          | 0.00/42.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc58953b9634448989d7f0c4ec39ee1e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"cc8885cabdc8437b93220777f74d0c3f.parquet:   0%|          | 0.00/43.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12dfaf79e0824600bd11ecb6b8158b08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ea94289cf49746818839e882f248f13d.parquet:   0%|          | 0.00/8.93M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e37213c2753475d88165f4707f685e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"f10233c4ea29412b9a07e7ca8aedd169.parquet:   0%|          | 0.00/41.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cce4a86f1604debb5246fef1c723a01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"f458d85ec1e747cf8b47ac222a46d8c2.parquet:   0%|          | 0.00/42.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d19eb7f843374872a5285f6c7812d788"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1661603 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fbe5a708b0a4e28ae3d742192f0322e"}},"metadata":{}},{"name":"stdout","text":"Dataset loaded with 1661603 examples\nPreprocessing data for answer generation...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1661603 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b3383c657ce42248510465fbc821847"}},"metadata":{}},{"name":"stdout","text":"Preprocessing data for distractor generation...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1661603 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8844524bb5b2463da46b2a83ed7dcc56"}},"metadata":{}},{"name":"stdout","text":"Splitting datasets into train and validation sets...\nAnswer generation: 1495442 training examples, 166161 validation examples\nDistractor generation: 1495442 training examples, 166161 validation examples\nLoading tokenizer and model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"430431b59c174acb81cdb10972ed5a0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c3d09a3bc9c4ed8ab8ac4e93927d75e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f26fab543eb49d7b64f7972b870ca33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fee9850b3d474ee495fca088cf862a9f"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a2c9d2e850b49b2acf317125a460d92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c91124d2c0f44effa9076592db5a85b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d190bfdb1c445dd8982468a7175721c"}},"metadata":{}},{"name":"stdout","text":"Tokenizing datasets...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1495442 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d3c9bf6e12f49ca9bc57ae797d204a9"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5.py:289: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/166161 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b25308efd481420a85fa9c1ea86412ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1495442 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b020a13b2a247168edc1bb831fe0317"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/166161 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3c0c706e4764decb57ad9b79820982d"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-2-afe8e6f1b936>:302: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  answer_trainer = Trainer(\n","output_type":"stream"},{"name":"stdout","text":"Initializing answer generation trainer...\nTraining answer generation model...\n","output_type":"stream"},{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='40098' max='560793' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 40098/560793 3:27:31 < 44:54:59, 3.22 it/s, Epoch 0.21/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset, Dataset\nfrom transformers import (\n    T5ForConditionalGeneration,\n    T5Tokenizer,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForSeq2Seq\n)\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.auto import tqdm\nimport re\nfrom collections import Counter\n\n# 1. Load dataset\nprint(\"Loading dataset...\")\ndataset = load_dataset(\"zjsd/RedStone-QA-mcq\")\nprint(f\"Dataset loaded with {len(dataset['train'])} examples\")\n\n# Reduce dataset size by taking only 20% of the examples\nprint(\"Reducing dataset size to 20% of original...\")\n# Create a smaller sample using the take method\nreduced_size = int(len(dataset['train']) * 0.2)\ndataset['train'] = dataset['train'].shuffle(seed=42).select(range(reduced_size))\nprint(f\"Reduced dataset size: {len(dataset['train'])} examples\")\n\n# 2. Define preprocessing functions\ndef preprocess_for_answer_generation(examples):\n    \"\"\"Preprocess data for training the model to generate the correct answer letter.\"\"\"\n    inputs = []\n    for text, question in zip(examples[\"text\"], examples[\"question\"]):\n        combined = f\"generate answer: {text} question: {question}\"\n        inputs.append(combined)\n    \n    # Extract just the letter from \"Answer:X\" format\n    labels = [answer.replace(\"Answer:\", \"\").strip() for answer in examples[\"answer\"]]\n    \n    return {\n        \"input\": inputs,\n        \"output\": labels\n    }\n\ndef preprocess_for_distractor_generation(examples):\n    \"\"\"Preprocess data for training the model to generate distractors.\"\"\"\n    inputs = []\n    outputs = []\n    \n    for text, question, answer, choices in zip(\n        examples[\"text\"], \n        examples[\"question\"], \n        examples[\"answer\"], \n        examples[\"choices\"]\n    ):\n        # Extract the letter from \"Answer:X\"\n        answer_letter = answer.replace(\"Answer:\", \"\").strip()\n        \n        # Find the index of the correct answer\n        answer_idx = ord(answer_letter) - ord('A')\n        \n        # Get the correct answer text\n        if 0 <= answer_idx < len(choices):\n            correct_answer_text = choices[answer_idx].split('. ', 1)[1] if '. ' in choices[answer_idx] else choices[answer_idx]\n            \n            # Create input for distractor generation\n            input_text = f\"generate distractors: {text} question: {question} correct answer: {correct_answer_text}\"\n            \n            # Combine all distractors as output\n            distractors = [choice.split('. ', 1)[1] if '. ' in choice else choice \n                          for i, choice in enumerate(choices) if i != answer_idx]\n            output_text = \" [SEP] \".join(distractors)\n            \n            inputs.append(input_text)\n            outputs.append(output_text)\n    \n    return {\n        \"input\": inputs,\n        \"output\": outputs\n    }\n\n# 3. Process datasets for both tasks\nprint(\"Preprocessing data for answer generation...\")\nanswer_dataset = dataset[\"train\"].map(\n    preprocess_for_answer_generation,\n    batched=True,\n    remove_columns=dataset[\"train\"].column_names\n)\n\nprint(\"Preprocessing data for distractor generation...\")\ndistractor_dataset = dataset[\"train\"].map(\n    preprocess_for_distractor_generation,\n    batched=True,\n    remove_columns=dataset[\"train\"].column_names\n)\n\n# 4. Split datasets into train and validation\nprint(\"Splitting datasets into train and validation sets...\")\n# Convert to pandas DataFrame for easier splitting\nanswer_df = answer_dataset.to_pandas()\ndistractor_df = distractor_dataset.to_pandas()\n\n# Split using pandas DataFrame\nanswer_train_df, answer_val_df = train_test_split(answer_df, test_size=0.1, random_state=42)\ndistractor_train_df, distractor_val_df = train_test_split(distractor_df, test_size=0.1, random_state=42)\n\nprint(f\"Answer generation: {len(answer_train_df)} training examples, {len(answer_val_df)} validation examples\")\nprint(f\"Distractor generation: {len(distractor_train_df)} training examples, {len(distractor_val_df)} validation examples\")\n\n# Convert back to HuggingFace datasets\nanswer_train_dataset = Dataset.from_pandas(answer_train_df)\nanswer_val_dataset = Dataset.from_pandas(answer_val_df)\ndistractor_train_dataset = Dataset.from_pandas(distractor_train_df)\ndistractor_val_dataset = Dataset.from_pandas(distractor_val_df)\n\n# 5. Load tokenizer and model\nprint(\"Loading tokenizer and model...\")\nmodel_name = \"google/flan-t5-small\"  # Using the small model for faster training\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\n\n# 6. Define tokenization function\ndef tokenize_function(examples):\n    model_inputs = tokenizer(\n        examples[\"input\"],\n        max_length=512,\n        padding=\"max_length\",\n        truncation=True\n    )\n    \n    # Tokenize targets\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            examples[\"output\"],\n            max_length=128,\n            padding=\"max_length\",\n            truncation=True\n        )\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# 7. Tokenize datasets\nprint(\"Tokenizing datasets...\")\nanswer_train_tokenized = answer_train_dataset.map(\n    tokenize_function, \n    batched=True,\n    remove_columns=[\"input\", \"output\"]\n)\nanswer_val_tokenized = answer_val_dataset.map(\n    tokenize_function, \n    batched=True,\n    remove_columns=[\"input\", \"output\"]\n)\n\ndistractor_train_tokenized = distractor_train_dataset.map(\n    tokenize_function, \n    batched=True,\n    remove_columns=[\"input\", \"output\"]\n)\ndistractor_val_tokenized = distractor_val_dataset.map(\n    tokenize_function, \n    batched=True,\n    remove_columns=[\"input\", \"output\"]\n)\n\n# 8. Define custom metrics computation functions\n\n# Helper function to compute custom ROUGE-like metrics\ndef compute_ngram_overlap(pred, ref, n=1):\n    \"\"\"Compute n-gram overlap between prediction and reference.\"\"\"\n    def get_ngrams(text, n):\n        tokens = text.lower().split()\n        ngrams = [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n        return Counter(ngrams)\n    \n    pred_ngrams = get_ngrams(pred, n)\n    ref_ngrams = get_ngrams(ref, n)\n    \n    # Find overlap\n    overlap = sum((pred_ngrams & ref_ngrams).values())\n    \n    # Compute precision and recall\n    if sum(pred_ngrams.values()) == 0:\n        precision = 0\n    else:\n        precision = overlap / sum(pred_ngrams.values())\n        \n    if sum(ref_ngrams.values()) == 0:\n        recall = 0\n    else:\n        recall = overlap / sum(ref_ngrams.values())\n    \n    # Compute F1\n    if precision + recall == 0:\n        f1 = 0\n    else:\n        f1 = 2 * precision * recall / (precision + recall)\n    \n    return precision, recall, f1\n\ndef compute_answer_metrics(eval_pred):\n    predictions, labels = eval_pred\n    # Replace -100 with the pad_token_id\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    \n    # Get the predicted tokens\n    predicted_tokens = np.argmax(predictions, axis=-1)\n    \n    # Decode to get the actual letters\n    decoded_preds = [tokenizer.decode(pred, skip_special_tokens=True) for pred in predicted_tokens]\n    decoded_labels = [tokenizer.decode(label, skip_special_tokens=True) for label in labels]\n    \n    # Calculate accuracy\n    correct = sum(1 for pred, label in zip(decoded_preds, decoded_labels) if pred.strip() == label.strip())\n    accuracy = correct / len(decoded_labels) if len(decoded_labels) > 0 else 0\n    \n    # Print some examples for debugging\n    print(\"\\nAnswer Generation Examples (Prediction, Reference):\")\n    for i in range(min(5, len(decoded_preds))):\n        print(f\"  {decoded_preds[i]} | {decoded_labels[i]}\")\n    \n    # Return metrics\n    return {\n        \"accuracy\": accuracy,\n        \"exact_match_ratio\": correct / len(decoded_labels) if len(decoded_labels) > 0 else 0,\n    }\n\ndef compute_distractor_metrics(eval_pred):\n    predictions, labels = eval_pred\n    # Replace -100 with the pad_token_id\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    \n    # Get the predicted tokens\n    predicted_tokens = np.argmax(predictions, axis=-1)\n    \n    # Decode to get the actual distractors\n    decoded_preds = [tokenizer.decode(pred, skip_special_tokens=True) for pred in predicted_tokens]\n    decoded_labels = [tokenizer.decode(label, skip_special_tokens=True) for label in labels]\n    \n    # Calculate our custom ROUGE-like scores\n    rouge1_scores = [compute_ngram_overlap(pred, ref, n=1) for pred, ref in zip(decoded_preds, decoded_labels)]\n    rouge2_scores = [compute_ngram_overlap(pred, ref, n=2) for pred, ref in zip(decoded_preds, decoded_labels)]\n    \n    # Average the scores\n    rouge1 = np.mean([score[2] for score in rouge1_scores]) if rouge1_scores else 0  # Use F1\n    rouge2 = np.mean([score[2] for score in rouge2_scores]) if rouge2_scores else 0  # Use F1\n    \n    # Print some examples for debugging\n    print(\"\\nDistractor Generation Examples (First 100 chars):\")\n    for i in range(min(3, len(decoded_preds))):\n        print(f\"  Pred: {decoded_preds[i][:100]}...\")\n        print(f\"  Ref:  {decoded_labels[i][:100]}...\")\n        print(f\"  ROUGE-1 F1: {rouge1_scores[i][2]:.4f}\")\n        print()\n    \n    # Return metrics\n    return {\n        \"rouge1\": rouge1,\n        \"rouge2\": rouge2,\n        \"avg_rouge\": (rouge1 + rouge2) / 2,\n    }\n\n# 9. Define training arguments for answer generation with reduced epochs\nanswer_training_args = TrainingArguments(\n    output_dir=\"./results/answer_generation\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=8,  # Increased batch size\n    per_device_eval_batch_size=8,   # Increased batch size\n    num_train_epochs=2,             # Reduced from 3 to 2\n    weight_decay=0.01,\n    save_total_limit=1,             # Reduced from 2 to 1\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    greater_is_better=True,\n    logging_dir=\"./logs/answer_generation\",\n    logging_steps=100,\n    report_to=[\"tensorboard\"],\n)\n\n# 10. Define training arguments for distractor generation with reduced epochs\ndistractor_training_args = TrainingArguments(\n    output_dir=\"./results/distractor_generation\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=5e-5,\n    per_device_train_batch_size=16,  # Increased batch size\n    per_device_eval_batch_size=16,   # Increased batch size\n    num_train_epochs=3,              # Reduced from 3 to 2\n    weight_decay=0.01,\n    save_total_limit=1,              # Reduced from 3 to 1\n    load_best_model_at_end=True,\n    metric_for_best_model=\"avg_rouge\",\n    greater_is_better=True,\n    logging_dir=\"./logs/distractor_generation\",\n    logging_steps=100,\n    report_to=[\"tensorboard\"],\n)\n\n# 11. Define data collator\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer,\n    model=model\n)\n\n# 12. Train answer generation model\nprint(\"Initializing answer generation trainer...\")\nanswer_trainer = Trainer(\n    model=model,\n    args=answer_training_args,\n    train_dataset=answer_train_tokenized,\n    eval_dataset=answer_val_tokenized,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_answer_metrics,\n)\n\nprint(\"Training answer generation model...\")\nanswer_trainer.train()\n\n# Get validation results\nprint(\"Evaluating answer generation model...\")\nanswer_eval_results = answer_trainer.evaluate()\nprint(\"\\n\" + \"=\"*50)\nprint(\"ANSWER GENERATION EVALUATION RESULTS\")\nprint(\"=\"*50)\nprint(f\"Accuracy: {answer_eval_results['eval_accuracy']:.4f}\")\nprint(f\"Exact Match: {answer_eval_results['eval_exact_match_ratio']:.4f}\")\nprint(\"=\"*50 + \"\\n\")\n\nanswer_trainer.save_model(\"./results/answer_generation/final_model\")\n\n# 13. Load a fresh model for distractor generation\nprint(\"Loading fresh model for distractor generation...\")\ndistractor_model = T5ForConditionalGeneration.from_pretrained(model_name)\n\n# 14. Train distractor generation model\nprint(\"Initializing distractor generation trainer...\")\ndistractor_trainer = Trainer(\n    model=distractor_model,\n    args=distractor_training_args,\n    train_dataset=distractor_train_tokenized,\n    eval_dataset=distractor_val_tokenized,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_distractor_metrics,\n)\n\nprint(\"Training distractor generation model...\")\ndistractor_trainer.train()\n\n# Get validation results\nprint(\"Evaluating distractor generation model...\")\ndistractor_eval_results = distractor_trainer.evaluate()\nprint(\"\\n\" + \"=\"*50)\nprint(\"DISTRACTOR GENERATION EVALUATION RESULTS\")\nprint(\"=\"*50)\nprint(f\"ROUGE-1 F1: {distractor_eval_results['eval_rouge1']:.4f}\")\nprint(f\"ROUGE-2 F1: {distractor_eval_results['eval_rouge2']:.4f}\")\nprint(f\"Average ROUGE: {distractor_eval_results['eval_avg_rouge']:.4f}\")\nprint(\"=\"*50 + \"\\n\")\n\ndistractor_trainer.save_model(\"./results/distractor_generation/final_model\")\n\n# 15. Test the models with a simple example\ndef generate_complete_mcq(context, question, answer_model, distractor_model, tokenizer, num_choices=4):\n    # Set models to evaluation mode\n    answer_model.eval()\n    distractor_model.eval()\n    \n    # Get the device\n    device = next(answer_model.parameters()).device\n    \n    # Step 1: Generate the answer letter\n    answer_input = f\"generate answer: {context} question: {question}\"\n    answer_input_ids = tokenizer(answer_input, return_tensors=\"pt\").input_ids.to(device)\n    \n    print(\"Generating answer letter...\")\n    answer_outputs = answer_model.generate(\n        answer_input_ids, \n        max_length=10,\n        num_beams=4,\n        early_stopping=True\n    )\n    answer_letter = tokenizer.decode(answer_outputs[0], skip_special_tokens=True).strip()\n    print(f\"Generated answer letter: {answer_letter}\")\n    \n    # For real inference, you would need another model to generate the correct answer text\n    # For this example, we'll use a placeholder\n    correct_answer_text = \"This is a placeholder for the correct answer content\"\n    \n    # Step 2: Generate distractors\n    distractor_input = f\"generate distractors: {context} question: {question} correct answer: {correct_answer_text}\"\n    distractor_input_ids = tokenizer(distractor_input, return_tensors=\"pt\").input_ids.to(device)\n    \n    print(\"Generating distractors...\")\n    distractor_outputs = distractor_model.generate(\n        distractor_input_ids, \n        max_length=256,\n        num_beams=4,\n        early_stopping=True\n    )\n    distractors_text = tokenizer.decode(distractor_outputs[0], skip_special_tokens=True)\n    distractors = distractors_text.split(\" [SEP] \")\n    print(f\"Generated {len(distractors)} distractors\")\n    \n    # Format as MCQ\n    choices = []\n    answer_idx = ord(answer_letter) - ord('A') if answer_letter.isalpha() else 0\n    \n    # Ensure we have the right number of choices\n    for i in range(num_choices):\n        if i == answer_idx:\n            choices.append(f\"{chr(65 + i)}. {correct_answer_text}\")\n        else:\n            distractor_idx = i if i < answer_idx else i - 1\n            if distractor_idx < len(distractors):\n                choices.append(f\"{chr(65 + i)}. {distractors[distractor_idx]}\")\n            else:\n                choices.append(f\"{chr(65 + i)}. Alternative distractor {i+1}\")\n    \n    return {\n        \"question\": question,\n        \"answer\": f\"Answer:{answer_letter}\",\n        \"choices\": choices\n    }\n\n# 16. Final summary and demo\nprint(\"\\n\" + \"=\"*70)\nprint(\" TRAINING SUMMARY \".center(70, \"=\"))\nprint(\"=\"*70)\nprint(f\"Answer Generation Model - Accuracy: {answer_eval_results['eval_accuracy']:.4f}\")\nprint(f\"Distractor Generation Model - Avg ROUGE: {distractor_eval_results['eval_avg_rouge']:.4f}\")\nprint(\"\\nModels saved to:\")\nprint(f\"  - Answer Generation: ./results/answer_generation/final_model\")\nprint(f\"  - Distractor Generation: ./results/distractor_generation/final_model\")\nprint(\"=\"*70)\n\n# Demo function to test the models (uncomment to run)\ndef demo_mcq_generation():\n    # Load trained models\n    answer_model_path = \"./results/answer_generation/final_model\"\n    distractor_model_path = \"./results/distractor_generation/final_model\"\n    \n    print(f\"Loading trained models from {answer_model_path} and {distractor_model_path}\")\n    \n    try:\n        answer_model = T5ForConditionalGeneration.from_pretrained(answer_model_path)\n        distractor_model = T5ForConditionalGeneration.from_pretrained(distractor_model_path)\n        \n        # Sample context and question\n        context = \"The Python programming language was created by Guido van Rossum and first released in 1991. It emphasizes code readability with its notable use of significant whitespace.\"\n        question = \"Who created the Python programming language?\"\n        \n        # Generate MCQ\n        mcq = generate_complete_mcq(context, question, answer_model, distractor_model, tokenizer)\n        \n        print(\"\\n===== GENERATED MCQ =====\")\n        print(f\"Question: {mcq['question']}\")\n        print(\"Choices:\")\n        for choice in mcq[\"choices\"]:\n            print(f\"  {choice}\")\n        print(f\"Correct Answer: {mcq['answer']}\")\n        \n    except Exception as e:\n        print(f\"Error during demo: {e}\")\n        print(\"Note: You need to run the training first to generate the model files.\")\n\nprint(\"\\nTraining complete! The trained models can now be used to generate MCQs.\")\n\n# Uncomment to run the demo after training\n# demo_mcq_generation()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T17:03:38.594544Z","iopub.execute_input":"2025-02-25T17:03:38.594840Z"}},"outputs":[{"name":"stdout","text":"Loading dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/813 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2eabd0a123545ee9a668381ed936f7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"02f2eecfda5642699cec7306db349dd7.parquet:   0%|          | 0.00/41.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68de76e3fd0a46d9b82c1bb90939d76f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"23afc2c9c15646b5b42c3c1fd8191bb3.parquet:   0%|          | 0.00/41.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da3a3009114e40db95568ad117f58557"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"3e76396d09c3438c82927f7147f79e4e.parquet:   0%|          | 0.00/43.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13df5276503a4734bd26ffdd80558389"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"4c027dcad52241f596b09eb8c6c893a9.parquet:   0%|          | 0.00/45.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5eb5173733fe4c9b84c04b5ede050585"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"69e90546dac941528cdd07d49b93b140.parquet:   0%|          | 0.00/41.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f896841ecb714409aee21b9d0f189a6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"a2687c7645db40708ea8db9371a0e703.parquet:   0%|          | 0.00/42.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19700975992c4f5e92a9b6709d9acde9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"cc8885cabdc8437b93220777f74d0c3f.parquet:   0%|          | 0.00/43.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d24bf2c22984174b4b186a8ce337b13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ea94289cf49746818839e882f248f13d.parquet:   0%|          | 0.00/8.93M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73c786eaf12d467c8af52fc0dcfbceb6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"f10233c4ea29412b9a07e7ca8aedd169.parquet:   0%|          | 0.00/41.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"729eb44461b44e6099e358284f4d6f38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"f458d85ec1e747cf8b47ac222a46d8c2.parquet:   0%|          | 0.00/42.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6aac7eb9e0ab4adf8f28f12081623a48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1661603 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfe509b5e65e47c18e6634aaceaf3113"}},"metadata":{}},{"name":"stdout","text":"Dataset loaded with 1661603 examples\nReducing dataset size to 20% of original...\nReduced dataset size: 332320 examples\nPreprocessing data for answer generation...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/332320 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b09bf36e25a24f9cbf9b8c19cdbb3bd8"}},"metadata":{}},{"name":"stdout","text":"Preprocessing data for distractor generation...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/332320 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9538962dacf34044ae0e503daf2fea7a"}},"metadata":{}},{"name":"stdout","text":"Splitting datasets into train and validation sets...\nAnswer generation: 299088 training examples, 33232 validation examples\nDistractor generation: 299088 training examples, 33232 validation examples\nLoading tokenizer and model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8d43e36ce4346c3845fe03886cee67d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96d924aa5c07403e92cad2750a3e7962"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc520206f78f4322b79e53b44db47685"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70a87e3455454011b130c033197aa1b3"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c9018e054cc44808d77b2fbb1811f2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24387506491c40429b1d05cf62969c5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8f68bb671ce44419aff9aa6dbf93c56"}},"metadata":{}},{"name":"stdout","text":"Tokenizing datasets...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/299088 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4aa46f5ffcee4072bcc3d97848899058"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5.py:289: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/33232 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c99e4d87cb24d9c86321deb9e0671ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/299088 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4491c27b918d4011906570f9ba7a1d6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/33232 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5209c2fb5b304a17851122384b49d314"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-1-803e05ab98e4>:309: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  answer_trainer = Trainer(\n","output_type":"stream"},{"name":"stdout","text":"Initializing answer generation trainer...\nTraining answer generation model...\n","output_type":"stream"},{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6699' max='37386' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 6699/37386 53:06 < 4:03:22, 2.10 it/s, Epoch 0.36/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}