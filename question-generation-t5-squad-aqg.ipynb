{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":160296,"sourceType":"datasetVersion","datasetId":72533}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install evaluate rouge rouge_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-13T09:22:03.359009Z","iopub.execute_input":"2024-10-13T09:22:03.359452Z","iopub.status.idle":"2024-10-13T09:22:20.862613Z","shell.execute_reply.started":"2024-10-13T09:22:03.359400Z","shell.execute_reply":"2024-10-13T09:22:20.861404Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install huggingface_hub","metadata":{"execution":{"iopub.status.busy":"2024-10-13T09:22:20.864620Z","iopub.execute_input":"2024-10-13T09:22:20.864967Z","iopub.status.idle":"2024-10-13T09:22:34.067270Z","shell.execute_reply.started":"2024-10-13T09:22:20.864928Z","shell.execute_reply":"2024-10-13T09:22:34.066156Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-10-13T09:22:34.068551Z","iopub.execute_input":"2024-10-13T09:22:34.068874Z","iopub.status.idle":"2024-10-13T09:22:34.432410Z","shell.execute_reply.started":"2024-10-13T09:22:34.068836Z","shell.execute_reply":"2024-10-13T09:22:34.431534Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch,nltk,spacy,string,transformers,json,evaluate,warnings\nimport torch.nn as nn\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom torch.optim import Adam\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler\nfrom sklearn.model_selection import train_test_split\nfrom transformers import T5Tokenizer, T5Model, T5ForConditionalGeneration, T5TokenizerFast\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom sklearn.metrics import f1_score\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-10-13T09:22:42.782624Z","iopub.execute_input":"2024-10-13T09:22:42.783517Z","iopub.status.idle":"2024-10-13T09:23:03.812661Z","shell.execute_reply.started":"2024-10-13T09:22:42.783474Z","shell.execute_reply":"2024-10-13T09:23:03.811662Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# nltk.download('punkt')          # Tokenizer models\n# nltk.download('wordnet')        # WordNet lexical database\n# nltk.download('omw-1.4')        # Open Multilingual WordNet\n# nltk.download('averaged_perceptron_tagger')  # POS tagger\n# nltk.download('stopwords')      # Common stop words\n# nltk.download('vader_lexicon')  # Sentiment analysis lexicon","metadata":{"execution":{"iopub.status.busy":"2024-10-13T06:25:08.265373Z","iopub.execute_input":"2024-10-13T06:25:08.266380Z","iopub.status.idle":"2024-10-13T06:25:08.272378Z","shell.execute_reply.started":"2024-10-13T06:25:08.266322Z","shell.execute_reply":"2024-10-13T06:25:08.271056Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TOKENIZER = T5TokenizerFast.from_pretrained(\"t5-small\")\nDEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nMODEL = T5ForConditionalGeneration.from_pretrained(\"t5-small\", return_dict=True)\nMODEL.to(DEVICE)\nOPTIMIZER = Adam(MODEL.parameters(), lr=0.00001)\nQ_LEN = 256   # Question Length\nT_LEN = 32    # Target Length\nBATCH_SIZE = 4\nEPOCHS = 5\nOUTPUT_DIR = '/kaggle/tmp/'\nOUTPUT_MODEL_NAME = 'T5-QG-finetuned-squad'","metadata":{"execution":{"iopub.status.busy":"2024-10-13T09:23:03.814503Z","iopub.execute_input":"2024-10-13T09:23:03.815164Z","iopub.status.idle":"2024-10-13T09:23:07.875168Z","shell.execute_reply.started":"2024-10-13T09:23:03.815125Z","shell.execute_reply":"2024-10-13T09:23:07.874350Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loading the data\n\nwith open('/kaggle/input/squad-20/train-v2.0.json') as f:\n    data = json.load(f)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T09:23:07.876333Z","iopub.execute_input":"2024-10-13T09:23:07.876669Z","iopub.status.idle":"2024-10-13T09:23:09.810525Z","shell.execute_reply.started":"2024-10-13T09:23:07.876636Z","shell.execute_reply":"2024-10-13T09:23:09.809488Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def prepare_data(data):\n    articles = []\n    \n    for article in data[\"data\"]:\n        for paragraph in article[\"paragraphs\"]:\n            for qa in paragraph[\"qas\"]:\n                # We use context as the input and question as the output (label)\n                context = paragraph[\"context\"]\n                question = qa[\"question\"]\n\n                # If you want to generate questions from both context and answer\n                if not qa[\"is_impossible\"]:\n                    answer = qa[\"answers\"][0][\"text\"]\n                else:\n                    answer = \"\"\n\n                inputs = {\"context\": context, \"question\": question, \"answer\": answer}\n                articles.append(inputs)\n\n    return articles","metadata":{"execution":{"iopub.status.busy":"2024-10-13T09:23:09.822057Z","iopub.execute_input":"2024-10-13T09:23:09.822446Z","iopub.status.idle":"2024-10-13T09:23:09.831661Z","shell.execute_reply.started":"2024-10-13T09:23:09.822399Z","shell.execute_reply":"2024-10-13T09:23:09.830759Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = prepare_data(data)\n\n# Create a Dataframe\ndata = pd.DataFrame(data)\ndata","metadata":{"execution":{"iopub.status.busy":"2024-10-13T09:23:09.834986Z","iopub.execute_input":"2024-10-13T09:23:09.835299Z","iopub.status.idle":"2024-10-13T09:23:10.182363Z","shell.execute_reply.started":"2024-10-13T09:23:09.835265Z","shell.execute_reply":"2024-10-13T09:23:10.181394Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# data = data.sample(n=1000, random_state=42)\n# data = data.reset_index(drop=True)\ndata, test_data = train_test_split(data, test_size=0.1, random_state=42)\n\ndata = data.reset_index(drop=True)\ntest_data = test_data.reset_index(drop=True)\n\nprint(f\"Data size: {len(data)}\")\nprint(f\"Testing data size: {len(test_data)}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-13T09:23:10.184466Z","iopub.execute_input":"2024-10-13T09:23:10.185468Z","iopub.status.idle":"2024-10-13T09:23:10.231668Z","shell.execute_reply.started":"2024-10-13T09:23:10.185418Z","shell.execute_reply":"2024-10-13T09:23:10.230667Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class QA_Dataset(Dataset):\n    def __init__(self, tokenizer, dataframe, q_len, t_len):\n        self.tokenizer = tokenizer\n        self.q_len = q_len\n        self.t_len = t_len\n        self.data = dataframe\n        self.questions = self.data[\"question\"]\n        self.context = self.data[\"context\"]\n        \n    def __len__(self):\n        return len(self.questions)\n    \n    def __getitem__(self, idx):\n        question = self.questions[idx]\n        context = self.context[idx]\n        \n        # Tokenizing context only (no answer included)\n        input_tokenized = self.tokenizer(context, max_length=self.q_len, padding=\"max_length\",\n                                         truncation=True, pad_to_max_length=True, add_special_tokens=True)\n        \n        # Tokenizing the question to use as labels\n        question_tokenized = self.tokenizer(question, max_length=self.t_len, padding=\"max_length\", \n                                          truncation=True, pad_to_max_length=True, add_special_tokens=True)\n        \n        labels = torch.tensor(question_tokenized[\"input_ids\"], dtype=torch.long)\n        labels[labels == 0] = -100  # Ignore padding tokens\n\n        return {\n            \"input_ids\": torch.tensor(input_tokenized[\"input_ids\"], dtype=torch.long),\n            \"attention_mask\": torch.tensor(input_tokenized[\"attention_mask\"], dtype=torch.long),\n            \"labels\": labels,\n            \"decoder_attention_mask\": torch.tensor(question_tokenized[\"attention_mask\"], dtype=torch.long)\n        }","metadata":{"execution":{"iopub.status.busy":"2024-10-13T09:23:10.232932Z","iopub.execute_input":"2024-10-13T09:23:10.233255Z","iopub.status.idle":"2024-10-13T09:23:10.349159Z","shell.execute_reply.started":"2024-10-13T09:23:10.233222Z","shell.execute_reply":"2024-10-13T09:23:10.348017Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dataloader\ntrain_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n\ntrain_sampler = RandomSampler(train_data.index)\nval_sampler = RandomSampler(val_data.index)\n\nqa_dataset = QA_Dataset(TOKENIZER, data, Q_LEN, T_LEN)\n\ntrain_loader = DataLoader(qa_dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\nval_loader = DataLoader(qa_dataset, batch_size=BATCH_SIZE, sampler=val_sampler)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T09:23:10.350557Z","iopub.execute_input":"2024-10-13T09:23:10.351076Z","iopub.status.idle":"2024-10-13T09:23:10.381036Z","shell.execute_reply.started":"2024-10-13T09:23:10.351005Z","shell.execute_reply":"2024-10-13T09:23:10.380005Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(train_loader),len(val_loader))","metadata":{"execution":{"iopub.status.busy":"2024-10-13T09:23:10.384409Z","iopub.execute_input":"2024-10-13T09:23:10.384798Z","iopub.status.idle":"2024-10-13T09:23:10.390003Z","shell.execute_reply.started":"2024-10-13T09:23:10.384763Z","shell.execute_reply":"2024-10-13T09:23:10.389037Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Lists to store loss values for each epoch\ntrain_losses = []\nval_losses = []\n\nfor epoch in range(EPOCHS):\n    MODEL.train()\n    train_loss = 0\n    train_batch_count = 0\n    \n    for batch in tqdm(train_loader, desc=\"Training batches\"):\n        input_ids = batch[\"input_ids\"].to(DEVICE)\n        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n        labels = batch[\"labels\"].to(DEVICE)\n        decoder_attention_mask = batch[\"decoder_attention_mask\"].to(DEVICE)\n\n        outputs = MODEL(\n                          input_ids=input_ids,\n                          attention_mask=attention_mask,\n                          labels=labels,\n                          decoder_attention_mask=decoder_attention_mask\n                        )\n\n        OPTIMIZER.zero_grad()\n        outputs.loss.backward()\n        OPTIMIZER.step()\n        train_loss += outputs.loss.item()\n        train_batch_count += 1\n\n    avg_train_loss = train_loss / train_batch_count\n    train_losses.append(avg_train_loss)\n\n    # Evaluation\n    MODEL.eval()\n    val_loss = 0\n    val_batch_count = 0\n\n    for batch in tqdm(val_loader, desc=\"Validation batches\"):\n        input_ids = batch[\"input_ids\"].to(DEVICE)\n        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n        labels = batch[\"labels\"].to(DEVICE)\n        decoder_attention_mask = batch[\"decoder_attention_mask\"].to(DEVICE)\n\n        outputs = MODEL(\n                          input_ids=input_ids,\n                          attention_mask=attention_mask,\n                          labels=labels,\n                          decoder_attention_mask=decoder_attention_mask\n                        )\n\n        val_loss += outputs.loss.item()\n        val_batch_count += 1\n\n    avg_val_loss = val_loss / val_batch_count\n    val_losses.append(avg_val_loss)\n    \n    print(f\"{epoch+1}/{EPOCHS} -> Train loss: {avg_train_loss}\\tValidation loss: {avg_val_loss}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-13T09:23:10.391872Z","iopub.execute_input":"2024-10-13T09:23:10.392737Z","iopub.status.idle":"2024-10-13T12:34:12.356016Z","shell.execute_reply.started":"2024-10-13T09:23:10.392690Z","shell.execute_reply":"2024-10-13T12:34:12.354980Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plotting the loss\nplt.figure(figsize=(10, 5))\nplt.plot(range(1, EPOCHS+1), train_losses, marker='o', label='Train Loss')\nplt.plot(range(1, EPOCHS+1), val_losses, marker='o', label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss Over Epochs')\nplt.legend()\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-13T12:48:43.508419Z","iopub.execute_input":"2024-10-13T12:48:43.509303Z","iopub.status.idle":"2024-10-13T12:48:43.848439Z","shell.execute_reply.started":"2024-10-13T12:48:43.509260Z","shell.execute_reply":"2024-10-13T12:48:43.847546Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the fine-tuned model and tokenizer locally\nMODEL.save_pretrained(\"T5-QG-finetuned-squad\")\nTOKENIZER.save_pretrained(\"T5-QG-finetuned-squad\")","metadata":{"execution":{"iopub.status.busy":"2024-10-13T12:50:18.507848Z","iopub.execute_input":"2024-10-13T12:50:18.508531Z","iopub.status.idle":"2024-10-13T12:50:19.207442Z","shell.execute_reply.started":"2024-10-13T12:50:18.508490Z","shell.execute_reply":"2024-10-13T12:50:19.206535Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import T5ForConditionalGeneration\n\n# Create your model card details\nMODEL.push_to_hub(\"meowwShoo/T5-QG-finetuned-squad\", use_temp_dir=False)\nTOKENIZER.push_to_hub(\"meowwShoo/T5-QG-finetuned-squad\", use_temp_dir=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T12:50:50.922554Z","iopub.execute_input":"2024-10-13T12:50:50.922969Z","iopub.status.idle":"2024-10-13T12:50:54.243768Z","shell.execute_reply.started":"2024-10-13T12:50:50.922929Z","shell.execute_reply":"2024-10-13T12:50:54.242842Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MODEL.save_pretrained(f'{OUTPUT_DIR}{OUTPUT_MODEL_NAME}')\nTOKENIZER.save_pretrained(f'{OUTPUT_DIR}{OUTPUT_MODEL_NAME}')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\n# Zip the directory\nshutil.make_archive('/kaggle/tmp/T5_Model', 'zip', '/kaggle/tmp/', 'AG_T5_Model')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_question(context):\n    # Tokenize the input context (without answers)\n    inputs = TOKENIZER(context, max_length=Q_LEN, padding=\"max_length\", truncation=True, add_special_tokens=True)\n    \n    # Convert to tensor and move to the device (GPU/CPU)\n    input_ids = torch.tensor(inputs[\"input_ids\"], dtype=torch.long).to(DEVICE).unsqueeze(0)\n    attention_mask = torch.tensor(inputs[\"attention_mask\"], dtype=torch.long).to(DEVICE).unsqueeze(0)\n\n    # Generate the question based on the context\n    outputs = MODEL.generate(input_ids=input_ids, attention_mask=attention_mask)\n    \n    # Decode the generated tokens to obtain the question\n    generated_question = TOKENIZER.decode(outputs[0], skip_special_tokens=True)\n\n    # Return the context and the generated question\n    results = {\n        \"Context\": context,\n        \"Generated Question\": generated_question\n    }\n\n    return results","metadata":{"execution":{"iopub.status.busy":"2024-10-13T12:53:02.232518Z","iopub.execute_input":"2024-10-13T12:53:02.232919Z","iopub.status.idle":"2024-10-13T12:53:02.239883Z","shell.execute_reply.started":"2024-10-13T12:53:02.232883Z","shell.execute_reply":"2024-10-13T12:53:02.238876Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example context\ncontext = \"The Eiffel Tower is located in Paris, France. It was built in 1889.\"\n\n# Generate a question from the context\nresult = predict_question(context)\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T13:00:21.797162Z","iopub.execute_input":"2024-10-13T13:00:21.797566Z","iopub.status.idle":"2024-10-13T13:00:22.009549Z","shell.execute_reply.started":"2024-10-13T13:00:21.797528Z","shell.execute_reply":"2024-10-13T13:00:22.008530Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example context\ncontext = \"Once upon a time, in a small village nestled between rolling hills and deep forests, there lived a young girl named Elara. She was curious and adventurous, always wandering off to explore the hidden corners of the village and beyond. Her favorite place was the ancient forest, where she often found mysterious plants, animals, and strange sounds that whispered secrets of old.\"\n\n# Generate a question from the context\nresult = predict_question(context)\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T13:00:26.046540Z","iopub.execute_input":"2024-10-13T13:00:26.046962Z","iopub.status.idle":"2024-10-13T13:00:26.151856Z","shell.execute_reply.started":"2024-10-13T13:00:26.046925Z","shell.execute_reply":"2024-10-13T13:00:26.150930Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import T5ForConditionalGeneration\n\n# Load the model from the Hugging Face Hub\nmodel = T5ForConditionalGeneration.from_pretrained(\"meowwShoo/T5-QG-finetuned-squad\")\n\nfrom transformers import T5Tokenizer\n\n# Load the tokenizer from the Hugging Face Hub\ntokenizer = T5Tokenizer.from_pretrained(\"meowwShoo/T5-QG-finetuned-squad\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-13T13:28:33.699932Z","iopub.execute_input":"2024-10-13T13:28:33.700301Z","iopub.status.idle":"2024-10-13T13:28:34.431745Z","shell.execute_reply.started":"2024-10-13T13:28:33.700264Z","shell.execute_reply":"2024-10-13T13:28:34.430914Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_text = \"Once upon a time, in a small village nestled between rolling hills and deep forests, there lived a young girl named Elara. She was curious and adventurous, always wandering off to explore the hidden corners of the village and beyond. Her favorite place was the ancient forest, where she often found mysterious plants, animals, and strange sounds that whispered secrets of old.\"\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n\n# Generate output (here for conditional generation tasks)\noutput_ids = model.generate(input_ids)\ngenerated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n\nprint(generated_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-13T13:28:43.515973Z","iopub.execute_input":"2024-10-13T13:28:43.516831Z","iopub.status.idle":"2024-10-13T13:28:43.722252Z","shell.execute_reply.started":"2024-10-13T13:28:43.516790Z","shell.execute_reply":"2024-10-13T13:28:43.721217Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_text = \"Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an outputsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next\"\n\ninput_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n\n# Generate output (here for conditional generation tasks)\noutput_ids = model.generate(input_ids)\ngenerated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n\nprint(generated_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-13T13:29:09.810963Z","iopub.execute_input":"2024-10-13T13:29:09.811339Z","iopub.status.idle":"2024-10-13T13:29:10.136373Z","shell.execute_reply.started":"2024-10-13T13:29:09.811296Z","shell.execute_reply":"2024-10-13T13:29:10.135394Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"context = data.iloc[0][\"context\"]\nquestion = data.iloc[0][\"question\"]\nanswer = data.iloc[0][\"answer\"]\npredict_answer(context, question, answer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"context = test_data.iloc[0][\"context\"]\nquestion = test_data.iloc[0][\"question\"]\nanswer = test_data.iloc[0][\"answer\"]\npredict_answer(context, question, answer)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize lists to store metric scores and metadata\nresults_list = []\nbleu_scores = []\nrouge1_scores = []\nrouge2_scores = []\nrougeL_scores = []\nrougeW_scores = []\nrougeS_scores = []\nrougeSU_scores = []\nf1_scores = []\nmeteor_scores = []\n\n# Iterate through test data\nfor idx in tqdm(range(len(test_data))):\n    context = test_data.iloc[idx][\"context\"]\n    question = test_data.iloc[idx][\"question\"]\n    ref_answer = test_data.iloc[idx][\"answer\"]\n    \n    metrics = predict_answer(context, question, ref_answer)\n    \n    if ref_answer:\n        # Append metrics to lists\n        bleu_scores.append(metrics.get(\"Sentence BLEU Score\", 0))\n        rouge1_scores.append(metrics.get(\"ROUGE-1 Score\", 0))\n        rouge2_scores.append(metrics.get(\"ROUGE-2 Score\", 0))\n        rougeL_scores.append(metrics.get(\"ROUGE-L Score\", 0))\n        rougeW_scores.append(metrics.get(\"ROUGE-W Score\", 0))\n        rougeS_scores.append(metrics.get(\"ROUGE-S Score\", 0))\n        rougeSU_scores.append(metrics.get(\"ROUGE-SU Score\", 0))\n        f1_scores.append(metrics.get(\"GLUE Score (F1)\", 0))\n        meteor_scores.append(metrics.get(\"METEOR Score\", 0))\n        \n        # Append all data to results list\n        results_list.append({\n            \"ID\": idx,\n            \"Context\": context,\n            \"Question\": question,\n            \"Ref Answer\": ref_answer,\n            \"Predicted Answer\": metrics.get(\"Predicted Answer\", \"\"),\n            \"Sentence BLEU Score\": metrics.get(\"Sentence BLEU Score\", 0),\n            \"ROUGE-1 Score\": metrics.get(\"ROUGE-1 Score\", 0),\n            \"ROUGE-2 Score\": metrics.get(\"ROUGE-2 Score\", 0),\n            \"ROUGE-L Score\": metrics.get(\"ROUGE-L Score\", 0),\n            \"ROUGE-W Score\": metrics.get(\"ROUGE-W Score\", 0),\n            \"ROUGE-S Score\": metrics.get(\"ROUGE-S Score\", 0),\n            \"ROUGE-SU Score\": metrics.get(\"ROUGE-SU Score\", 0),\n            \"METEOR Score\": metrics.get(\"METEOR Score\", 0),\n            \"GLUE Score (F1)\": metrics.get(\"GLUE Score (F1)\", 0)\n        })\n\n# Convert results to DataFrame and save to CSV\nresults_df = pd.DataFrame(results_list)\nresults_df.to_csv('model_evaluation_results.csv', index=False)\n\n# Calculate average, std, max, and min\ndef compute_stats(scores):\n    return np.mean(scores), np.std(scores), np.max(scores), np.min(scores)\n\navg_bleu, std_bleu, max_bleu, min_bleu = compute_stats(bleu_scores)\navg_rouge1, std_rouge1, max_rouge1, min_rouge1 = compute_stats(rouge1_scores)\navg_rouge2, std_rouge2, max_rouge2, min_rouge2 = compute_stats(rouge2_scores)\navg_rougeL, std_rougeL, max_rougeL, min_rougeL = compute_stats(rougeL_scores)\navg_rougeW, std_rougeW, max_rougeW, min_rougeW = compute_stats(rougeW_scores)\navg_rougeS, std_rougeS, max_rougeS, min_rougeS = compute_stats(rougeS_scores)\navg_rougeSU, std_rougeSU, max_rougeSU, min_rougeSU = compute_stats(rougeSU_scores)\navg_f1, std_f1, max_f1, min_f1 = compute_stats(f1_scores)\navg_meteor, std_meteor, max_meteor, min_meteor = compute_stats(meteor_scores)\n\n# Print results\nprint(f\"Average BLEU Score: {avg_bleu:.4f}, Std Dev: {std_bleu:.4f}, Max: {max_bleu:.4f}, Min: {min_bleu:.4f}\")\nprint(f\"Average ROUGE-1 Score: {avg_rouge1:.4f}, Std Dev: {std_rouge1:.4f}, Max: {max_rouge1:.4f}, Min: {min_rouge1:.4f}\")\nprint(f\"Average ROUGE-2 Score: {avg_rouge2:.4f}, Std Dev: {std_rouge2:.4f}, Max: {max_rouge2:.4f}, Min: {min_rouge2:.4f}\")\nprint(f\"Average ROUGE-L Score: {avg_rougeL:.4f}, Std Dev: {std_rougeL:.4f}, Max: {max_rougeL:.4f}, Min: {min_rougeL:.4f}\")\nprint(f\"Average ROUGE-W Score: {avg_rougeW:.4f}, Std Dev: {std_rougeW:.4f}, Max: {max_rougeW:.4f}, Min: {min_rougeW:.4f}\")\nprint(f\"Average ROUGE-S Score: {avg_rougeS:.4f}, Std Dev: {std_rougeS:.4f}, Max: {max_rougeS:.4f}, Min: {min_rougeS:.4f}\")\nprint(f\"Average ROUGE-SU Score: {avg_rougeSU:.4f}, Std Dev: {std_rougeSU:.4f}, Max: {max_rougeSU:.4f}, Min: {min_rougeSU:.4f}\")\nprint(f\"Average METEOR Score: {avg_meteor:.4f}, Std Dev: {std_meteor:.4f}, Max: {max_meteor:.4f}, Min: {min_meteor:.4f}\")\nprint(f\"Average F1 Score: {avg_f1:.4f}, Std Dev: {std_f1:.4f}, Max: {max_f1:.4f}, Min: {min_f1:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}