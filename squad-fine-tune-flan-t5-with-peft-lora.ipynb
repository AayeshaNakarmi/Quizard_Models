{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":160296,"sourceType":"datasetVersion","datasetId":72533},{"sourceId":6131005,"sourceType":"datasetVersion","datasetId":3515188},{"sourceId":4251,"sourceType":"modelInstanceVersion","modelInstanceId":3045,"modelId":575}],"dockerImageVersionId":30528,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"##### Fine-Tune T5 on SQuAD","metadata":{"tags":[]}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# Table of Contents","metadata":{}},{"cell_type":"markdown","source":"- [ 1 - Set up Kernel, Load Required Dependencies, Dataset and LLM](#1)\n  - [ 1.1 - Set up Kernel and Required Dependencies](#1.1)\n  - [ 1.2 - Load Dataset and LLM](#1.2)\n  - [ 1.3 - Test the Model with Zero Shot Inferencing](#1.3)\n- [ 2 - Perform Full Fine-Tuning](#2)\n  - [ 2.1 - Preprocess the Dialog-Summary Dataset](#2.1)\n  - [ 2.2 - Fine-Tune the Model with the Preprocessed Dataset](#2.2)\n  - [ 2.3 - Evaluate the Model Qualitatively (Human Evaluation)](#2.3)\n  - [ 2.4 - Evaluate the Model Quantitatively (with ROUGE Metric)](#2.4)\n- [ 3 - Perform Parameter Efficient Fine-Tuning (PEFT)](#3)\n  - [ 3.1 - Setup the PEFT/LoRA model for Fine-Tuning](#3.1)\n  - [ 3.2 - Train PEFT Adapter](#3.2)\n  - [ 3.3 - Evaluate the Model Qualitatively (Human Evaluation)](#3.3)\n  - [ 3.4 - Evaluate the Model Quantitatively (with ROUGE Metric)](#3.4)","metadata":{"tags":[]}},{"cell_type":"markdown","source":"<a name='1'></a>\n## 1 - Set up Kernel, Load Required Dependencies, Dataset and LLM","metadata":{}},{"cell_type":"markdown","source":"<a name='1.1'></a>\n### 1.1 - Set up Kernel and Required Dependencies","metadata":{}},{"cell_type":"markdown","source":"Now install the required packages for the LLM and datasets.\n\n<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSIxMjUiIHZpZXdCb3g9IjAgMCA4MDAgMTI1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogICAgPGRlZnM+CiAgICAgICAgPGxpbmVhckdyYWRpZW50IGlkPSJmYWRlR3JhZGllbnQiIHgxPSIwIiB4Mj0iMSI+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiNGMEYwRjAiLz4KICAgICAgICAgICAgPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIiBzdG9wLW9wYWNpdHk9IjAiLz4KICAgICAgICA8L2xpbmVhckdyYWRpZW50PgogICAgICAgIDxtYXNrIGlkPSJmYWRlTWFzayI+CiAgICAgICAgICAgIDxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSI3NTAiIGhlaWdodD0iMTI1IiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSIxMjUiIGZpbGw9InVybCgjZmFkZUdyYWRpZW50KSIvPgogICAgICAgIDwvbWFzaz4KICAgIDwvZGVmcz4KICAgIDxwYXRoIGQ9Ik0zLDUwIEE1MCw1MCAwIDAgMSA1MywzIEw3OTcsMyBMNzk3LDk3IEw5Nyw5NyBMNTAsMTE1IEwzLDk3IFoiIGZpbGw9IiNGMEYwRjAiIHN0cm9rZT0iI0UwRTBFMCIgc3Ryb2tlLXdpZHRoPSIxIiBtYXNrPSJ1cmwoI2ZhZGVNYXNrKSIvPgogICAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgcj0iMzAiIGZpbGw9IiM1N2M0ZjgiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIxIi8+CiAgICA8Y2lyY2xlIGN4PSI1MCIgY3k9IjUwIiByPSIyNSIgZmlsbD0iI0YwRjBGMCIvPgogICAgPGxpbmUgeDE9IjUwIiB5MT0iNTAiIHgyPSI1MCIgeTI9IjMwIiBzdHJva2U9IiM1N2M0ZjgiIHN0cm9rZS13aWR0aD0iMyIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIi8+CiAgICA8bGluZSB4MT0iNTAiIHkxPSI1MCIgeDI9IjY1IiB5Mj0iNTAiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIzIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KICAgIDx0ZXh0IHg9IjEwMCIgeT0iMzQiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxNCIgZmlsbD0iIzMzMzMzMyI+VGhlIG5leHQgY2VsbCBtYXkgdGFrZSBhIGZldyBtaW51dGVzIHRvIHJ1bi4gUGxlYXNlIGJlIHBhdGllbnQuPC90ZXh0PgogICAgPHRleHQgeD0iMTAwIiB5PSI1NiIgZm9udC1mYW1pbHk9IkFyaWFsLCBzYW5zLXNlcmlmIiBmb250LXNpemU9IjE0IiBmaWxsPSIjMzMzMzMzIj5JZ25vcmUgdGhlIHdhcm5pbmdzIGFuZCBlcnJvcnMsIGFsb25nIHdpdGggdGhlIG5vdGUgYWJvdXQgcmVzdGFydGluZyB0aGUga2VybmVsIGF0IHRoZSBlbmQuPC90ZXh0Pgo8L3N2Zz4K\" alt=\"Time alert open medium\"/>","metadata":{"tags":[]}},{"cell_type":"code","source":"! pip install -q -U bitsandbytes\n! pip install -q -U transformers\n! pip install -q -U peft\n! pip install -q -U accelerate\n! pip install -q -U datasets\n! pip install -q -U wandb\n! pip install -q -U trl","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%pip install --upgrade pip\n%pip install --disable-pip-version-check \\\n    torch==1.13.1 \\\n    torchdata==0.5.1 --quiet\n\n%pip install \\\n    transformers==4.27.2 \\\n    datasets==2.11.0 \\\n    evaluate==0.4.0 \\\n    rouge_score==0.1.2 \\\n    loralib==0.1.1 \\\n    peft==0.3.0 --quiet","metadata":{"tags":[],"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-11-30T13:17:21.024015Z","iopub.execute_input":"2024-11-30T13:17:21.024813Z","iopub.status.idle":"2024-11-30T13:19:18.447202Z","shell.execute_reply.started":"2024-11-30T13:17:21.024785Z","shell.execute_reply":"2024-11-30T13:19:18.446039Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (23.1.2)\nCollecting pip\n  Downloading pip-24.3.1-py3-none-any.whl (1.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 23.1.2\n    Uninstalling pip-23.1.2:\n      Successfully uninstalled pip-23.1.2\nSuccessfully installed pip-24.3.1\nNote: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSI1MCIgdmlld0JveD0iMCAwIDgwMCA1MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxkZWZzPgogICAgICAgIDxsaW5lYXJHcmFkaWVudCBpZD0iZmFkZUdyYWRpZW50IiB4MT0iMCIgeDI9IjEiPgogICAgICAgICAgICA8c3RvcCBvZmZzZXQ9IjAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIi8+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMTAwJSIgc3RvcC1jb2xvcj0iI0YwRjBGMCIgc3RvcC1vcGFjaXR5PSIwIi8+CiAgICAgICAgPC9saW5lYXJHcmFkaWVudD4KICAgICAgICA8bWFzayBpZD0iZmFkZU1hc2siPgogICAgICAgICAgICA8cmVjdCB4PSIwIiB5PSIwIiB3aWR0aD0iNzUwIiBoZWlnaHQ9IjUwIiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSI1MCIgZmlsbD0idXJsKCNmYWRlR3JhZGllbnQpIi8+CiAgICAgICAgPC9tYXNrPgogICAgPC9kZWZzPgogICAgPHBhdGggZD0iTTI1LDUwIFEwLDUwIDAsMjUgTDUwLDMgTDk3LDI1IEw3OTcsMjUgTDc5Nyw1MCBMMjUsNTAgWiIgZmlsbD0iI0YwRjBGMCIgc3Ryb2tlPSIjRTBFMEUwIiBzdHJva2Utd2lkdGg9IjEiIG1hc2s9InVybCgjZmFkZU1hc2spIi8+Cjwvc3ZnPgo=\" alt=\"Time alert close\"/>","metadata":{"tags":[]}},{"cell_type":"code","source":"!pip install awscli ","metadata":{"_kg_hide-output":true,"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T13:19:18.449695Z","iopub.execute_input":"2024-11-30T13:19:18.450448Z","iopub.status.idle":"2024-11-30T13:19:29.637402Z","shell.execute_reply.started":"2024-11-30T13:19:18.450415Z","shell.execute_reply":"2024-11-30T13:19:29.636360Z"}},"outputs":[{"name":"stdout","text":"Collecting awscli\n  Downloading awscli-1.36.12-py3-none-any.whl.metadata (11 kB)\nCollecting botocore==1.35.71 (from awscli)\n  Downloading botocore-1.35.71-py3-none-any.whl.metadata (5.7 kB)\nCollecting docutils<0.17,>=0.10 (from awscli)\n  Downloading docutils-0.16-py2.py3-none-any.whl.metadata (2.7 kB)\nCollecting s3transfer<0.11.0,>=0.10.0 (from awscli)\n  Downloading s3transfer-0.10.4-py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: PyYAML<6.1,>=3.10 in /opt/conda/lib/python3.10/site-packages (from awscli) (6.0)\nRequirement already satisfied: colorama<0.4.7,>=0.2.5 in /opt/conda/lib/python3.10/site-packages (from awscli) (0.4.6)\nCollecting rsa<4.8,>=3.1.2 (from awscli)\n  Downloading rsa-4.7.2-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from botocore==1.35.71->awscli) (1.0.1)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.10/site-packages (from botocore==1.35.71->awscli) (2.8.2)\nRequirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.10/site-packages (from botocore==1.35.71->awscli) (1.26.15)\nRequirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from rsa<4.8,>=3.1.2->awscli) (0.4.8)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.35.71->awscli) (1.16.0)\nDownloading awscli-1.36.12-py3-none-any.whl (4.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading botocore-1.35.71-py3-none-any.whl (13.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m154.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading docutils-0.16-py2.py3-none-any.whl (548 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m548.2/548.2 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading rsa-4.7.2-py3-none-any.whl (34 kB)\nDownloading s3transfer-0.10.4-py3-none-any.whl (83 kB)\nInstalling collected packages: rsa, docutils, botocore, s3transfer, awscli\n  Attempting uninstall: rsa\n    Found existing installation: rsa 4.9\n    Uninstalling rsa-4.9:\n      Successfully uninstalled rsa-4.9\n  Attempting uninstall: docutils\n    Found existing installation: docutils 0.20.1\n    Uninstalling docutils-0.20.1:\n      Successfully uninstalled docutils-0.20.1\n  Attempting uninstall: botocore\n    Found existing installation: botocore 1.29.161\n    Uninstalling botocore-1.29.161:\n      Successfully uninstalled botocore-1.29.161\n  Attempting uninstall: s3transfer\n    Found existing installation: s3transfer 0.6.1\n    Uninstalling s3transfer-0.6.1:\n      Successfully uninstalled s3transfer-0.6.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\naiobotocore 2.5.2 requires botocore<1.29.162,>=1.29.161, but you have botocore 1.35.71 which is incompatible.\nboto3 1.26.100 requires botocore<1.30.0,>=1.29.100, but you have botocore 1.35.71 which is incompatible.\nboto3 1.26.100 requires s3transfer<0.7.0,>=0.6.0, but you have s3transfer 0.10.4 which is incompatible.\nkfp 2.0.1 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed awscli-1.36.12 botocore-1.35.71 docutils-0.16 rsa-4.7.2 s3transfer-0.10.4\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"Import the necessary components. Some of them are new for this week, they will be discussed later in the notebook. ","metadata":{"tags":[]}},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\nimport torch\nimport time\nimport evaluate\nimport pandas as pd\nimport numpy as np","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-30T13:19:29.638943Z","iopub.execute_input":"2024-11-30T13:19:29.639765Z","iopub.status.idle":"2024-11-30T13:19:41.012727Z","shell.execute_reply.started":"2024-11-30T13:19:29.639728Z","shell.execute_reply":"2024-11-30T13:19:41.011819Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport os\nimport wandb\nimport tqdm\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom time import time\nimport seaborn as sns\nfrom datasets import Dataset\nimport matplotlib.pyplot as plt\nfrom datasets import load_dataset\nimport torch\nimport torch.nn.functional as F\nfrom peft import (\n    LoraConfig, \n    PeftConfig, \n    LoraConfig, \n    TaskType, \n    PeftModel, \n    get_peft_model,\n    prepare_model_for_kbit_training\n)\nfrom transformers import (\n    AutoConfig,\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    AutoModelForSeq2SeqLM,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    TrainerCallback,\n    EarlyStoppingCallback\n)\nfrom sklearn.metrics import (\n    accuracy_score,\n    confusion_matrix, \n    f1_score,  \n    precision_score, \n    recall_score\n)\nfrom trl import SFTTrainer,setup_chat_format\nfrom kaggle_secrets import UserSecretsClient","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T13:21:07.513286Z","iopub.execute_input":"2024-11-30T13:21:07.513661Z","iopub.status.idle":"2024-11-30T13:21:07.746049Z","shell.execute_reply.started":"2024-11-30T13:21:07.513632Z","shell.execute_reply":"2024-11-30T13:21:07.744916Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m16\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m13 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96mdatasets\u001b[0m \u001b[94mimport\u001b[0m load_dataset                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m14 \u001b[0m\u001b[94mimport\u001b[0m \u001b[4;96mtorch\u001b[0m                                                                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m15 \u001b[0m\u001b[94mimport\u001b[0m \u001b[4;96mtorch\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mnn\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mfunctional\u001b[0m \u001b[94mas\u001b[0m \u001b[4;96mF\u001b[0m                                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m16 \u001b[94mfrom\u001b[0m \u001b[4;96mpeft\u001b[0m \u001b[94mimport\u001b[0m (                                                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m17 \u001b[0m\u001b[2m│   \u001b[0mLoraConfig,                                                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m18 \u001b[0m\u001b[2m│   \u001b[0mPeftConfig,                                                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m19 \u001b[0m\u001b[2m│   \u001b[0mLoraConfig,                                                                             \u001b[31m│\u001b[0m\n\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n\u001b[1;91mImportError: \u001b[0mcannot import name \u001b[32m'prepare_model_for_kbit_training'\u001b[0m from \u001b[32m'peft'\u001b[0m \n\u001b[1m(\u001b[0m\u001b[35m/opt/conda/lib/python3.10/site-packages/peft/\u001b[0m\u001b[95m__init__.py\u001b[0m\u001b[1m)\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">16</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">13 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">datasets</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> load_dataset                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">14 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">torch</span>                                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">15 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">torch.nn.functional</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">F</span>                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>16 <span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">peft</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> (                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">17 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>LoraConfig,                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">18 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>PeftConfig,                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">19 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>LoraConfig,                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ImportError: </span>cannot import name <span style=\"color: #008000; text-decoration-color: #008000\">'prepare_model_for_kbit_training'</span> from <span style=\"color: #008000; text-decoration-color: #008000\">'peft'</span> \n<span style=\"font-weight: bold\">(</span><span style=\"color: #800080; text-decoration-color: #800080\">/opt/conda/lib/python3.10/site-packages/peft/</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">__init__.py</span><span style=\"font-weight: bold\">)</span>\n</pre>\n"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"<a name='1.2'></a>\n### 1.2 - Load Dataset and LLM\n\nYou are going to continue experimenting with the [DialogSum](https://huggingface.co/datasets/knkarthick/dialogsum) Hugging Face dataset. It contains 10,000+ dialogues with the corresponding manually labeled summaries and topics. ","metadata":{"tags":[]}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load the SQuAD v2.0 dataset\ndataset = load_dataset(\"squad_v2\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-30T13:19:41.013840Z","iopub.execute_input":"2024-11-30T13:19:41.014189Z","iopub.status.idle":"2024-11-30T13:19:44.834052Z","shell.execute_reply.started":"2024-11-30T13:19:41.014147Z","shell.execute_reply":"2024-11-30T13:19:44.833156Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/8.92k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b93444eb048041f3b7a8d6c496e9edff"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset None/squad_v2 to /root/.cache/huggingface/datasets/parquet/squad_v2-ea29cd02ee439285/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57ca289ad4284d57a0b9856eb389b658"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/16.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7eeff03ec02d4a59b389771d389d8dec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.35M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5264860a4e242b3bc70d08188c3f29e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b10bcad2eb5b4282af89bea0f1dd79b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/130319 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e772efbf09b4789bd18950ce18db046"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/11873 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6eb31e1cee74311abc161f29eb0434a"}},"metadata":{}},{"name":"stdout","text":"Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/squad_v2-ea29cd02ee439285/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"846ac43f51f9413e9ffd488ba266d5e5"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Preprocess the train and validation data\ndef preprocess_data(data):\n    articles = []\n    \n    # Iterate through each article in the dataset\n    for article in data:\n        context = article[\"context\"]  # context is a string for each article\n        question = article[\"question\"]  # question field\n        \n        # Store only context and question (as input-output pairs for QG)\n        articles.append({\n            'context': context,\n            'question': question\n        })\n    \n    return articles\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T13:19:44.836186Z","iopub.execute_input":"2024-11-30T13:19:44.836462Z","iopub.status.idle":"2024-11-30T13:19:44.841285Z","shell.execute_reply.started":"2024-11-30T13:19:44.836438Z","shell.execute_reply":"2024-11-30T13:19:44.840493Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"print(dataset['train'][0])  # Print the first element of the 'train' split to check the structure","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T13:19:44.842772Z","iopub.execute_input":"2024-11-30T13:19:44.843012Z","iopub.status.idle":"2024-11-30T13:19:44.967139Z","shell.execute_reply.started":"2024-11-30T13:19:44.842992Z","shell.execute_reply":"2024-11-30T13:19:44.966146Z"}},"outputs":[{"name":"stdout","text":"{'id': '56be85543aeaaa14008c9063', 'title': 'Beyoncé', 'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".', 'question': 'When did Beyonce start becoming popular?', 'answers': {'text': ['in the late 1990s'], 'answer_start': [269]}}\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"train_data = preprocess_data(dataset['train'])\nval_data = preprocess_data(dataset['validation'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T13:19:44.968469Z","iopub.execute_input":"2024-11-30T13:19:44.968824Z","iopub.status.idle":"2024-11-30T13:19:52.912438Z","shell.execute_reply.started":"2024-11-30T13:19:44.968792Z","shell.execute_reply":"2024-11-30T13:19:52.911479Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"Load the pre-trained [FLAN-T5 model](https://huggingface.co/docs/transformers/model_doc/flan-t5) and its tokenizer directly from HuggingFace. Notice that you will be using the [small version](https://huggingface.co/google/flan-t5-base) of FLAN-T5. Setting `torch_dtype=torch.bfloat16` specifies the memory type to be used by this model.","metadata":{"tags":[]}},{"cell_type":"code","source":"print(train_data[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T13:19:52.913657Z","iopub.execute_input":"2024-11-30T13:19:52.913921Z","iopub.status.idle":"2024-11-30T13:19:52.918856Z","shell.execute_reply.started":"2024-11-30T13:19:52.913898Z","shell.execute_reply":"2024-11-30T13:19:52.917936Z"}},"outputs":[{"name":"stdout","text":"{'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".', 'question': 'When did Beyonce start becoming popular?'}\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Print the size of train_data and val_data\nprint(f\"Size of train_data: {len(train_data)}\")\nprint(f\"Size of val_data: {len(val_data)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T13:19:52.919981Z","iopub.execute_input":"2024-11-30T13:19:52.920242Z","iopub.status.idle":"2024-11-30T13:19:52.929658Z","shell.execute_reply.started":"2024-11-30T13:19:52.920221Z","shell.execute_reply":"2024-11-30T13:19:52.928858Z"}},"outputs":[{"name":"stdout","text":"Size of train_data: 130319\nSize of val_data: 11873\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# # model_name='google/flan-t5-base'\n# model_name='/kaggle/input/flan-t5/pytorch/base/4'\n# original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n# tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# # Measure time to prepare model and tokenizer\n# time_start = time()\n\n# # Define model ID and compute data type\n# model_id = 'google/flan-t5-base'\n\nmodel_id='/kaggle/input/flan-t5/pytorch/base/4'\ncompute_dtype = torch.bfloat16\n\n# Configure BitsAndBytes\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=True\n)\n\n# Load model configuration\nmodel_config = AutoConfig.from_pretrained(\n    model_id,\n    trust_remote_code=True,\n    max_new_tokens=1024\n)\n\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n# Load the Flan-T5 model\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\n    model_id,\n    trust_remote_code=True,\n    config=model_config,\n    quantization_config=bnb_config,  # Use quantization if specified\n    device_map='auto',  # Automatically map the model to available devices\n)\n\n# Setup chat format and prepare for k-bit training\nmodel, tokenizer = setup_chat_format(model, tokenizer)  # Ensure this function is defined\nmodel = prepare_model_for_kbit_training(model)  # Ensure this function is defined\n\n# Measure and print the time taken to prepare the model and tokenizer\ntime_end = time()\nprint(f\"Time to prepare model and tokenizer: {round(time_end - time_start, 3)} sec.\")","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-30T13:21:13.124614Z","iopub.execute_input":"2024-11-30T13:21:13.125203Z","iopub.status.idle":"2024-11-30T13:21:13.166200Z","shell.execute_reply.started":"2024-11-30T13:21:13.125172Z","shell.execute_reply":"2024-11-30T13:21:13.165008Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m16\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m13 \u001b[0mcompute_dtype = torch.bfloat16                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m14 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m15 \u001b[0m\u001b[2m# Configure BitsAndBytes\u001b[0m                                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m16 bnb_config = BitsAndBytesConfig(                                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m17 \u001b[0m\u001b[2m│   \u001b[0mload_in_4bit=\u001b[94mTrue\u001b[0m,                                                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m18 \u001b[0m\u001b[2m│   \u001b[0mbnb_4bit_quant_type=\u001b[33m\"\u001b[0m\u001b[33mnf4\u001b[0m\u001b[33m\"\u001b[0m,                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m19 \u001b[0m\u001b[2m│   \u001b[0mbnb_4bit_compute_dtype=compute_dtype,                                                   \u001b[31m│\u001b[0m\n\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n\u001b[1;91mNameError: \u001b[0mname \u001b[32m'BitsAndBytesConfig'\u001b[0m is not defined\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">16</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">13 </span>compute_dtype = torch.bfloat16                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">14 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">15 # Configure BitsAndBytes</span>                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>16 bnb_config = BitsAndBytesConfig(                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">17 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>load_in_4bit=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>,                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">18 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>bnb_4bit_quant_type=<span style=\"color: #808000; text-decoration-color: #808000\">\"nf4\"</span>,                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">19 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>bnb_4bit_compute_dtype=compute_dtype,                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'BitsAndBytesConfig'</span> is not defined\n</pre>\n"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"It is possible to pull out the number of model parameters and find out how many of them are trainable. The following function can be used to do that, at this stage, you do not need to go into details of it. ","metadata":{"tags":[]}},{"cell_type":"code","source":"def print_number_of_trainable_model_parameters(model):\n    trainable_model_params = 0\n    all_model_params = 0\n    for _, param in model.named_parameters():\n        all_model_params += param.numel()\n        if param.requires_grad:\n            trainable_model_params += param.numel()\n    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n\nprint(print_number_of_trainable_model_parameters(original_model))","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-30T13:19:53.540347Z","iopub.status.idle":"2024-11-30T13:19:53.540627Z","shell.execute_reply.started":"2024-11-30T13:19:53.540493Z","shell.execute_reply":"2024-11-30T13:19:53.540506Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(dataset.keys())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T13:19:53.542005Z","iopub.status.idle":"2024-11-30T13:19:53.542443Z","shell.execute_reply.started":"2024-11-30T13:19:53.542226Z","shell.execute_reply":"2024-11-30T13:19:53.542248Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a name='1.3'></a>\n### 1.3 - Test the Model with Zero Shot Inferencing\n\nTest the model with the zero shot inferencing. You can see that the model struggles to summarize the dialogue compared to the baseline summary, but it does pull out some important information from the text which indicates the model can be fine-tuned to the task at hand.","metadata":{"tags":[]}},{"cell_type":"code","source":"index = 200\n\n# Use the 'validation' split (since 'test' does not exist)\ncontext = dataset['validation'][index]['context']\n\n# Update the prompt to focus on question generation based on the given context\nprompt = f\"\"\"\nGenerate a question based on the following context.\n\nContext:\n{context}\n\nQuestion:\n\"\"\"\n\n# Tokenize the prompt and feed it to the model for generation\ninputs = tokenizer(prompt, return_tensors='pt')\n\n# Generate the output (question) from the model\noutput = tokenizer.decode(\n    model.generate(\n        inputs[\"input_ids\"], \n        max_new_tokens=50,  # Adjust the number of tokens to control question length\n    )[0], \n    skip_special_tokens=True\n)\n\n# Print the output\ndash_line = '-'.join('' for x in range(100))\nprint(dash_line)\nprint(f'INPUT PROMPT:\\n{prompt}')\nprint(dash_line)\nprint(f'MODEL GENERATED QUESTION:\\n{output}')\n","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-30T13:19:53.543499Z","iopub.status.idle":"2024-11-30T13:19:53.543760Z","shell.execute_reply.started":"2024-11-30T13:19:53.543633Z","shell.execute_reply":"2024-11-30T13:19:53.543646Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tokenize_data(data, tokenizer, max_input_length=512, max_target_length=64):\n    tokenized_data = []\n\n    for article in data:\n        context = article[\"context\"]  # Context from the dataset\n        question = article[\"question\"]  # The question is the target\n\n        # Tokenize context (input text) and question (target text) separately\n        inputs = tokenizer(context, padding=\"max_length\", truncation=True, max_length=max_input_length, return_tensors=\"pt\")\n        targets = tokenizer(question, padding=\"max_length\", truncation=True, max_length=max_target_length, return_tensors=\"pt\")\n\n        tokenized_data.append({\n            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n            \"labels\": targets[\"input_ids\"].squeeze()\n        })\n    \n    return tokenized_data\n\n# Tokenize the preprocessed data (train and validation)\ntrain_tokenized = tokenize_data(train_data, tokenizer)\nval_tokenized = tokenize_data(val_data, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T13:19:53.544906Z","iopub.status.idle":"2024-11-30T13:19:53.545243Z","shell.execute_reply.started":"2024-11-30T13:19:53.545059Z","shell.execute_reply":"2024-11-30T13:19:53.545073Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass QuestionGenerationDataset(Dataset):\n    def __init__(self, tokenized_data):\n        self.data = tokenized_data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\n# Create datasets\ntrain_dataset = QuestionGenerationDataset(train_tokenized)\nval_dataset = QuestionGenerationDataset(val_tokenized)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T13:19:53.546340Z","iopub.status.idle":"2024-11-30T13:19:53.546666Z","shell.execute_reply.started":"2024-11-30T13:19:53.546506Z","shell.execute_reply":"2024-11-30T13:19:53.546521Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n# Create DataLoaders for training and validation\ntrain_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=8)\n\n# Example of iterating through the training batches\nfor batch in train_dataloader:\n    print(batch)\n    break  # Just to inspect the first batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T13:19:53.547426Z","iopub.status.idle":"2024-11-30T13:19:53.547706Z","shell.execute_reply.started":"2024-11-30T13:19:53.547572Z","shell.execute_reply":"2024-11-30T13:19:53.547586Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print the size (length) of the datasets\nprint(f\"Number of training samples: {len(train_dataset)}\")\nprint(f\"Number of validation samples: {len(val_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T13:19:53.548944Z","iopub.status.idle":"2024-11-30T13:19:53.549268Z","shell.execute_reply.started":"2024-11-30T13:19:53.549096Z","shell.execute_reply":"2024-11-30T13:19:53.549133Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tokenize_function(example):\n    start_prompt = 'Given the following context, create a detailed and contextually rich question.\\n\\n'\n    end_prompt = '\\n\\nQuestion: '\n    \n    # Concatenate the context with the start and end prompt\n    prompt = [start_prompt + context + end_prompt for context in example[\"context\"]]\n    \n    # Tokenize the input (context) and the target (question)\n    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n    \n    # Use the question as the target (label)\n    example['labels'] = tokenizer(example[\"question\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n    \n    return example\n\n# Map the tokenize function to all splits in the dataset (train, validation, test)\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n\n# Remove unnecessary columns\ntokenized_datasets = tokenized_datasets.remove_columns(['id', 'context', 'question', 'answers'])\n\n# Check the result (for train split)\nprint(tokenized_datasets['train'][0])  # Check the first element in the train split\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T13:19:53.550408Z","iopub.status.idle":"2024-11-30T13:19:53.550672Z","shell.execute_reply.started":"2024-11-30T13:19:53.550547Z","shell.execute_reply":"2024-11-30T13:19:53.550560Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenized_datasets = dataset.map(tokenize_function, batched=True)\n\n# After tokenization, you can access the splits:\ntrain_dataset = tokenized_datasets['train']\nval_dataset = tokenized_datasets['validation']\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T13:19:53.552016Z","iopub.status.idle":"2024-11-30T13:19:53.552318Z","shell.execute_reply.started":"2024-11-30T13:19:53.552181Z","shell.execute_reply":"2024-11-30T13:19:53.552195Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To save some time in the lab, you will subsample the dataset:","metadata":{"tags":[]}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Print the size (length) of the datasets\nprint(f\"Number of training samples: {len(train_dataset)}\")\nprint(f\"Number of validation samples: {len(val_dataset)}\")","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T13:19:53.553140Z","iopub.status.idle":"2024-11-30T13:19:53.553436Z","shell.execute_reply.started":"2024-11-30T13:19:53.553296Z","shell.execute_reply":"2024-11-30T13:19:53.553309Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The output dataset is ready for fine-tuning.","metadata":{}},{"cell_type":"markdown","source":"<a name='2.2'></a>\n### 2.2 - Fine-Tune the Model with the Preprocessed Dataset\n\nNow utilize the built-in Hugging Face `Trainer` class (see the documentation [here](https://huggingface.co/docs/transformers/main_classes/trainer)). Pass the preprocessed dataset with reference to the original model. Other training parameters are found experimentally and there is no need to go into details about those at the moment.","metadata":{}},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T13:19:53.554300Z","iopub.status.idle":"2024-11-30T13:19:53.554566Z","shell.execute_reply.started":"2024-11-30T13:19:53.554440Z","shell.execute_reply":"2024-11-30T13:19:53.554453Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Start training process...\n\n<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSIxMjUiIHZpZXdCb3g9IjAgMCA4MDAgMTI1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogICAgPGRlZnM+CiAgICAgICAgPGxpbmVhckdyYWRpZW50IGlkPSJmYWRlR3JhZGllbnQiIHgxPSIwIiB4Mj0iMSI+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiNGMEYwRjAiLz4KICAgICAgICAgICAgPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIiBzdG9wLW9wYWNpdHk9IjAiLz4KICAgICAgICA8L2xpbmVhckdyYWRpZW50PgogICAgICAgIDxtYXNrIGlkPSJmYWRlTWFzayI+CiAgICAgICAgICAgIDxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSI3NTAiIGhlaWdodD0iMTI1IiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSIxMjUiIGZpbGw9InVybCgjZmFkZUdyYWRpZW50KSIvPgogICAgICAgIDwvbWFzaz4KICAgIDwvZGVmcz4KICAgIDxwYXRoIGQ9Ik0zLDUwIEE1MCw1MCAwIDAgMSA1MywzIEw3OTcsMyBMNzk3LDk3IEw5Nyw5NyBMNTAsMTE1IEwzLDk3IFoiIGZpbGw9IiNGMEYwRjAiIHN0cm9rZT0iI0UwRTBFMCIgc3Ryb2tlLXdpZHRoPSIxIiBtYXNrPSJ1cmwoI2ZhZGVNYXNrKSIvPgogICAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgcj0iMzAiIGZpbGw9IiM1N2M0ZjgiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIxIi8+CiAgICA8Y2lyY2xlIGN4PSI1MCIgY3k9IjUwIiByPSIyNSIgZmlsbD0iI0YwRjBGMCIvPgogICAgPGxpbmUgeDE9IjUwIiB5MT0iNTAiIHgyPSI1MCIgeTI9IjMwIiBzdHJva2U9IiM1N2M0ZjgiIHN0cm9rZS13aWR0aD0iMyIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIi8+CiAgICA8bGluZSB4MT0iNTAiIHkxPSI1MCIgeDI9IjY1IiB5Mj0iNTAiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIzIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KICAgIDx0ZXh0IHg9IjEwMCIgeT0iMzQiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxNCIgZmlsbD0iIzMzMzMzMyI+VGhlIG5leHQgY2VsbCBtYXkgdGFrZSBhIGZldyBtaW51dGVzIHRvIHJ1bi4gUGxlYXNlIGJlIHBhdGllbnQuPC90ZXh0PgogICAgPHRleHQgeD0iMTAwIiB5PSI1NiIgZm9udC1mYW1pbHk9IkFyaWFsLCBzYW5zLXNlcmlmIiBmb250LXNpemU9IjE0IiBmaWxsPSIjMzMzMzMzIj5Zb3UgY2FuIHNhZmVseSBpZ25vcmUgdGhlIHdhcm5pbmcgbWVzc2FnZXMuPC90ZXh0Pgo8L3N2Zz4K\" alt=\"Time alert open medium\"/>","metadata":{"tags":[]}},{"cell_type":"markdown","source":"<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSI1MCIgdmlld0JveD0iMCAwIDgwMCA1MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxkZWZzPgogICAgICAgIDxsaW5lYXJHcmFkaWVudCBpZD0iZmFkZUdyYWRpZW50IiB4MT0iMCIgeDI9IjEiPgogICAgICAgICAgICA8c3RvcCBvZmZzZXQ9IjAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIi8+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMTAwJSIgc3RvcC1jb2xvcj0iI0YwRjBGMCIgc3RvcC1vcGFjaXR5PSIwIi8+CiAgICAgICAgPC9saW5lYXJHcmFkaWVudD4KICAgICAgICA8bWFzayBpZD0iZmFkZU1hc2siPgogICAgICAgICAgICA8cmVjdCB4PSIwIiB5PSIwIiB3aWR0aD0iNzUwIiBoZWlnaHQ9IjUwIiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSI1MCIgZmlsbD0idXJsKCNmYWRlR3JhZGllbnQpIi8+CiAgICAgICAgPC9tYXNrPgogICAgPC9kZWZzPgogICAgPHBhdGggZD0iTTI1LDUwIFEwLDUwIDAsMjUgTDUwLDMgTDk3LDI1IEw3OTcsMjUgTDc5Nyw1MCBMMjUsNTAgWiIgZmlsbD0iI0YwRjBGMCIgc3Ryb2tlPSIjRTBFMEUwIiBzdHJva2Utd2lkdGg9IjEiIG1hc2s9InVybCgjZmFkZU1hc2spIi8+Cjwvc3ZnPgo=\" alt=\"Time alert close\"/>","metadata":{"tags":[]}},{"cell_type":"markdown","source":"Training a fully fine-tuned version of the model would take a few hours on a GPU. To save time, download a checkpoint of the fully fine-tuned model to use in the rest of this notebook. This fully fine-tuned model will also be referred to as the **instruct model** in this lab.","metadata":{}},{"cell_type":"code","source":"# !aws s3 cp --recursive s3://dlai-generative-ai/models/flan-dialogue-summary-checkpoint/ ./flan-dialogue-summary-checkpoint/","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-30T13:19:53.555871Z","iopub.status.idle":"2024-11-30T13:19:53.556199Z","shell.execute_reply.started":"2024-11-30T13:19:53.556023Z","shell.execute_reply":"2024-11-30T13:19:53.556037Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The size of the downloaded instruct model is approximately 1GB.","metadata":{"tags":[]}},{"cell_type":"code","source":"# !ls -alh /kaggle/working/flan-dialogue-summary-checkpoint/pytorch_model.bin","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-30T13:19:53.557778Z","iopub.status.idle":"2024-11-30T13:19:53.558065Z","shell.execute_reply.started":"2024-11-30T13:19:53.557929Z","shell.execute_reply":"2024-11-30T13:19:53.557943Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Create an instance of the `AutoModelForSeq2SeqLM` class for the instruct model:","metadata":{"tags":[]}},{"cell_type":"markdown","source":"<a name='2.3'></a>\n### 2.3 - Evaluate the Model Qualitatively (Human Evaluation)\n\nAs with many GenAI applications, a qualitative approach where you ask yourself the question \"Is my model behaving the way it is supposed to?\" is usually a good starting point. In the example below (the same one we started this notebook with), you can see how the fine-tuned model is able to create a reasonable summary of the dialogue compared to the original inability to understand what is being asked of the model.","metadata":{}},{"cell_type":"markdown","source":"Evaluate the models computing ROUGE metrics. Notice the improvement in the results!","metadata":{"tags":[]}},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, TaskType\n\nlora_config = LoraConfig(\n    r=32, # Rank\n    lora_alpha=32,\n    target_modules=[\"q\", \"v\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T13:19:53.559420Z","iopub.status.idle":"2024-11-30T13:19:53.559711Z","shell.execute_reply.started":"2024-11-30T13:19:53.559573Z","shell.execute_reply":"2024-11-30T13:19:53.559587Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Apply LoRA to your model\npeft_model = get_peft_model(original_model, lora_config)\n\n# Check the number of trainable parameters\ndef print_number_of_trainable_model_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"Trainable parameters in PEFT model: {print_number_of_trainable_model_parameters(peft_model)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T13:19:53.561005Z","iopub.status.idle":"2024-11-30T13:19:53.561305Z","shell.execute_reply.started":"2024-11-30T13:19:53.561169Z","shell.execute_reply":"2024-11-30T13:19:53.561183Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Add LoRA adapter layers/parameters to the original LLM to be trained.","metadata":{"tags":[]}},{"cell_type":"code","source":"# Define the output directory for saving model checkpoints\noutput_dir = f'/kaggle/working/peft-flan-t5-squad-{str(int(time.time()))}'\n\n# Define TrainingArguments for PEFT fine-tuning\npeft_training_args = TrainingArguments(\n    output_dir=output_dir,\n    learning_rate=2e-5,  # Optimal learning rate for PEFT (higher than standard fine-tuning, but still small)\n    num_train_epochs=7,  # More epochs for better adaptation to the task\n    logging_steps=500,  # Log every 500 steps (adjust based on the dataset size)\n    evaluation_strategy=\"steps\",  # Evaluate every few steps\n    eval_steps=500,  # Evaluate every 500 steps (adjust for your dataset size)\n    save_steps=500,  # Save the model every 500 steps\n    per_device_train_batch_size=16,  # Batch size for training, adjust depending on available memory\n    per_device_eval_batch_size=16,  # Batch size for evaluation, adjust based on memory\n    warmup_steps=1000,  # Number of steps for learning rate warmup\n    weight_decay=0.01,  # Weight decay to prevent overfitting\n    gradient_accumulation_steps=2,  # Accumulate gradients over 2 steps to simulate larger batch size\n    fp16=True,  # Use mixed precision for faster training\n    report_to=None,  # Disable reporting to external services (you can enable if needed)\n    save_total_limit=3,  # Limit the number of saved checkpoints to avoid filling up disk space\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T13:19:53.562203Z","iopub.status.idle":"2024-11-30T13:19:53.562501Z","shell.execute_reply.started":"2024-11-30T13:19:53.562351Z","shell.execute_reply":"2024-11-30T13:19:53.562365Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize the Trainer with model, training arguments, and datasets\npeft_trainer = Trainer(\n    model=peft_model,  # Make sure `original_model` is the FLAN-T5 model\n    args=peft_training_args,\n    train_dataset=train_dataset,  # Use your tokenized training dataset\n    eval_dataset=val_dataset,  # Use your tokenized validation dataset\n)\n\n# Start training\npeft_trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T13:19:53.563470Z","iopub.status.idle":"2024-11-30T13:19:53.563758Z","shell.execute_reply.started":"2024-11-30T13:19:53.563620Z","shell.execute_reply":"2024-11-30T13:19:53.563635Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a name='3.2'></a>\n### 3.2 - Train PEFT Adapter\n\nDefine training arguments and create `Trainer` instance.","metadata":{"tags":[]}},{"cell_type":"code","source":"output_dir = f'/kaggle/working/peft-dialogue-summary-training-{str(int(time.time()))}'\n\npeft_training_args = TrainingArguments(\n    output_dir=output_dir,\n    learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n    num_train_epochs=1,\n    logging_steps=1,\n    max_steps=1    \n)\n    \npeft_trainer = Trainer(\n    model=peft_model,\n    args=peft_training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-30T13:19:53.564951Z","iopub.status.idle":"2024-11-30T13:19:53.565250Z","shell.execute_reply.started":"2024-11-30T13:19:53.565081Z","shell.execute_reply":"2024-11-30T13:19:53.565094Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now everything is ready to train the PEFT adapter and save the model.\n\n<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSIxMjUiIHZpZXdCb3g9IjAgMCA4MDAgMTI1IiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgogICAgPGRlZnM+CiAgICAgICAgPGxpbmVhckdyYWRpZW50IGlkPSJmYWRlR3JhZGllbnQiIHgxPSIwIiB4Mj0iMSI+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMCUiIHN0b3AtY29sb3I9IiNGMEYwRjAiLz4KICAgICAgICAgICAgPHN0b3Agb2Zmc2V0PSIxMDAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIiBzdG9wLW9wYWNpdHk9IjAiLz4KICAgICAgICA8L2xpbmVhckdyYWRpZW50PgogICAgICAgIDxtYXNrIGlkPSJmYWRlTWFzayI+CiAgICAgICAgICAgIDxyZWN0IHg9IjAiIHk9IjAiIHdpZHRoPSI3NTAiIGhlaWdodD0iMTI1IiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSIxMjUiIGZpbGw9InVybCgjZmFkZUdyYWRpZW50KSIvPgogICAgICAgIDwvbWFzaz4KICAgIDwvZGVmcz4KICAgIDxwYXRoIGQ9Ik0zLDUwIEE1MCw1MCAwIDAgMSA1MywzIEw3OTcsMyBMNzk3LDk3IEw5Nyw5NyBMNTAsMTE1IEwzLDk3IFoiIGZpbGw9IiNGMEYwRjAiIHN0cm9rZT0iI0UwRTBFMCIgc3Ryb2tlLXdpZHRoPSIxIiBtYXNrPSJ1cmwoI2ZhZGVNYXNrKSIvPgogICAgPGNpcmNsZSBjeD0iNTAiIGN5PSI1MCIgcj0iMzAiIGZpbGw9IiM1N2M0ZjgiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIxIi8+CiAgICA8Y2lyY2xlIGN4PSI1MCIgY3k9IjUwIiByPSIyNSIgZmlsbD0iI0YwRjBGMCIvPgogICAgPGxpbmUgeDE9IjUwIiB5MT0iNTAiIHgyPSI1MCIgeTI9IjMwIiBzdHJva2U9IiM1N2M0ZjgiIHN0cm9rZS13aWR0aD0iMyIgc3Ryb2tlLWxpbmVjYXA9InJvdW5kIi8+CiAgICA8bGluZSB4MT0iNTAiIHkxPSI1MCIgeDI9IjY1IiB5Mj0iNTAiIHN0cm9rZT0iIzU3YzRmOCIgc3Ryb2tlLXdpZHRoPSIzIiBzdHJva2UtbGluZWNhcD0icm91bmQiLz4KICAgIDx0ZXh0IHg9IjEwMCIgeT0iMzQiIGZvbnQtZmFtaWx5PSJBcmlhbCwgc2Fucy1zZXJpZiIgZm9udC1zaXplPSIxNCIgZmlsbD0iIzMzMzMzMyI+VGhlIG5leHQgY2VsbCBtYXkgdGFrZSBhIGZldyBtaW51dGVzIHRvIHJ1bi48L3RleHQ+Cjwvc3ZnPgo=\" alt=\"Time alert open medium\"/>","metadata":{}},{"cell_type":"code","source":"peft_trainer.train()\n\npeft_model_path=\"/kaggle/working/peft-dialogue-summary-checkpoint-local\"\n\npeft_trainer.model.save_pretrained(peft_model_path)\ntokenizer.save_pretrained(peft_model_path)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-30T13:19:53.566226Z","iopub.status.idle":"2024-11-30T13:19:53.566521Z","shell.execute_reply.started":"2024-11-30T13:19:53.566374Z","shell.execute_reply":"2024-11-30T13:19:53.566388Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<img src=\"data:image/svg+xml;base64,Cjxzdmcgd2lkdGg9IjgwMCIgaGVpZ2h0PSI1MCIgdmlld0JveD0iMCAwIDgwMCA1MCIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KICAgIDxkZWZzPgogICAgICAgIDxsaW5lYXJHcmFkaWVudCBpZD0iZmFkZUdyYWRpZW50IiB4MT0iMCIgeDI9IjEiPgogICAgICAgICAgICA8c3RvcCBvZmZzZXQ9IjAlIiBzdG9wLWNvbG9yPSIjRjBGMEYwIi8+CiAgICAgICAgICAgIDxzdG9wIG9mZnNldD0iMTAwJSIgc3RvcC1jb2xvcj0iI0YwRjBGMCIgc3RvcC1vcGFjaXR5PSIwIi8+CiAgICAgICAgPC9saW5lYXJHcmFkaWVudD4KICAgICAgICA8bWFzayBpZD0iZmFkZU1hc2siPgogICAgICAgICAgICA8cmVjdCB4PSIwIiB5PSIwIiB3aWR0aD0iNzUwIiBoZWlnaHQ9IjUwIiBmaWxsPSJ3aGl0ZSIvPgogICAgICAgICAgICA8cmVjdCB4PSI3NTAiIHk9IjAiIHdpZHRoPSI1MCIgaGVpZ2h0PSI1MCIgZmlsbD0idXJsKCNmYWRlR3JhZGllbnQpIi8+CiAgICAgICAgPC9tYXNrPgogICAgPC9kZWZzPgogICAgPHBhdGggZD0iTTI1LDUwIFEwLDUwIDAsMjUgTDUwLDMgTDk3LDI1IEw3OTcsMjUgTDc5Nyw1MCBMMjUsNTAgWiIgZmlsbD0iI0YwRjBGMCIgc3Ryb2tlPSIjRTBFMEUwIiBzdHJva2Utd2lkdGg9IjEiIG1hc2s9InVybCgjZmFkZU1hc2spIi8+Cjwvc3ZnPgo=\" alt=\"Time alert close\"/>","metadata":{"tags":[]}},{"cell_type":"markdown","source":"That training was performed on a subset of data. To load a fully trained PEFT model, read a checkpoint of a PEFT model from S3.","metadata":{"tags":[]}},{"cell_type":"code","source":"# !aws s3 cp --recursive s3://dlai-generative-ai/models/peft-dialogue-summary-checkpoint/ /kaggle/working/peft-dialogue-summary-checkpoint-from-s3/ ","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-30T13:19:53.568026Z","iopub.status.idle":"2024-11-30T13:19:53.568336Z","shell.execute_reply.started":"2024-11-30T13:19:53.568197Z","shell.execute_reply":"2024-11-30T13:19:53.568212Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Check that the size of this model is much less than the original LLM:","metadata":{"tags":[]}},{"cell_type":"code","source":"# !ls -al /kaggle/working/peft-dialogue-summary-checkpoint-from-s3/adapter_model.bin","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-30T13:19:53.569423Z","iopub.status.idle":"2024-11-30T13:19:53.569706Z","shell.execute_reply.started":"2024-11-30T13:19:53.569570Z","shell.execute_reply":"2024-11-30T13:19:53.569584Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Prepare this model by adding an adapter to the original FLAN-T5 model. You are setting `is_trainable=False` because the plan is only to perform inference with this PEFT model. If you were preparing the model for further training, you would set `is_trainable=True`.","metadata":{"tags":[]}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from peft import PeftModel, PeftConfig\n\npeft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"/kaggle/input/flan-t5/pytorch/base/4\", torch_dtype=torch.bfloat16)\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/flan-t5/pytorch/base/4\")\n\npeft_model = PeftModel.from_pretrained(peft_model_base, \n                                       '/kaggle/input/generative-ai-with-llms-lab-2/lab_2/peft-dialogue-summary-checkpoint-from-s3', \n                                       torch_dtype=torch.bfloat16,\n                                       is_trainable=False)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-30T13:19:53.571027Z","iopub.status.idle":"2024-11-30T13:19:53.571333Z","shell.execute_reply.started":"2024-11-30T13:19:53.571194Z","shell.execute_reply":"2024-11-30T13:19:53.571208Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The number of trainable parameters will be `0` due to `is_trainable=False` setting:","metadata":{"tags":[]}},{"cell_type":"code","source":"print(print_number_of_trainable_model_parameters(peft_model))","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-30T13:19:53.572403Z","iopub.status.idle":"2024-11-30T13:19:53.572693Z","shell.execute_reply.started":"2024-11-30T13:19:53.572558Z","shell.execute_reply":"2024-11-30T13:19:53.572572Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a name='3.3'></a>\n### 3.3 - Evaluate the Model Qualitatively (Human Evaluation)\n\nMake inferences for the same example as in sections [1.3](#1.3) and [2.3](#2.3), with the original model, fully fine-tuned and PEFT model.","metadata":{}},{"cell_type":"code","source":"peft_model = peft_model.to('cpu')\ninstruct_model = instruct_model.to('cpu')\noriginal_model = original_model.to('cpu')","metadata":{"execution":{"iopub.status.busy":"2024-11-30T13:19:53.574403Z","iopub.status.idle":"2024-11-30T13:19:53.574813Z","shell.execute_reply.started":"2024-11-30T13:19:53.574601Z","shell.execute_reply":"2024-11-30T13:19:53.574622Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"index = 200\ndialogue = dataset['test'][index]['dialogue']\nbaseline_human_summary = dataset['test'][index]['summary']\n\nprompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary: \"\"\"\n\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\noriginal_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\noriginal_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n\ninstruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\ninstruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n\npeft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\npeft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n\nprint(dash_line)\nprint(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\nprint(dash_line)\nprint(f'ORIGINAL MODEL:\\n{original_model_text_output}')\nprint(dash_line)\nprint(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')\nprint(dash_line)\nprint(f'PEFT MODEL: {peft_model_text_output}')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-30T13:19:53.576027Z","iopub.status.idle":"2024-11-30T13:19:53.576520Z","shell.execute_reply.started":"2024-11-30T13:19:53.576282Z","shell.execute_reply":"2024-11-30T13:19:53.576304Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a name='3.4'></a>\n### 3.4 - Evaluate the Model Quantitatively (with ROUGE Metric)\nPerform inferences for the sample of the test dataset (only 10 dialogues and summaries to save time). ","metadata":{}},{"cell_type":"code","source":"dialogues = dataset['test'][0:10]['dialogue']\nhuman_baseline_summaries = dataset['test'][0:10]['summary']\n\noriginal_model_summaries = []\ninstruct_model_summaries = []\npeft_model_summaries = []\n\nfor idx, dialogue in enumerate(dialogues):\n    prompt = f\"\"\"\nSummarize the following conversation.\n\n{dialogue}\n\nSummary: \"\"\"\n    \n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\n    human_baseline_text_output = human_baseline_summaries[idx]\n    \n    original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n\n    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n\n    peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n\n    original_model_summaries.append(original_model_text_output)\n    instruct_model_summaries.append(instruct_model_text_output)\n    peft_model_summaries.append(peft_model_text_output)\n\nzipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries, peft_model_summaries))\n \ndf = pd.DataFrame(zipped_summaries, columns = ['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries', 'peft_model_summaries'])\ndf","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-30T13:19:53.578150Z","iopub.status.idle":"2024-11-30T13:19:53.578554Z","shell.execute_reply.started":"2024-11-30T13:19:53.578344Z","shell.execute_reply":"2024-11-30T13:19:53.578364Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"raw","source":"Compute ROUGE score for this subset of the data. ","metadata":{}},{"cell_type":"code","source":"rouge = evaluate.load('rouge')\n\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries[0:len(original_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\ninstruct_model_results = rouge.compute(\n    predictions=instruct_model_summaries,\n    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\npeft_model_results = rouge.compute(\n    predictions=peft_model_summaries,\n    references=human_baseline_summaries[0:len(peft_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('INSTRUCT MODEL:')\nprint(instruct_model_results)\nprint('PEFT MODEL:')\nprint(peft_model_results)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-30T13:19:53.579814Z","iopub.status.idle":"2024-11-30T13:19:53.580072Z","shell.execute_reply.started":"2024-11-30T13:19:53.579947Z","shell.execute_reply":"2024-11-30T13:19:53.579959Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Notice, that PEFT model results are not too bad, while the training process was much easier!","metadata":{}},{"cell_type":"markdown","source":"You already computed ROUGE score on the full dataset, after loading the results from the `data/dialogue-summary-training-results.csv` file. Load the values for the PEFT model now and check its performance compared to other models.","metadata":{}},{"cell_type":"code","source":"human_baseline_summaries = results['human_baseline_summaries'].values\noriginal_model_summaries = results['original_model_summaries'].values\ninstruct_model_summaries = results['instruct_model_summaries'].values\npeft_model_summaries     = results['peft_model_summaries'].values\n\noriginal_model_results = rouge.compute(\n    predictions=original_model_summaries,\n    references=human_baseline_summaries[0:len(original_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\ninstruct_model_results = rouge.compute(\n    predictions=instruct_model_summaries,\n    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\npeft_model_results = rouge.compute(\n    predictions=peft_model_summaries,\n    references=human_baseline_summaries[0:len(peft_model_summaries)],\n    use_aggregator=True,\n    use_stemmer=True,\n)\n\nprint('ORIGINAL MODEL:')\nprint(original_model_results)\nprint('INSTRUCT MODEL:')\nprint(instruct_model_results)\nprint('PEFT MODEL:')\nprint(peft_model_results)","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-30T13:19:53.581254Z","iopub.status.idle":"2024-11-30T13:19:53.581514Z","shell.execute_reply.started":"2024-11-30T13:19:53.581386Z","shell.execute_reply":"2024-11-30T13:19:53.581398Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The results show less of an improvement over full fine-tuning, but the benefits of PEFT typically outweigh the slightly-lower performance metrics.\n\nCalculate the improvement of PEFT over the original model:","metadata":{}},{"cell_type":"code","source":"print(\"Absolute percentage improvement of PEFT MODEL over HUMAN BASELINE\")\n\nimprovement = (np.array(list(peft_model_results.values())) - np.array(list(original_model_results.values())))\nfor key, value in zip(peft_model_results.keys(), improvement):\n    print(f'{key}: {value*100:.2f}%')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-30T13:19:53.582586Z","iopub.status.idle":"2024-11-30T13:19:53.582877Z","shell.execute_reply.started":"2024-11-30T13:19:53.582743Z","shell.execute_reply":"2024-11-30T13:19:53.582756Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now calculate the improvement of PEFT over a full fine-tuned model:","metadata":{}},{"cell_type":"code","source":"print(\"Absolute percentage improvement of PEFT MODEL over INSTRUCT MODEL\")\n\nimprovement = (np.array(list(peft_model_results.values())) - np.array(list(instruct_model_results.values())))\nfor key, value in zip(peft_model_results.keys(), improvement):\n    print(f'{key}: {value*100:.2f}%')","metadata":{"tags":[],"execution":{"iopub.status.busy":"2024-11-30T13:19:53.583829Z","iopub.status.idle":"2024-11-30T13:19:53.584152Z","shell.execute_reply.started":"2024-11-30T13:19:53.583987Z","shell.execute_reply":"2024-11-30T13:19:53.584001Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here you see a small percentage decrease in the ROUGE metrics vs. full fine-tuned. However, the training requires much less computing and memory resources (often just a single GPU).","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Source:\n - https://www.coursera.org/learn/generative-ai-with-llms\n - https://www.coursera.org/learn/generative-ai-with-llms/gradedLti/x0gc1/lab-2-fine-tune-a-generative-ai-model-for-dialogue-summarization\n \nhttps://creativecommons.org/licenses/by-sa/2.0/legalcode","metadata":{}}]}