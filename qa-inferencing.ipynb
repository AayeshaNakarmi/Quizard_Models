{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10063570,"sourceType":"datasetVersion","datasetId":6201861},{"sourceId":10203147,"sourceType":"datasetVersion","datasetId":6305330},{"sourceId":85986,"sourceType":"modelInstanceVersion","modelInstanceId":72246,"modelId":78150},{"sourceId":178402,"sourceType":"modelInstanceVersion","modelInstanceId":151979,"modelId":174433}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import keras_nlp\n\n# Load the model architecture\ngemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma2_instruct_2b_en\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-25T11:35:21.400412Z","iopub.execute_input":"2024-12-25T11:35:21.400668Z","iopub.status.idle":"2024-12-25T11:37:01.264215Z","shell.execute_reply.started":"2024-12-25T11:35:21.400631Z","shell.execute_reply":"2024-12-25T11:37:01.263237Z"}},"outputs":[{"name":"stderr","text":"normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Path to your weights file\nweights_path = \"/kaggle/input/gemma-quizard-weights/keras/default/1/QA-Gemma-Quizard.weights.h5\"\n\n# Load the weights\ngemma_lm.load_weights(weights_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T11:37:01.266572Z","iopub.execute_input":"2024-12-25T11:37:01.267126Z","iopub.status.idle":"2024-12-25T11:38:08.212853Z","shell.execute_reply.started":"2024-12-25T11:37:01.267093Z","shell.execute_reply":"2024-12-25T11:38:08.211808Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 210 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T11:38:08.214425Z","iopub.execute_input":"2024-12-25T11:38:08.215336Z","iopub.status.idle":"2024-12-25T11:38:12.464857Z","shell.execute_reply.started":"2024-12-25T11:38:08.215284Z","shell.execute_reply":"2024-12-25T11:38:12.464026Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"sampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\ngemma_lm.compile(sampler=sampler)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T11:38:12.466315Z","iopub.execute_input":"2024-12-25T11:38:12.466943Z","iopub.status.idle":"2024-12-25T11:38:12.480881Z","shell.execute_reply.started":"2024-12-25T11:38:12.466904Z","shell.execute_reply":"2024-12-25T11:38:12.479761Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"template = \"\"\"Instruction: Generate an answer to the question using the provided context.  Provide the answer only without including any additional content.\"\n\nContext:\n{Context}\n\nQuestion:\n{Question}\n\nResponse:\n{Answer}\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T11:38:12.482595Z","iopub.execute_input":"2024-12-25T11:38:12.483133Z","iopub.status.idle":"2024-12-25T11:38:12.488521Z","shell.execute_reply.started":"2024-12-25T11:38:12.483086Z","shell.execute_reply":"2024-12-25T11:38:12.487476Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"prompt = template.format(\n    Context=\"The Great Barrier Reef is the world's largest coral reef system, located in the Coral Sea, off the coast of Queensland, Australia. It is composed of over 2,900 individual reefs and 900 islands stretching over 2,300 kilometers. The reef is known for its biodiversity, hosting countless marine species, and is a popular destination for snorkeling and diving enthusiasts. However, it faces threats from climate change, overfishing, and pollution.\",\n    Question=\"What are some of the major threats faced by the Great Barrier Reef?\",\n    Answer=\"\"\n)\n\nsampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\ngemma_lm.compile(sampler=sampler)\nprint(gemma_lm.generate(prompt, max_length=256))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T11:38:12.489923Z","iopub.execute_input":"2024-12-25T11:38:12.490240Z","iopub.status.idle":"2024-12-25T11:39:08.993321Z","shell.execute_reply.started":"2024-12-25T11:38:12.490210Z","shell.execute_reply":"2024-12-25T11:39:08.992218Z"}},"outputs":[{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1735126711.185662      79 service.cc:145] XLA service 0x7e2c1ee06950 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1735126711.187009      79 service.cc:153]   StreamExecutor device (0): Host, Default Version\nI0000 00:00:1735126711.603908      79 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"Instruction: Generate an answer to the question using the provided context.  Provide the answer only without including any additional content.\"\n\nContext:\nThe Great Barrier Reef is the world's largest coral reef system, located in the Coral Sea, off the coast of Queensland, Australia. It is composed of over 2,900 individual reefs and 900 islands stretching over 2,300 kilometers. The reef is known for its biodiversity, hosting countless marine species, and is a popular destination for snorkeling and diving enthusiasts. However, it faces threats from climate change, overfishing, and pollution.\n\nQuestion:\nWhat are some of the major threats faced by the Great Barrier Reef?\n\nResponse:\n\nThe major threats to the Great Barrier Reef include climate change, overfishing, and pollution, which have led to coral bleaching and habitat degradation, impacting its biodiversity and ecological balance.\n\n\"\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"prompt = template.format(\n    Context=\"After training a machine translation model on a large dataset, the BLEU score was calculated to evaluate the quality of the model's translations compared to a set of human-generated reference translations. The model's output was compared to the reference texts using n-gram overlap, and a brevity penalty was applied to avoid overly short translations. The BLEU score was found to be 0.75, which indicates that the model's translations closely align with the reference texts.\",\n    Question=\"What is the BLEU score of the model's output as calculated during evaluation?\",\n    Answer=\"\"\n)\n\nsampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\ngemma_lm.compile(sampler=sampler)\nprint(gemma_lm.generate(prompt, max_length=256))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-25T11:39:08.996485Z","iopub.execute_input":"2024-12-25T11:39:08.996965Z","iopub.status.idle":"2024-12-25T11:39:56.690693Z","shell.execute_reply.started":"2024-12-25T11:39:08.996933Z","shell.execute_reply":"2024-12-25T11:39:56.689582Z"}},"outputs":[{"name":"stdout","text":"Instruction: Generate an answer to the question using the provided context.  Provide the answer only without including any additional content.\"\n\nContext:\nAfter training a machine translation model on a large dataset, the BLEU score was calculated to evaluate the quality of the model's translations compared to a set of human-generated reference translations. The model's output was compared to the reference texts using n-gram overlap, and a brevity penalty was applied to avoid overly short translations. The BLEU score was found to be 0.75, which indicates that the model's translations closely align with the reference texts.\n\nQuestion:\nWhat is the BLEU score of the model's output as calculated during evaluation?\n\nResponse:\n\nThe BLEU score was 0.75, indicating that the model's translations were reasonably accurate and aligned with the reference texts.\\\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\n# Retrieve the Hugging Face API token from Kaggle secrets\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T11:03:10.721494Z","iopub.execute_input":"2024-12-17T11:03:10.721946Z","iopub.status.idle":"2024-12-17T11:03:10.937239Z","shell.execute_reply.started":"2024-12-17T11:03:10.721908Z","shell.execute_reply":"2024-12-17T11:03:10.935939Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"!pip install PyPDF2 sentence-transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T11:03:10.938600Z","iopub.execute_input":"2024-12-17T11:03:10.938959Z","iopub.status.idle":"2024-12-17T11:03:24.359882Z","shell.execute_reply.started":"2024-12-17T11:03:10.938919Z","shell.execute_reply":"2024-12-17T11:03:24.357950Z"}},"outputs":[{"name":"stdout","text":"Collecting PyPDF2\n  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\nCollecting sentence-transformers\n  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.45.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.4.0+cpu)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.14.1)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.25.1)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (10.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.5.15)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.20.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: PyPDF2, sentence-transformers\nSuccessfully installed PyPDF2-3.0.1 sentence-transformers-3.3.1\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import PyPDF2\nimport re\nimport nltk\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sentence_transformers import SentenceTransformer\nimport numpy as np\nimport keras_nlp\n\n# Download the Punkt tokenizer for sentence splitting (only need to run once)\nnltk.download('punkt')\n\n# Function to extract text from PDF\ndef extract_text_from_pdf(pdf_path):\n    try:\n        with open(pdf_path, 'rb') as file:\n            reader = PyPDF2.PdfReader(file)\n            text = \"\"\n            for page_num in range(len(reader.pages)):\n                page = reader.pages[page_num]\n                text += page.extract_text() or \"\"\n        return text\n    except Exception as e:\n        print(f\"Error extracting text from PDF: {e}\")\n        return \"\"\n\n# Function to exclude references section\ndef exclude_references(text):\n    match = re.search(r'(References|Bibliography|Works Cited)', text, re.IGNORECASE)\n    if match:\n        text = text[:match.start()]\n    return text\n\n# Function to split text into sentences\ndef split_text_into_sentences(text):\n    return nltk.tokenize.sent_tokenize(text)\n\n# Load the pre-trained model and tokenizer for question generation\nquestion_model_name = \"aayeshanakarmi/T5-QG-finetuned-squad\"\nquestion_tokenizer = AutoTokenizer.from_pretrained(question_model_name, use_auth_token=hf_token)\nquestion_model = AutoModelForSeq2SeqLM.from_pretrained(question_model_name, use_auth_token=hf_token)\n\n# Load SentenceTransformer model for semantic similarity\nsentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Load the Gemma model for answer generation\ngemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma2_instruct_2b_en\")\nweights_path = \"/kaggle/input/gemma-quizard-weights/keras/default/1/QA-Gemma-Quizard.weights.h5\"  # Update to actual path\ngemma_lm.load_weights(weights_path)\n\n# Configure the sampler for the Gemma model\nsampler = keras_nlp.samplers.TopKSampler(k=5, seed=2)\ngemma_lm.compile(sampler=sampler)\n\n# Template for answer generation\nanswer_template = \"\"\"Instruction: Generate an answer to the question using the provided context. Provide the answer only without including any additional content.\n\nContext:\n{Context}\n\nQuestion:\n{Question}\n\nResponse:\n\"\"\"\n\n# Function to generate questions from a group of sentences\ndef generate_questions_from_group(sentence_group):\n    paragraph = \" \".join(sentence_group)\n    inputs = question_tokenizer(paragraph, return_tensors=\"pt\", truncation=True, max_length=512)\n    outputs = question_model.generate(inputs['input_ids'], max_length=150, num_beams=4, early_stopping=True)\n    question = question_tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    if not question.strip():\n        question = \"No valid question generated.\"\n    \n    return question\n\n# Function to generate answers using the Gemma model\ndef generate_answer_with_gemma(context, question):\n    prompt = answer_template.format(Context=context, Question=question)\n    response = gemma_lm.generate(prompt, max_length=256)\n    return response  # Return the generated answer\n\n# Function to generate unique questions and answers\ndef generate_unique_questions_and_answers(sentences):\n    # Create embeddings for all sentences once\n    embeddings = sentence_model.encode(sentences)\n    qa_pairs = []\n\n    # Group sentences based on similarity and generate questions and answers\n    for i in range(len(embeddings)):\n        current_group = [sentences[i]]\n        for j in range(i + 1, len(embeddings)):\n            similarity = cosine_similarity([embeddings[i]], [embeddings[j]])[0][0]\n            if similarity > 0.7:\n                current_group.append(sentences[j])\n        \n        # Generate question and answer if the group is large enough\n        if len(current_group) > 1:\n            question = generate_questions_from_group(current_group)\n            context = \" \".join(current_group)\n            answer = generate_answer_with_gemma(context, question)\n            qa_pairs.append((question, answer, context))\n    \n    return qa_pairs\n\n# Example usage\npdf_path = \"/kaggle/input/attention-is-all-you-need/7181-attention-is-all-you-need.pdf\"\n\n# Step 1: Extract text from PDF\npdf_text = extract_text_from_pdf(pdf_path)\n\n# Step 2: Exclude the references section\npdf_text = exclude_references(pdf_text)\n\n# Step 3: Split the text into sentences\nsentences = split_text_into_sentences(pdf_text)\n\n# Step 4: Generate unique questions and answers based on context relevance\nqa_pairs = generate_unique_questions_and_answers(sentences)\n\n# Step 5: Save generated questions and answers to a file or print them\noutput_file = \"/kaggle/working/generated_questions_and_answers_with_gemma.txt\"\nwith open(output_file, \"w\") as f:\n    for i, (question, answer, context) in enumerate(qa_pairs):\n        f.write(f\"Question {i+1}: {question}\\n\")\n        f.write(f\"Answer {i+1}: {answer}\\n\")\n        f.write(f\"Context {i+1}: {context}\\n\\n\")\n\nprint(f\"{len(qa_pairs)} question-answer pairs generated and saved to '{output_file}'.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T08:08:39.878347Z","iopub.execute_input":"2024-12-17T08:08:39.878771Z","iopub.status.idle":"2024-12-17T08:13:35.512225Z","shell.execute_reply.started":"2024-12-17T08:08:39.878731Z","shell.execute_reply":"2024-12-17T08:13:35.508729Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:796: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/20.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9e88810322f4defb5d00c628f0f648f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74efb5ba5873490c87cf86b82a9b0cab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"facf4fd0add54bedbe28a7f315d07e57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d5fa0b914394f9b814a3a851b327352"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.50k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"554a4a409d9841d782c2ead6d4fb0f28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eff9601317ed4e07ac5fa4fa06476dd9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f5a8fe3adf44076a471240cb6fd79ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb1c620077aa4a259bdf2c4441b396df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b6365f876694628b53257a717ad3a58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11ce041f532849528a8e7f17d598c8af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"914cf08eef94446491931890a0c874dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d3b35f080044634a794942fbf49cd7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b705d0d51fbf4720ace03eb32c5fde26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bffe775f29604f69b9522480b9612b7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b75cabd01a79485a80e96d2df514ef48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d63c809dbde4fd5a3e5a27d385346f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d7b541afb1c4082a63297f204c9109e"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44fa8c46f1c341d1a312ca838a9287c6"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 210 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91629e3b20a14dbdab5c71914841d5ed"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 121\u001b[0m\n\u001b[1;32m    118\u001b[0m sentences \u001b[38;5;241m=\u001b[39m split_text_into_sentences(pdf_text)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Step 4: Generate unique questions and answers based on context relevance\u001b[39;00m\n\u001b[0;32m--> 121\u001b[0m qa_pairs \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_unique_questions_and_answers\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Step 5: Save generated questions and answers to a file or print them\u001b[39;00m\n\u001b[1;32m    124\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/working/generated_questions_and_answers_with_gemma.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n","Cell \u001b[0;32mIn[11], line 103\u001b[0m, in \u001b[0;36mgenerate_unique_questions_and_answers\u001b[0;34m(sentences)\u001b[0m\n\u001b[1;32m    101\u001b[0m         question \u001b[38;5;241m=\u001b[39m generate_questions_from_group(current_group)\n\u001b[1;32m    102\u001b[0m         context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(current_group)\n\u001b[0;32m--> 103\u001b[0m         answer \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_answer_with_gemma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m         qa_pairs\u001b[38;5;241m.\u001b[39mappend((question, answer, context))\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m qa_pairs\n","Cell \u001b[0;32mIn[11], line 82\u001b[0m, in \u001b[0;36mgenerate_answer_with_gemma\u001b[0;34m(context, question)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_answer_with_gemma\u001b[39m(context, question):\n\u001b[1;32m     81\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m answer_template\u001b[38;5;241m.\u001b[39mformat(Context\u001b[38;5;241m=\u001b[39mcontext, Question\u001b[38;5;241m=\u001b[39mquestion)\n\u001b[0;32m---> 82\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mgemma_lm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras_nlp/src/models/causal_lm.py:360\u001b[0m, in \u001b[0;36mCausalLM.generate\u001b[0;34m(self, inputs, max_length, stop_token_ids)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m [preprocess(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[0;32m--> 360\u001b[0m outputs \u001b[38;5;241m=\u001b[39m [generate(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    362\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [postprocess(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m outputs]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras_nlp/src/models/causal_lm.py:360\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m [preprocess(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[0;32m--> 360\u001b[0m outputs \u001b[38;5;241m=\u001b[39m [\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    362\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [postprocess(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m outputs]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras_nlp/src/models/causal_lm.py:350\u001b[0m, in \u001b[0;36mCausalLM.generate.<locals>.generate\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(x):\n\u001b[0;32m--> 350\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgenerate_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_token_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop_token_ids\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1515\u001b[0m   )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":11},{"cell_type":"code","source":"!pip install torch transformers optimum","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T11:50:32.099933Z","iopub.execute_input":"2024-12-14T11:50:32.100607Z","iopub.status.idle":"2024-12-14T11:50:44.306012Z","shell.execute_reply.started":"2024-12-14T11:50:32.100549Z","shell.execute_reply":"2024-12-14T11:50:44.303761Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0+cpu)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: optimum in /opt/conda/lib/python3.10/site-packages (1.23.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: coloredlogs in /opt/conda/lib/python3.10/site-packages (from optimum) (15.0.1)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from optimum) (3.0.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: humanfriendly>=9.1 in /opt/conda/lib/python3.10/site-packages (from coloredlogs->optimum) (10.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (17.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (2.2.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (0.70.16)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->optimum) (3.9.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->optimum) (4.0.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->optimum) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->optimum) (1.16.0)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# import torch\n# from transformers import AutoTokenizer, AutoModelForCausalLM\n# from optimum.quantization import QuantizationConfig, quantize\n\n# # Option 1: 4-bit Quantization\n# def load_quantized_gemma(model_name=\"google/gemma-2b-it\"):\n#     # Load tokenizer\n#     tokenizer = AutoTokenizer.from_pretrained(model_name)\n    \n#     # Quantization configuration\n#     quantization_config = QuantizationConfig(\n#         load_in_4bit=True,\n#         bnb_4bit_compute_dtype=torch.float16,\n#         bnb_4bit_quant_type=\"nf4\",\n#         bnb_4bit_use_double_quant=True\n#     )\n    \n#     # Load model with quantization\n#     model = AutoModelForCausalLM.from_pretrained(\n#         model_name,\n#         quantization_config=quantization_config,\n#         device_map=\"auto\"  # Automatic device placement\n#     )\n    \n#     return model, tokenizer\n\n# # Option 2: 8-bit Quantization\n# def load_8bit_quantized_gemma(model_name=\"google/gemma-2b-it\"):\n#     # Load tokenizer\n#     tokenizer = AutoTokenizer.from_pretrained(model_name)\n    \n#     # Load model in 8-bit\n#     model = AutoModelForCausalLM.from_pretrained(\n#         model_name,\n#         load_in_8bit=True,\n#         device_map=\"auto\"\n#     )\n    \n#     return model, tokenizer\n\n# # Option 3: Dynamic Quantization (works on CPU)\n# def load_dynamically_quantized_gemma(model_name=\"google/gemma-2b-it\"):\n#     # Load tokenizer\n#     tokenizer = AutoTokenizer.from_pretrained(model_name)\n    \n#     # Load full precision model\n#     model = AutoModelForCausalLM.from_pretrained(model_name)\n    \n#     # Apply dynamic quantization\n#     model = torch.quantization.quantize_dynamic(\n#         model, \n#         {torch.nn.Linear},  # Quantize linear layers\n#         dtype=torch.qint8\n#     )\n    \n#     return model, tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T12:07:40.237745Z","iopub.execute_input":"2024-12-14T12:07:40.238531Z","iopub.status.idle":"2024-12-14T12:07:40.247567Z","shell.execute_reply.started":"2024-12-14T12:07:40.238470Z","shell.execute_reply":"2024-12-14T12:07:40.246057Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import os\nimport gc\nimport csv\nimport re\nimport nltk\nimport torch\nimport numpy as np\nimport pandas as pd\nimport PyPDF2\nfrom tqdm import tqdm\n\n# Memory-efficient imports\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Download NLTK resources\nnltk.download('punkt', quiet=True)\n\nclass MemoryEfficientQAGenerator:\n    def __init__(self, \n                 pdf_path, \n                 question_model_name=\"aayeshanakarmi/T5-QG-finetuned-squad\", \n                 sentence_model_name='all-MiniLM-L6-v2'):\n        # Initialize with minimal memory usage\n        self.pdf_path = pdf_path\n        \n        # Use CPU to reduce memory overhead\n        self.device = torch.device('cpu')\n        \n        # Load models with minimal memory\n        self.question_tokenizer = AutoTokenizer.from_pretrained(\n            question_model_name, \n            low_cpu_mem_usage=True\n        )\n        self.question_model = AutoModelForSeq2SeqLM.from_pretrained(\n            question_model_name, \n            low_cpu_mem_usage=True,\n            device_map='cpu'\n        )\n        \n        # Lightweight sentence transformer\n        self.sentence_model = SentenceTransformer(\n            sentence_model_name, \n            device=str(self.device)\n        )\n        \n        # Memory-efficient configurations\n        self.max_sentences = 500  # Limit sentences to process\n        self.similarity_threshold = 0.6  # Adjustable similarity threshold\n    \n    def extract_text_from_pdf(self):\n        \"\"\"Memory-efficient PDF text extraction\"\"\"\n        try:\n            with open(self.pdf_path, 'rb') as file:\n                reader = PyPDF2.PdfReader(file)\n                text = \"\"\n                for page in reader.pages:\n                    text += page.extract_text() or \"\"\n                return self._preprocess_text(text)\n        except Exception as e:\n            print(f\"PDF extraction error: {e}\")\n            return \"\"\n    \n    def _preprocess_text(self, text):\n        \"\"\"Advanced text preprocessing\"\"\"\n        # Remove references section\n        ref_pattern = r'(References|Bibliography|Works Cited).*'\n        text = re.sub(ref_pattern, '', text, flags=re.IGNORECASE | re.DOTALL)\n        \n        # Additional cleaning\n        text = re.sub(r'\\s+', ' ', text).strip()\n        return text\n    \n    def split_sentences(self, text):\n        \"\"\"Memory-efficient sentence splitting\"\"\"\n        sentences = nltk.sent_tokenize(text)\n        return sentences[:self.max_sentences]  # Limit sentences\n    \n    def generate_questions(self, sentence_group):\n        \"\"\"Generate questions with memory constraints\"\"\"\n        try:\n            paragraph = \" \".join(sentence_group)\n            inputs = self.question_tokenizer(\n                paragraph, \n                return_tensors=\"pt\", \n                truncation=True, \n                max_length=512\n            )\n            \n            outputs = self.question_model.generate(\n                inputs['input_ids'], \n                max_length=150, \n                num_beams=4, \n                early_stopping=True\n            )\n            \n            question = self.question_tokenizer.decode(\n                outputs[0], \n                skip_special_tokens=True\n            )\n            \n            return question.strip() or \"No valid question generated.\"\n        except Exception as e:\n            print(f\"Question generation error: {e}\")\n            return \"Question generation failed.\"\n    \n    def generate_context_qa_pairs(self, sentences):\n        \"\"\"Generate context-based Q&A pairs with memory efficiency\"\"\"\n        # Create embeddings with controlled memory\n        embeddings = self.sentence_model.encode(\n            sentences, \n            batch_size=16,  # Controlled batch processing\n            show_progress_bar=True\n        )\n        \n        qa_pairs = []\n        processed_indices = set()\n        \n        for i in tqdm(range(len(embeddings)), desc=\"Generating Q&A Pairs\"):\n            if i in processed_indices:\n                continue\n            \n            current_group = [sentences[i]]\n            current_embedding = embeddings[i]\n            \n            # Find similar sentences\n            for j in range(i + 1, len(embeddings)):\n                if j in processed_indices:\n                    continue\n                \n                similarity = cosine_similarity(\n                    [current_embedding], \n                    [embeddings[j]]\n                )[0][0]\n                \n                if similarity > self.similarity_threshold:\n                    current_group.append(sentences[j])\n                    processed_indices.add(j)\n            \n            # Process groups with multiple sentences\n            if len(current_group) > 1:\n                question = self.generate_questions(current_group)\n                context = \" \".join(current_group)\n                qa_pairs.append((question, context))\n                \n                # Free up memory\n                del current_group, context, question\n                gc.collect()\n        \n        return qa_pairs\n    \n    def safe_write_csv(self, qa_pairs, output_path='qa_pairs.csv'):\n        \"\"\"Safely write Q&A pairs to CSV with robust error handling\"\"\"\n        try:\n            # Use csv module for more control\n            with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:\n                csv_writer = csv.writer(\n                    csvfile, \n                    quoting=csv.QUOTE_ALL,  # Quote all fields\n                    escapechar='\\\\',  # Escape problematic characters\n                    delimiter=','\n                )\n                \n                # Write header\n                csv_writer.writerow(['Question', 'Context'])\n                \n                # Write data rows\n                for question, context in tqdm(qa_pairs, desc=\"Writing to CSV\"):\n                    # Sanitize text to remove any problematic characters\n                    safe_question = self._sanitize_text(question)\n                    safe_context = self._sanitize_text(context)\n                    \n                    csv_writer.writerow([safe_question, safe_context])\n            \n            print(f\"Successfully saved {len(qa_pairs)} Q&A pairs to {output_path}\")\n        \n        except Exception as e:\n            print(f\"Error writing to CSV: {e}\")\n            \n            # Fallback: write to text file if CSV fails\n            self._write_to_txt(qa_pairs, output_path.replace('.csv', '.txt'))\n    \n    def _sanitize_text(self, text):\n        \"\"\"Remove or escape problematic characters\"\"\"\n        # Remove non-printable characters\n        text = ''.join(char for char in text if char.isprintable())\n        \n        # Remove newlines and extra whitespace\n        text = re.sub(r'\\s+', ' ', text).strip()\n        \n        return text\n    \n    def _write_to_txt(self, qa_pairs, output_path):\n        \"\"\"Fallback method to write to text file\"\"\"\n        try:\n            with open(output_path, 'w', encoding='utf-8') as f:\n                for i, (question, context) in enumerate(qa_pairs, 1):\n                    f.write(f\"Q{i}: {question}\\n\")\n                    f.write(f\"Context{i}: {context}\\n\\n\")\n            print(f\"Fallback: Saved Q&A pairs to text file {output_path}\")\n        except Exception as e:\n            print(f\"Error writing to text file: {e}\")\n    \n    def generate_and_save_qa(self, output_path='qa_pairs.csv'):\n        \"\"\"End-to-end Q&A generation with memory management\"\"\"\n        # Extract and preprocess text\n        text = self.extract_text_from_pdf()\n        sentences = self.split_sentences(text)\n        \n        # Generate Q&A pairs\n        qa_pairs = self.generate_context_qa_pairs(sentences)\n        \n        # Save to CSV with robust error handling\n        self.safe_write_csv(qa_pairs, output_path)\n        \n        # Cleanup\n        del text, sentences, qa_pairs\n        gc.collect()\n\n# Usage example\ndef main():\n    pdf_path = \"/kaggle/input/attention-is-all-you-need/7181-attention-is-all-you-need.pdf\"\n    \n    # Memory-efficient setup\n    torch.cuda.empty_cache()\n    generator = MemoryEfficientQAGenerator(pdf_path)\n    generator.generate_and_save_qa()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T08:13:35.515822Z","iopub.status.idle":"2024-12-17T08:13:35.516264Z","shell.execute_reply.started":"2024-12-17T08:13:35.516033Z","shell.execute_reply":"2024-12-17T08:13:35.516050Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport gc\nimport csv\nimport pandas as pd\nimport keras_nlp\nimport tensorflow as tf\n\nclass MemoryEfficientQAGenerator:\n    def __init__(self, \n                 qa_csv_path, \n                 gemma_model_path,\n                 output_path='qa_pairs_with_answers101.csv',\n                 batch_size=4):\n        \"\"\"\n        Ultra memory-efficient Q&A answer generator\n        \n        :param qa_csv_path: Path to input Q&A CSV\n        :param gemma_model_path: Path to Gemma model weights\n        :param output_path: Path for output CSV\n        :param batch_size: Number of Q&A pairs to process simultaneously\n        \"\"\"\n        # Limit GPU memory growth\n        gpus = tf.config.experimental.list_physical_devices('GPU')\n        if gpus:\n            try:\n                for gpu in gpus:\n                    tf.config.experimental.set_memory_growth(gpu, True)\n            except RuntimeError as e:\n                print(f\"Memory growth setting error: {e}\")\n        \n        # Reduce TensorFlow logging\n        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n        \n        # Configuration\n        self.qa_csv_path = qa_csv_path\n        self.output_path = output_path\n        self.batch_size = batch_size\n        \n        # Load model with minimal memory footprint\n        self.model = self._load_model(gemma_model_path)\n    \n    def _load_model(self, weights_path):\n        \"\"\"\n        Load Gemma model with minimal memory usage\n        \"\"\"\n        try:\n            # Load base model architecture\n            model = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma2_instruct_2b_en\")\n            \n            # Configure sampler\n            sampler = keras_nlp.samplers.TopKSampler(k=5, seed=42)\n            model.compile(sampler=sampler)\n            \n            # Load specific weights\n            model.load_weights(weights_path)\n            \n            return model\n        except Exception as e:\n            print(f\"Model loading error: {e}\")\n            return None\n    \n    def _create_prompt(self, context, question):\n        \"\"\"\n        Create standardized prompt for answer generation\n        \"\"\"\n        return f\"\"\"Instruction: Generate a concise answer to the question using the provided context. \nProvide only the direct answer without additional explanation.\n\nContext:\n{context}\n\nQuestion:\n{question}\n\nAnswer:\n\"\"\"\n    \n    def generate_answers(self):\n        \"\"\"\n        Generate answers in memory-efficient batches\n        \"\"\"\n        # Read input CSV\n        try:\n            df = pd.read_csv(self.qa_csv_path)\n        except Exception as e:\n            print(f\"CSV reading error: {e}\")\n            return\n        \n        # Prepare output storage\n        results = []\n        \n        # Process in batches\n        for i in range(0, len(df), self.batch_size):\n            batch = df.iloc[i:i+self.batch_size]\n            \n            for _, row in batch.iterrows():\n                try:\n                    # Create prompt\n                    prompt = self._create_prompt(\n                        row['Context'], \n                        row['Question']\n                    )\n                    \n                    # Generate answer\n                    answer = self.model.generate(\n                        prompt, \n                        max_length=200\n                    )\n                    \n                    # Store results\n                    results.append({\n                        'Question': row['Question'],\n                        'Context': row['Context'],\n                        'Answer': answer.strip()\n                    })\n                    \n                    # Manual garbage collection\n                    gc.collect()\n                    \n                except Exception as e:\n                    print(f\"Answer generation error: {e}\")\n                    results.append({\n                        'Question': row['Question'],\n                        'Context': row['Context'],\n                        'Answer': 'Generation Failed'\n                    })\n        \n        # Create output DataFrame\n        output_df = pd.DataFrame(results)\n        \n        # Save to CSV\n        try:\n            output_df.to_csv(self.output_path, index=False)\n            print(f\"Successfully saved {len(results)} Q&A pairs\")\n        except Exception as e:\n            print(f\"CSV writing error: {e}\")\n        \n        # Final cleanup\n        del output_df\n        gc.collect()\n\ndef main():\n    # Paths configuration\n    qa_csv_path = 'qa_pairs.csv'\n    gemma_model_path = '/kaggle/input/gemma-quizard-weights/keras/default/1/QA-Gemma-Quizard.weights.h5'\n    output_path = 'qa_pairs_with_answers.csv'\n    \n    # Initialize generator\n    generator = MemoryEfficientQAGenerator(\n        qa_csv_path, \n        gemma_model_path, \n        output_path,\n        batch_size=4  # Adjust based on your memory constraints\n    )\n    \n    # Generate answers\n    generator.generate_answers()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T13:58:59.510054Z","iopub.execute_input":"2024-12-14T13:58:59.518102Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport gc\nimport csv\nimport pandas as pd\nimport keras_nlp\nimport tensorflow as tf\n\nclass MemoryEfficientQAGenerator:\n    def __init__(self, \n                 qa_csv_path, \n                 gemma_model_path,\n                 output_path='qa_pairs_with_answers.csv',\n                 batch_size=4):\n        \"\"\"\n        Ultra memory-efficient Q&A answer generator\n        \n        :param qa_csv_path: Path to input Q&A CSV\n        :param gemma_model_path: Path to Gemma model weights\n        :param output_path: Path for output CSV\n        :param batch_size: Number of Q&A pairs to process simultaneously\n        \"\"\"\n        # Limit GPU memory growth\n        gpus = tf.config.experimental.list_physical_devices('GPU')\n        if gpus:\n            try:\n                for gpu in gpus:\n                    tf.config.experimental.set_memory_growth(gpu, True)\n            except RuntimeError as e:\n                print(f\"Memory growth setting error: {e}\")\n        \n        # Reduce TensorFlow logging\n        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n        \n        # Configuration\n        self.qa_csv_path = qa_csv_path\n        self.output_path = output_path\n        self.batch_size = batch_size\n        \n        # Load model with minimal memory footprint\n        self.model = self._load_model(gemma_model_path)\n    \n    def _load_model(self, weights_path):\n        \"\"\"\n        Load Gemma model with minimal memory usage\n        \"\"\"\n        try:\n            # Load base model architecture\n            model = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma2_instruct_2b_en\")\n            \n            # Configure sampler\n            sampler = keras_nlp.samplers.TopKSampler(k=5, seed=42)\n            model.compile(sampler=sampler)\n            \n            # Load specific weights\n            model.load_weights(weights_path)\n            \n            return model\n        except Exception as e:\n            print(f\"Model loading error: {e}\")\n            return None\n    \n    def _create_prompt(self, context, question):\n        \"\"\"\n        Create standardized prompt for answer generation\n        \"\"\"\n        return f\"\"\"Instruction: Generate a concise answer to the question using the provided context. \nProvide only the direct answer without additional explanation.\n\nContext:\n{context}\n\nQuestion:\n{question}\n\nAnswer:\n\"\"\"\n    \n    def generate_answers(self):\n        \"\"\"\n        Generate answers in memory-efficient batches\n        \"\"\"\n        if not self.model:\n            print(\"Model not loaded. Aborting generation.\")\n            return\n\n        try:\n            # Read the input CSV\n            df = pd.read_csv(self.qa_csv_path)\n            if not {'Context', 'Question'}.issubset(df.columns):\n                print(\"Input CSV must contain 'Context' and 'Question' columns.\")\n                return\n        except Exception as e:\n            print(f\"CSV reading error: {e}\")\n            return\n\n        results = []\n        for i in range(0, len(df), self.batch_size):\n            batch = df.iloc[i:i + self.batch_size]\n            for _, row in batch.iterrows():\n                try:\n                    # Create the prompt\n                    prompt = self._create_prompt(row['Context'], row['Question'])\n\n                    # Log the prompt being sent to the model\n                    print(f\"\\nPrompt being sent to the model:\\n{prompt}\")g\n\n                    # Generate the answer\n                    answer = self.model.generate(prompt, max_length=200).strip()\n\n                    # Store the result\n                    results.append({\n                        'Question': row['Question'],\n                        'Context': row['Context'],\n                        'Answer': answer\n                    })\n\n                except Exception as e:\n                    print(f\"Error at batch {i}, row {_}: {e}\")\n                    results.append({\n                        'Question': row['Question'],\n                        'Context': row['Context'],\n                        'Answer': 'Generation Failed'\n                    })\n\n            # Perform garbage collection after processing the batch\n            gc.collect()\n\n        # Save results to the output CSV\n        try:\n            pd.DataFrame(results).to_csv(self.output_path, index=False)\n            print(f\"Successfully saved {len(results)} Q&A pairs.\")\n        except Exception as e:\n            print(f\"CSV writing error: {e}\")\n\ndef main():\n    # Paths configuration\n    qa_csv_path = 'qa_pairs.csv'\n    gemma_model_path = '/kaggle/input/gemma-quizard-weights/keras/default/1/QA-Gemma-Quizard.weights.h5'\n    output_path = 'qa_pairs_with_answers.csv'\n    \n    # Initialize generator\n    generator = MemoryEfficientQAGenerator(\n        qa_csv_path, \n        gemma_model_path, \n        output_path,\n        batch_size=4  # Adjust based on your memory constraints\n    )\n    \n    # Generate answers\n    generator.generate_answers()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T14:00:38.710062Z","iopub.execute_input":"2024-12-14T14:00:38.710537Z","iopub.status.idle":"2024-12-14T14:05:01.424074Z","shell.execute_reply.started":"2024-12-14T14:00:38.710493Z","shell.execute_reply":"2024-12-14T14:05:01.421654Z"}},"outputs":[{"name":"stderr","text":"normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n/opt/conda/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 210 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n","output_type":"stream"},{"name":"stdout","text":"\nPrompt being sent to the model:\nInstruction: Generate a concise answer to the question using the provided context. \nProvide only the direct answer without additional explanation.\n\nContext:\nAttention Is All You Need Ashish Vaswani Google Brain avaswani@google.comNoam Shazeer Google Brain noam@google.comNiki Parmar Google Research nikip@google.comJakob Uszkoreit Google Research usz@google.com Llion Jones Google Research llion@google.comAidan N. Gomezy University of Toronto aidan@cs.toronto.eduŁukasz Kaiser Google Brain lukaszkaiser@google.com Illia Polosukhinz illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. 3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,29]. 7 Conclusion In this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\n\nQuestion:\nWhat are the dominant sequence transduction models based on?\n\nAnswer:\n\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1734185029.455250     402 service.cc:145] XLA service 0x7ee0dafa1010 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1734185029.455343     402 service.cc:153]   StreamExecutor device (0): Host, Default Version\nI0000 00:00:1734185029.741994     402 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\nPrompt being sent to the model:\nInstruction: Generate a concise answer to the question using the provided context. \nProvide only the direct answer without additional explanation.\n\nContext:\nThe best performing models also connect the encoder and decoder through an attention mechanism. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [31, 21, 13]. Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [ 2,16]. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. 3.2.3 Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [31, 2, 8]. The encoder contains self-attention layers. Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. 3.3 Position-wise Feed-Forward Networks In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. As side beneﬁt, self-attention could yield more interpretable models.\n\nQuestion:\nWhat does the Transformer use in the decoder stack?\n\nAnswer:\n\n\nPrompt being sent to the model:\nInstruction: Generate a concise answer to the question using the provided context. \nProvide only the direct answer without additional explanation.\n\nContext:\nWe propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs such as images, audio and video.\n\nQuestion:\nWhat is the Transformer based solely on?\n\nAnswer:\n\n\nPrompt being sent to the model:\nInstruction: Generate a concise answer to the question using the provided context. \nProvide only the direct answer without additional explanation.\n\nContext:\nExperiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence lengthnis smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [31] and byte-pair [ 25] representations. 6 Results 6.1 Machine Translation On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2:0 BLEU, establishing a new state-of-the-art BLEU score of 28:4. On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41:0, outperforming all of the previously published single models, at less than 1=4the training cost of the previous state-of-the-art model. Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. For translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based on recurrent or convolutional layers.\n\nQuestion:\nWhat is the BLEU score of the big transformer model?\n\nAnswer:\n\n\nPrompt being sent to the model:\nInstruction: Generate a concise answer to the question using the provided context. \nProvide only the direct answer without additional explanation.\n\nContext:\nOur model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. 5.1 Training Data and Batching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art.\n\nQuestion:\nHow many sentence pairs did we train on the standard WMT 2014 dataset?\n\nAnswer:\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 153\u001b[0m\n\u001b[1;32m    150\u001b[0m     generator\u001b[38;5;241m.\u001b[39mgenerate_answers()\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[1], line 150\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    142\u001b[0m generator \u001b[38;5;241m=\u001b[39m MemoryEfficientQAGenerator(\n\u001b[1;32m    143\u001b[0m     qa_csv_path, \n\u001b[1;32m    144\u001b[0m     gemma_model_path, \n\u001b[1;32m    145\u001b[0m     output_path,\n\u001b[1;32m    146\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m  \u001b[38;5;66;03m# Adjust based on your memory constraints\u001b[39;00m\n\u001b[1;32m    147\u001b[0m )\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# Generate answers\u001b[39;00m\n\u001b[0;32m--> 150\u001b[0m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_answers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[1], line 108\u001b[0m, in \u001b[0;36mMemoryEfficientQAGenerator.generate_answers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPrompt being sent to the model:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Generate the answer\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Store the result\u001b[39;00m\n\u001b[1;32m    111\u001b[0m results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuestion\u001b[39m\u001b[38;5;124m'\u001b[39m: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuestion\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContext\u001b[39m\u001b[38;5;124m'\u001b[39m: row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContext\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnswer\u001b[39m\u001b[38;5;124m'\u001b[39m: answer\n\u001b[1;32m    115\u001b[0m })\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras_nlp/src/models/causal_lm.py:360\u001b[0m, in \u001b[0;36mCausalLM.generate\u001b[0;34m(self, inputs, max_length, stop_token_ids)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m [preprocess(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[0;32m--> 360\u001b[0m outputs \u001b[38;5;241m=\u001b[39m [generate(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    362\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [postprocess(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m outputs]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras_nlp/src/models/causal_lm.py:360\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m [preprocess(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[0;32m--> 360\u001b[0m outputs \u001b[38;5;241m=\u001b[39m [\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    362\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [postprocess(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m outputs]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras_nlp/src/models/causal_lm.py:350\u001b[0m, in \u001b[0;36mCausalLM.generate.<locals>.generate\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(x):\n\u001b[0;32m--> 350\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgenerate_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_token_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop_token_ids\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1515\u001b[0m   )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport gc\nimport csv\nimport pandas as pd\nimport keras_nlp\nimport tensorflow as tf\n\nclass MemoryEfficientQAGenerator:\n    def __init__(self, \n                 qa_csv_path, \n                 gemma_model_path,\n                 output_path='/kaggle/working/qa_pairs_with_answers.csv',\n                 batch_size=4):\n        # Limit GPU memory growth\n        gpus = tf.config.experimental.list_physical_devices('GPU')\n        if gpus:\n            try:\n                for gpu in gpus:\n                    tf.config.experimental.set_memory_growth(gpu, True)\n            except RuntimeError as e:\n                print(f\"Memory growth setting error: {e}\")\n        \n        # Reduce TensorFlow logging\n        os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n        \n        # Configuration\n        self.qa_csv_path = qa_csv_path\n        self.output_path = output_path\n        self.batch_size = batch_size\n        \n        # Load model with minimal memory footprint\n        self.model = self._load_model(gemma_model_path)\n    \n    def _load_model(self, weights_path):\n        \"\"\"\n        Load Gemma model with minimal memory usage\n        \"\"\"\n        try:\n            # Load base model architecture\n            model = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma2_instruct_2b_en\")\n            \n            # Configure sampler\n            sampler = keras_nlp.samplers.TopKSampler(k=5, seed=42)\n            model.compile(sampler=sampler)\n            \n            # Load specific weights\n            model.load_weights(weights_path)\n            \n            return model\n        except Exception as e:\n            print(f\"Model loading error: {e}\")\n            return None\n    \n    def _create_prompt(self, context, question):\n        \"\"\"\n        Create standardized prompt for answer generation\n        \"\"\"\n        return f\"\"\"Instruction: Generate a concise answer to the question using the provided context. \nProvide only the direct answer without additional explanation.\n\nContext:\n{context}\n\nQuestion:\n{question}\n\nAnswer:\n\"\"\"\n    \n    def generate_answers(self):\n        \"\"\"\n        Generate answers in memory-efficient batches\n        \"\"\"\n        if not self.model:\n            print(\"Model not loaded. Aborting generation.\")\n            return\n\n        try:\n            # Open the output CSV file for writing\n            with open(self.output_path, 'w', newline='', encoding='utf-8') as csvfile:\n                # Create CSV writer\n                csv_writer = csv.writer(csvfile)\n                # Write headers\n                csv_writer.writerow(['Question', 'Context', 'Answer'])\n                \n                # Read input CSV\n                df = pd.read_csv(self.qa_csv_path)\n                if not {'Context', 'Question'}.issubset(df.columns):\n                    print(\"Input CSV must contain 'Context' and 'Question' columns.\")\n                    return\n\n                # Process each row\n                for index, row in df.iterrows():\n                    try:\n                        # Create the prompt\n                        prompt = self._create_prompt(row['Context'], row['Question'])\n\n                        # Print the prompt details\n                        print(f\"\\n--- PROMPT {index + 1} ---\")\n                        print(f\"Context: {row['Context']}\")\n                        print(f\"Question: {row['Question']}\")\n\n                        # Generate the answer\n                        answer = self.model.generate(prompt, max_length=200)\n                        \n                        # Print the generated answer\n                        print(\"\\n--- ANSWER ---\")\n                        print(answer)\n                        print(\"-\" * 50)\n\n                        # Write to CSV immediately\n                        csv_writer.writerow([\n                            row['Question'], \n                            row['Context'], \n                            answer\n                        ])\n                        csvfile.flush()  # Ensure writing to disk\n\n                        # Aggressive memory clearing\n                        del answer\n                        gc.collect()\n                        tf.keras.backend.clear_session()\n\n                    except Exception as e:\n                        print(f\"Error processing row {index}: {e}\")\n                        \n                        # Write error to CSV\n                        csv_writer.writerow([\n                            row['Question'], \n                            row['Context'], \n                            'Generation Failed'\n                        ])\n                        csvfile.flush()\n\n        except Exception as e:\n            print(f\"CSV processing error: {e}\")\n\ndef main():\n    # Paths configuration\n    qa_csv_path = '/kaggle/input/qa-pairs-attention-paper/qa_pairs.csv'\n    gemma_model_path = '/kaggle/input/gemma-quizard-weights/keras/default/1/QA-Gemma-Quizard.weights.h5'\n    output_path = 'qa_pairs_with_answers.csv'\n    \n    # Initialize generator\n    generator = MemoryEfficientQAGenerator(\n        qa_csv_path, \n        gemma_model_path, \n        output_path,\n        batch_size=4  # Adjust based on your memory constraints\n    )\n    \n    # Generate answers\n    generator.generate_answers()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-17T11:03:24.364971Z","iopub.execute_input":"2024-12-17T11:03:24.365417Z","iopub.status.idle":"2024-12-17T11:06:11.511851Z","shell.execute_reply.started":"2024-12-17T11:03:24.365375Z","shell.execute_reply":"2024-12-17T11:06:11.510561Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 210 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n","output_type":"stream"},{"name":"stdout","text":"\n--- PROMPT 1 ---\nContext: Attention Is All You Need Ashish Vaswani Google Brain avaswani@google.comNoam Shazeer Google Brain noam@google.comNiki Parmar Google Research nikip@google.comJakob Uszkoreit Google Research usz@google.com Llion Jones Google Research llion@google.comAidan N. Gomezy University of Toronto aidan@cs.toronto.eduŁukasz Kaiser Google Brain lukaszkaiser@google.com Illia Polosukhinz illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. 3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,29]. 7 Conclusion In this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\nQuestion: What are the dominant sequence transduction models based on?\n\n--- ANSWER ---\nInstruction: Generate a concise answer to the question using the provided context. \nProvide only the direct answer without additional explanation.\n\nContext:\nAttention Is All You Need Ashish Vaswani Google Brain avaswani@google.comNoam Shazeer Google Brain noam@google.comNiki Parmar Google Research nikip@google.comJakob Uszkoreit Google Research usz@google.com Llion Jones Google Research llion@google.comAidan N. Gomezy University of Toronto aidan@cs.toronto.eduŁukasz Kaiser Google Brain lukaszkaiser@google.com Illia Polosukhinz illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. 3 Model Architecture Most competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,29]. 7 Conclusion In this work, we presented\n--------------------------------------------------\n\n--- PROMPT 2 ---\nContext: The best performing models also connect the encoder and decoder through an attention mechanism. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [31, 21, 13]. Attention mechanisms have become an integral part of compelling sequence modeling and transduc- tion models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [ 2,16]. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. 3.2.3 Applications of Attention in our Model The Transformer uses multi-head attention in three different ways: In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [31, 2, 8]. The encoder contains self-attention layers. Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. 3.3 Position-wise Feed-Forward Networks In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. As side beneﬁt, self-attention could yield more interpretable models.\nQuestion: What does the Transformer use in the decoder stack?\nError processing row 1: Exception encountered when calling GemmaTokenizer.call().\n\n\u001b[1m{{function_node __wrapped__SentencepieceTokenizeOp_device_/job:localhost/replica:0/task:0/device:CPU:0}} Resource localhost/_0_SentencepieceOp/N10tensorflow4text12_GLOBAL__N_121SentencepieceResourceE does not exist. [Op:SentencepieceTokenizeOp]\u001b[0m\n\nArguments received by GemmaTokenizer.call():\n  • inputs=tf.Tensor(shape=(1,), dtype=string)\n  • args=<class 'inspect._empty'>\n  • training=None\n  • kwargs=<class 'inspect._empty'>\n\n--- PROMPT 3 ---\nContext: We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying entirely on self-attention to compute representations of its input and output without using sequence- aligned RNNs or convolution. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs such as images, audio and video.\nQuestion: What is the Transformer based solely on?\nError processing row 2: Exception encountered when calling GemmaTokenizer.call().\n\n\u001b[1m{{function_node __wrapped__SentencepieceTokenizeOp_device_/job:localhost/replica:0/task:0/device:CPU:0}} Resource localhost/_0_SentencepieceOp/N10tensorflow4text12_GLOBAL__N_121SentencepieceResourceE does not exist. [Op:SentencepieceTokenizeOp]\u001b[0m\n\nArguments received by GemmaTokenizer.call():\n  • inputs=tf.Tensor(shape=(1,), dtype=string)\n  • args=<class 'inspect._empty'>\n  • training=None\n  • kwargs=<class 'inspect._empty'>\n\n--- PROMPT 4 ---\nContext: Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence lengthnis smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [31] and byte-pair [ 25] representations. 6 Results 6.1 Machine Translation On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2:0 BLEU, establishing a new state-of-the-art BLEU score of 28:4. On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41:0, outperforming all of the previously published single models, at less than 1=4the training cost of the previous state-of-the-art model. Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. For translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based on recurrent or convolutional layers.\nQuestion: What is the BLEU score of the big transformer model?\nError processing row 3: Exception encountered when calling GemmaTokenizer.call().\n\n\u001b[1m{{function_node __wrapped__SentencepieceTokenizeOp_device_/job:localhost/replica:0/task:0/device:CPU:0}} Resource localhost/_0_SentencepieceOp/N10tensorflow4text12_GLOBAL__N_121SentencepieceResourceE does not exist. [Op:SentencepieceTokenizeOp]\u001b[0m\n\nArguments received by GemmaTokenizer.call():\n  • inputs=tf.Tensor(shape=(1,), dtype=string)\n  • args=<class 'inspect._empty'>\n  • training=None\n  • kwargs=<class 'inspect._empty'>\n\n--- PROMPT 5 ---\nContext: Our model achieves 28.4 BLEU on the WMT 2014 English- to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. 5.1 Training Data and Batching We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art.\nQuestion: How many sentence pairs did we train on the standard WMT 2014 dataset?\nError processing row 4: Exception encountered when calling GemmaTokenizer.call().\n\n\u001b[1m{{function_node __wrapped__SentencepieceTokenizeOp_device_/job:localhost/replica:0/task:0/device:CPU:0}} Resource localhost/_0_SentencepieceOp/N10tensorflow4text12_GLOBAL__N_121SentencepieceResourceE does not exist. [Op:SentencepieceTokenizeOp]\u001b[0m\n\nArguments received by GemmaTokenizer.call():\n  • inputs=tf.Tensor(shape=(1,), dtype=string)\n  • args=<class 'inspect._empty'>\n  • training=None\n  • kwargs=<class 'inspect._empty'>\n\n--- PROMPT 6 ---\nContext: 1 Introduction Recurrent neural networks, long short-term memory [ 12] and gated recurrent [ 7] neural networks in particular, have been ﬁrmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [ 29,2,5]. End-to-end memory networks are based on a recurrent attention mechanism instead of sequence- aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [28]. Learning long-range dependencies is a key challenge in many sequence transduction tasks.\nQuestion: What is a key challenge in many sequence transduction tasks?\nError processing row 5: Exception encountered when calling GemmaTokenizer.call().\n\n\u001b[1m{{function_node __wrapped__SentencepieceTokenizeOp_device_/job:localhost/replica:0/task:0/device:CPU:0}} Resource localhost/_0_SentencepieceOp/N10tensorflow4text12_GLOBAL__N_121SentencepieceResourceE does not exist. [Op:SentencepieceTokenizeOp]\u001b[0m\n\nArguments received by GemmaTokenizer.call():\n  • inputs=tf.Tensor(shape=(1,), dtype=string)\n  • args=<class 'inspect._empty'>\n  • training=None\n  • kwargs=<class 'inspect._empty'>\n\n--- PROMPT 7 ---\nContext: Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. 3.2.1 Scaled Dot-Product Attention We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). We compute the dot products of the 3Scaled Dot-Product Attention Multi-Head Attention Figure 2: (left) Scaled Dot-Product Attention.\nQuestion: What did Noam call our particular attention?\nError processing row 6: Exception encountered when calling GemmaTokenizer.call().\n\n\u001b[1m{{function_node __wrapped__SentencepieceTokenizeOp_device_/job:localhost/replica:0/task:0/device:CPU:0}} Resource localhost/_0_SentencepieceOp/N10tensorflow4text12_GLOBAL__N_121SentencepieceResourceE does not exist. [Op:SentencepieceTokenizeOp]\u001b[0m\n\nArguments received by GemmaTokenizer.call():\n  • inputs=tf.Tensor(shape=(1,), dtype=string)\n  • args=<class 'inspect._empty'>\n  • training=None\n  • kwargs=<class 'inspect._empty'>\n\n--- PROMPT 8 ---\nContext: Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research. The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor .\nQuestion: What code did Niki use to train and evaluate models?\nError processing row 7: Exception encountered when calling GemmaTokenizer.call().\n\n\u001b[1m{{function_node __wrapped__SentencepieceTokenizeOp_device_/job:localhost/replica:0/task:0/device:CPU:0}} Resource localhost/_0_SentencepieceOp/N10tensorflow4text12_GLOBAL__N_121SentencepieceResourceE does not exist. [Op:SentencepieceTokenizeOp]\u001b[0m\n\nArguments received by GemmaTokenizer.call():\n  • inputs=tf.Tensor(shape=(1,), dtype=string)\n  • args=<class 'inspect._empty'>\n  • training=None\n  • kwargs=<class 'inspect._empty'>\n\n--- PROMPT 9 ---\nContext: yWork performed while at Google Brain. zWork performed while at Google Research.\nQuestion: What did zWork perform while at Google Research?\nError processing row 8: Exception encountered when calling GemmaTokenizer.call().\n\n\u001b[1m{{function_node __wrapped__SentencepieceTokenizeOp_device_/job:localhost/replica:0/task:0/device:CPU:0}} Resource localhost/_0_SentencepieceOp/N10tensorflow4text12_GLOBAL__N_121SentencepieceResourceE does not exist. [Op:SentencepieceTokenizeOp]\u001b[0m\n\nArguments received by GemmaTokenizer.call():\n  • inputs=tf.Tensor(shape=(1,), dtype=string)\n  • args=<class 'inspect._empty'>\n  • training=None\n  • kwargs=<class 'inspect._empty'>\n\n--- PROMPT 10 ---\nContext: In all but a few cases [ 22], however, such attention mechanisms are used in conjunction with a recurrent network. Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. 4 Why Self-Attention In this section we compare various aspects of self-attention layers to the recurrent and convolu- tional layers commonly used for mapping one variable-length sequence of symbol representations (x1;:::;x n)to another sequence of equal length (z1;:::;z n), withxi;zi2Rd, such as a hidden layer in a typical sequence transduction encoder or decoder. As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n)sequential operations. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.\nQuestion: What does a self-attention layer connect all positions with?\nError processing row 9: Exception encountered when calling GemmaTokenizer.call().\n\n\u001b[1m{{function_node __wrapped__SentencepieceTokenizeOp_device_/job:localhost/replica:0/task:0/device:CPU:0}} Resource localhost/_0_SentencepieceOp/N10tensorflow4text12_GLOBAL__N_121SentencepieceResourceE does not exist. [Op:SentencepieceTokenizeOp]\u001b[0m\n\nArguments received by GemmaTokenizer.call():\n  • inputs=tf.Tensor(shape=(1,), dtype=string)\n  • args=<class 'inspect._empty'>\n  • training=None\n  • kwargs=<class 'inspect._empty'>\n\n--- PROMPT 11 ---\nContext: This makes it more difﬁcult to learn dependencies between distant positions [ 11]. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [ 11].\nQuestion: What is one key factor affecting the ability to learn dependencies between distant positions?\nError processing row 10: Exception encountered when calling GemmaTokenizer.call().\n\n\u001b[1m{{function_node __wrapped__SentencepieceTokenizeOp_device_/job:localhost/replica:0/task:0/device:CPU:0}} Resource localhost/_0_SentencepieceOp/N10tensorflow4text12_GLOBAL__N_121SentencepieceResourceE does not exist. [Op:SentencepieceTokenizeOp]\u001b[0m\n\nArguments received by GemmaTokenizer.call():\n  • inputs=tf.Tensor(shape=(1,), dtype=string)\n  • args=<class 'inspect._empty'>\n  • training=None\n  • kwargs=<class 'inspect._empty'>\n\n--- PROMPT 12 ---\nContext: In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position- 2Figure 1: The Transformer - model architecture. (right) Multi-Head Attention consists of several attention layers running in parallel. With a single attention head, averaging inhibits this. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality.\nQuestion: What does Multi-Head Attention consist of?\nError processing row 11: Exception encountered when calling GemmaTokenizer.call().\n\n\u001b[1m{{function_node __wrapped__SentencepieceTokenizeOp_device_/job:localhost/replica:0/task:0/device:CPU:0}} Resource localhost/_0_SentencepieceOp/N10tensorflow4text12_GLOBAL__N_121SentencepieceResourceE does not exist. [Op:SentencepieceTokenizeOp]\u001b[0m\n\nArguments received by GemmaTokenizer.call():\n  • inputs=tf.Tensor(shape=(1,), dtype=string)\n  • args=<class 'inspect._empty'>\n  • training=None\n  • kwargs=<class 'inspect._empty'>\n\n--- PROMPT 13 ---\nContext: Here, the encoder maps an input sequence of symbol representations (x1;:::;x n)to a sequence of continuous representations z= (z1;:::;z n). Given z, the decoder then generates an output sequence (y1;:::;y m)of symbols one element at a time. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.\nQuestion: What does the decoder map to?\nError processing row 12: Exception encountered when calling GemmaTokenizer.call().\n\n\u001b[1m{{function_node __wrapped__SentencepieceTokenizeOp_device_/job:localhost/replica:0/task:0/device:CPU:0}} Resource localhost/_0_SentencepieceOp/N10tensorflow4text12_GLOBAL__N_121SentencepieceResourceE does not exist. [Op:SentencepieceTokenizeOp]\u001b[0m\n\nArguments received by GemmaTokenizer.call():\n  • inputs=tf.Tensor(shape=(1,), dtype=string)\n  • args=<class 'inspect._empty'>\n  • training=None\n  • kwargs=<class 'inspect._empty'>\n\n--- PROMPT 14 ---\nContext: 3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N= 6 identical layers. Decoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.\nQuestion: What is the decoder composed of?\nError processing row 13: Exception encountered when calling GemmaTokenizer.call().\n\n\u001b[1m{{function_node __wrapped__SentencepieceTokenizeOp_device_/job:localhost/replica:0/task:0/device:CPU:0}} Resource localhost/_0_SentencepieceOp/N10tensorflow4text12_GLOBAL__N_121SentencepieceResourceE does not exist. [Op:SentencepieceTokenizeOp]\u001b[0m\n\nArguments received by GemmaTokenizer.call():\n  • inputs=tf.Tensor(shape=(1,), dtype=string)\n  • args=<class 'inspect._empty'>\n  • training=None\n  • kwargs=<class 'inspect._empty'>\n\n--- PROMPT 15 ---\nContext: Each layer has two sub-layers. We employ a residual connection [ 10] around each of the two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is LayerNorm( x+ Sublayer( x)), where Sublayer(x)is the function implemented by the sub-layer itself.\nQuestion: What is the output of each sub-layer?\nError processing row 14: Exception encountered when calling GemmaTokenizer.call().\n\n\u001b[1m{{function_node __wrapped__SentencepieceTokenizeOp_device_/job:localhost/replica:0/task:0/device:CPU:0}} Resource localhost/_0_SentencepieceOp/N10tensorflow4text12_GLOBAL__N_121SentencepieceResourceE does not exist. [Op:SentencepieceTokenizeOp]\u001b[0m\n\nArguments received by GemmaTokenizer.call():\n  • inputs=tf.Tensor(shape=(1,), dtype=string)\n  • args=<class 'inspect._empty'>\n  • training=None\n  • kwargs=<class 'inspect._empty'>\n\n--- PROMPT 16 ---\nContext: To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512 . The dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality dff= 2048 .\nQuestion: What is dmodel = 512?\nError processing row 15: Exception encountered when calling GemmaTokenizer.call().\n\n\u001b[1m{{function_node __wrapped__SentencepieceTokenizeOp_device_/job:localhost/replica:0/task:0/device:CPU:0}} Resource localhost/_0_SentencepieceOp/N10tensorflow4text12_GLOBAL__N_121SentencepieceResourceE does not exist. [Op:SentencepieceTokenizeOp]\u001b[0m\n\nArguments received by GemmaTokenizer.call():\n  • inputs=tf.Tensor(shape=(1,), dtype=string)\n  • args=<class 'inspect._empty'>\n  • training=None\n  • kwargs=<class 'inspect._empty'>\n\n--- PROMPT 17 ---\nContext: 3.2 Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. We compute the matrix of outputs as: Attention(Q;K;V ) = softmax(QKT pdk)V (1) The two most commonly used attention functions are additive attention [ 2], and dot-product (multi- plicative) attention. 3.2.2 Multi-Head Attention Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned linear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional output values. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2.\nQuestion: What are the two most commonly used attention functions?\nError processing row 16: Exception encountered when calling GemmaTokenizer.call().\n\n\u001b[1m{{function_node __wrapped__SentencepieceTokenizeOp_device_/job:localhost/replica:0/task:0/device:CPU:0}} Resource localhost/_0_SentencepieceOp/N10tensorflow4text12_GLOBAL__N_121SentencepieceResourceE does not exist. [Op:SentencepieceTokenizeOp]\u001b[0m\n\nArguments received by GemmaTokenizer.call():\n  • inputs=tf.Tensor(shape=(1,), dtype=string)\n  • args=<class 'inspect._empty'>\n  • training=None\n  • kwargs=<class 'inspect._empty'>\n\n--- PROMPT 18 ---\nContext: Dot-product attention is identical to our algorithm, except for the scaling factor of1pdk. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efﬁcient in practice, since it can be implemented using highly optimized matrix multiplication code. While for small values of dkthe two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk[3]. To counteract this effect, we scale the dot products by1pdk. We implement this inside of scaled dot-product attention by masking out (setting to 1) all values in the input of the softmax which correspond to illegal connections.\nQuestion: Dot-product attention is identical to what algorithm?\nError processing row 17: Exception encountered when calling GemmaTokenizer.call().\n\n\u001b[1m{{function_node __wrapped__SentencepieceTokenizeOp_device_/job:localhost/replica:0/task:0/device:CPU:0}} Resource localhost/_0_SentencepieceOp/N10tensorflow4text12_GLOBAL__N_121SentencepieceResourceE does not exist. [Op:SentencepieceTokenizeOp]\u001b[0m\n\nArguments received by GemmaTokenizer.call():\n  • inputs=tf.Tensor(shape=(1,), dtype=string)\n  • args=<class 'inspect._empty'>\n  • training=None\n  • kwargs=<class 'inspect._empty'>\n\n--- PROMPT 19 ---\nContext: 4To illustrate why the dot products get large, assume that the components of qandkare independent random variables with mean 0and variance 1. Then their dot product, qk=Pdk i=1qiki, has mean 0and variance dk.\nQuestion: What is qandkare's dot product?\nError processing row 18: Exception encountered when calling GemmaTokenizer.call().\n\n\u001b[1m{{function_node __wrapped__SentencepieceTokenizeOp_device_/job:localhost/replica:0/task:0/device:CPU:0}} Resource localhost/_0_SentencepieceOp/N10tensorflow4text12_GLOBAL__N_121SentencepieceResourceE does not exist. [Op:SentencepieceTokenizeOp]\u001b[0m\n\nArguments received by GemmaTokenizer.call():\n  • inputs=tf.Tensor(shape=(1,), dtype=string)\n  • args=<class 'inspect._empty'>\n  • training=None\n  • kwargs=<class 'inspect._empty'>\n\n--- PROMPT 20 ---\nContext: This allows every position in the decoder to attend over all positions in the input sequence. Each position in the encoder can attend to all positions in the previous layer of the encoder.\nQuestion: What can each position in the encoder attend to?\nError processing row 19: Exception encountered when calling GemmaTokenizer.call().\n\n\u001b[1m{{function_node __wrapped__SentencepieceTokenizeOp_device_/job:localhost/replica:0/task:0/device:CPU:0}} Resource localhost/_0_SentencepieceOp/N10tensorflow4text12_GLOBAL__N_121SentencepieceResourceE does not exist. [Op:SentencepieceTokenizeOp]\u001b[0m\n\nArguments received by GemmaTokenizer.call():\n  • inputs=tf.Tensor(shape=(1,), dtype=string)\n  • args=<class 'inspect._empty'>\n  • training=None\n  • kwargs=<class 'inspect._empty'>\n\n--- PROMPT 21 ---\nContext: 3.4 Embeddings and Softmax Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor- mation and softmax function to convert the decoder output to predicted next-token probabilities.\nQuestion: What do we use to convert input tokens and output tokens to?\nError processing row 20: Exception encountered when calling GemmaTokenizer.call().\n\n\u001b[1m{{function_node __wrapped__SentencepieceTokenizeOp_device_/job:localhost/replica:0/task:0/device:CPU:0}} Resource localhost/_0_SentencepieceOp/N10tensorflow4text12_GLOBAL__N_121SentencepieceResourceE does not exist. [Op:SentencepieceTokenizeOp]\u001b[0m\n\nArguments received by GemmaTokenizer.call():\n  • inputs=tf.Tensor(shape=(1,), dtype=string)\n  • args=<class 'inspect._empty'>\n  • training=None\n  • kwargs=<class 'inspect._empty'>\n\n--- PROMPT 22 ---\nContext: In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [ 24]. In the embedding layers, we multiply those weights bypdmodel.\nQuestion: What does the pre-softmax linear transformation mean?\nError processing row 21: Exception encountered when calling GemmaTokenizer.call().\n\n\u001b[1m{{function_node __wrapped__SentencepieceTokenizeOp_device_/job:localhost/replica:0/task:0/device:CPU:0}} Resource localhost/_0_SentencepieceOp/N10tensorflow4text12_GLOBAL__N_121SentencepieceResourceE does not exist. [Op:SentencepieceTokenizeOp]\u001b[0m\n\nArguments received by GemmaTokenizer.call():\n  • inputs=tf.Tensor(shape=(1,), dtype=string)\n  • args=<class 'inspect._empty'>\n  • training=None\n  • kwargs=<class 'inspect._empty'>\n\n--- PROMPT 23 ---\nContext: nis the sequence length, dis the representation dimension, kis the kernel size of convolutions and rthe size of the neighborhood in restricted self-attention. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size rin 6the input sequence centered around the respective output position. Even with k=n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.\nQuestion: What does k=n mean?\nError processing row 22: Exception encountered when calling GemmaTokenizer.call().\n\n\u001b[1m{{function_node __wrapped__SentencepieceTokenizeOp_device_/job:localhost/replica:0/task:0/device:CPU:0}} Resource localhost/_0_SentencepieceOp/N10tensorflow4text12_GLOBAL__N_121SentencepieceResourceE does not exist. [Op:SentencepieceTokenizeOp]\u001b[0m\n\nArguments received by GemmaTokenizer.call():\n  • inputs=tf.Tensor(shape=(1,), dtype=string)\n  • args=<class 'inspect._empty'>\n  • training=None\n  • kwargs=<class 'inspect._empty'>\n\n--- PROMPT 24 ---\nContext: Layer Type Complexity per Layer Sequential Maximum Path Length Operations Self-Attention O(n2d) O(1) O(1) Recurrent O(nd2) O(n) O(n) Convolutional O(knd2)O(1) O(logk(n)) Self-Attention (restricted) O(rnd)O(1) O(n=r) bottoms of the encoder and decoder stacks. One is the total computational complexity per layer. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity considerably, to O(knd+nd2).\nQuestion: What is the total computational complexity per layer?\nError processing row 23: Exception encountered when calling GemmaTokenizer.call().\n\n\u001b[1m{{function_node __wrapped__SentencepieceTokenizeOp_device_/job:localhost/replica:0/task:0/device:CPU:0}} Resource localhost/_0_SentencepieceOp/N10tensorflow4text12_GLOBAL__N_121SentencepieceResourceE does not exist. [Op:SentencepieceTokenizeOp]\u001b[0m\n\nArguments received by GemmaTokenizer.call():\n  • inputs=tf.Tensor(shape=(1,), dtype=string)\n  • args=<class 'inspect._empty'>\n  • training=None\n  • kwargs=<class 'inspect._empty'>\n\n--- PROMPT 25 ---\nContext: The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [ 8], and observe nearly identical results to the base model.\nQuestion: What do the positional embeddings have?\nError processing row 24: Exception encountered when calling GemmaTokenizer.call().\n\n\u001b[1m{{function_node __wrapped__SentencepieceTokenizeOp_device_/job:localhost/replica:0/task:0/device:CPU:0}} Resource localhost/_0_SentencepieceOp/N10tensorflow4text12_GLOBAL__N_121SentencepieceResourceE does not exist. [Op:SentencepieceTokenizeOp]\u001b[0m\n\nArguments received by GemmaTokenizer.call():\n  • inputs=tf.Tensor(shape=(1,), dtype=string)\n  • args=<class 'inspect._empty'>\n  • training=None\n  • kwargs=<class 'inspect._empty'>\n\n--- PROMPT 26 ---\nContext: The third is the path length between long-range dependencies in the network. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.\nQuestion: What is the third type of network?\nError processing row 25: Exception encountered when calling GemmaTokenizer.call().\n\n\u001b[1m{{function_node __wrapped__SentencepieceTokenizeOp_device_/job:localhost/replica:0/task:0/device:CPU:0}} Resource localhost/_0_SentencepieceOp/N10tensorflow4text12_GLOBAL__N_121SentencepieceResourceE does not exist. [Op:SentencepieceTokenizeOp]\u001b[0m\n\nArguments received by GemmaTokenizer.call():\n  • inputs=tf.Tensor(shape=(1,), dtype=string)\n  • args=<class 'inspect._empty'>\n  • training=None\n  • kwargs=<class 'inspect._empty'>\n\n--- PROMPT 27 ---\nContext: Sentences were encoded using byte-pair encoding [ 3], which has a shared source- target vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [ 31]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens.\nQuestion: How many tokens did the WMT 2014 English-French dataset contain?\nError processing row 26: Exception encountered when calling GemmaTokenizer.call().\n\n\u001b[1m{{function_node __wrapped__SentencepieceTokenizeOp_device_/job:localhost/replica:0/task:0/device:CPU:0}} Resource localhost/_0_SentencepieceOp/N10tensorflow4text12_GLOBAL__N_121SentencepieceResourceE does not exist. [Op:SentencepieceTokenizeOp]\u001b[0m\n\nArguments received by GemmaTokenizer.call():\n  • inputs=tf.Tensor(shape=(1,), dtype=string)\n  • args=<class 'inspect._empty'>\n  • training=None\n  • kwargs=<class 'inspect._empty'>\n\n--- PROMPT 28 ---\nContext: For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds.\nQuestion: How long did each training step take?\nError processing row 27: Exception encountered when calling GemmaTokenizer.call().\n\n\u001b[1m{{function_node __wrapped__SentencepieceTokenizeOp_device_/job:localhost/replica:0/task:0/device:CPU:0}} Resource localhost/_0_SentencepieceOp/N10tensorflow4text12_GLOBAL__N_121SentencepieceResourceE does not exist. [Op:SentencepieceTokenizeOp]\u001b[0m\n\nArguments received by GemmaTokenizer.call():\n  • inputs=tf.Tensor(shape=(1,), dtype=string)\n  • args=<class 'inspect._empty'>\n  • training=None\n  • kwargs=<class 'inspect._empty'>\n\n--- PROMPT 29 ---\nContext: 7Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost. The Transformer (big) model trained for English-to-French used dropout rate Pdrop= 0:1, instead of 0:3. 6.2 Model Variations To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013.\nQuestion: How did the Transformer achieve better BLEU scores than previous state-of-the-art models?\nError processing row 28: Exception encountered when calling GemmaTokenizer.call().\n\n\u001b[1m{{function_node __wrapped__SentencepieceTokenizeOp_device_/job:localhost/replica:0/task:0/device:CPU:0}} Resource localhost/_0_SentencepieceOp/N10tensorflow4text12_GLOBAL__N_121SentencepieceResourceE does not exist. [Op:SentencepieceTokenizeOp]\u001b[0m\n\nArguments received by GemmaTokenizer.call():\n  • inputs=tf.Tensor(shape=(1,), dtype=string)\n  • args=<class 'inspect._empty'>\n  • training=None\n  • kwargs=<class 'inspect._empty'>\n\n--- PROMPT 30 ---\nContext: Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models. In the former task our best model outperforms even all previously reported ensembles.\nQuestion: What model surpasses all previously published models?\nError processing row 29: Exception encountered when calling GemmaTokenizer.call().\n\n\u001b[1m{{function_node __wrapped__SentencepieceTokenizeOp_device_/job:localhost/replica:0/task:0/device:CPU:0}} Resource localhost/_0_SentencepieceOp/N10tensorflow4text12_GLOBAL__N_121SentencepieceResourceE does not exist. [Op:SentencepieceTokenizeOp]\u001b[0m\n\nArguments received by GemmaTokenizer.call():\n  • inputs=tf.Tensor(shape=(1,), dtype=string)\n  • args=<class 'inspect._empty'>\n  • training=None\n  • kwargs=<class 'inspect._empty'>\n\n--- PROMPT 31 ---\nContext: For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search as described in the previous section, but no checkpoint averaging.\nQuestion: How many checkpoints did the base models use?\nError processing row 30: Exception encountered when calling GemmaTokenizer.call().\n\n\u001b[1m{{function_node __wrapped__SentencepieceTokenizeOp_device_/job:localhost/replica:0/task:0/device:CPU:0}} Resource localhost/_0_SentencepieceOp/N10tensorflow4text12_GLOBAL__N_121SentencepieceResourceE does not exist. [Op:SentencepieceTokenizeOp]\u001b[0m\n\nArguments received by GemmaTokenizer.call():\n  • inputs=tf.Tensor(shape=(1,), dtype=string)\n  • args=<class 'inspect._empty'>\n  • training=None\n  • kwargs=<class 'inspect._empty'>\n","output_type":"stream"}],"execution_count":10}]}