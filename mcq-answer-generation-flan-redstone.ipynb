{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset, Dataset\nfrom transformers import (\n    T5ForConditionalGeneration,\n    T5Tokenizer,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForSeq2Seq\n)\nimport numpy as np\nimport gc\nimport os\nfrom tqdm.auto import tqdm\n\n# # Create output directories if they don't exist\n# os.makedirs(\"./results/answer_generation\", exist_ok=True)\n# os.makedirs(\"./logs/answer_generation\", exist_ok=True)\n\nos.makedirs(\"/kaggle/working/results/answer_generation\", exist_ok=True)\nos.makedirs(\"/kaggle/working/logs/answer_generation\", exist_ok=True)\n\n\n# Enable memory optimization for PyTorch\ntorch.cuda.empty_cache()\n\n# 1. Load dataset - load only a subset directly\nprint(\"Loading dataset...\")\ndataset = load_dataset(\"zjsd/RedStone-QA-mcq\", split=f\"train[:{int(0.02*100)}%]\")\nprint(f\"Dataset loaded with {len(dataset)} examples\")\n\n# 2. Define preprocessing function for answer generation\ndef preprocess_for_answer_generation(examples, batch_size=64):\n    \"\"\"Preprocess data for training the model to generate the correct answer letter.\"\"\"\n    inputs = []\n    labels = []\n    \n    for i in range(0, len(examples[\"text\"]), batch_size):\n        batch_texts = examples[\"text\"][i:i+batch_size]\n        batch_questions = examples[\"question\"][i:i+batch_size]\n        batch_answers = examples[\"answer\"][i:i+batch_size]\n        \n        for text, question, answer in zip(batch_texts, batch_questions, batch_answers):\n            combined = f\"generate answer: {text} question: {question}\"\n            inputs.append(combined)\n            # Extract just the letter from \"Answer:X\" format\n            labels.append(answer.replace(\"Answer:\", \"\").strip())\n    \n    return {\n        \"input\": inputs,\n        \"output\": labels\n    }\n\n# 3. Process dataset in chunks to save memory\nprint(\"Preprocessing data for answer generation...\")\nanswer_dataset = Dataset.from_dict(preprocess_for_answer_generation(dataset))\n\n# Free up memory\ndel dataset\ngc.collect()\ntorch.cuda.empty_cache()\n\n# 4. Split dataset into train and validation\nprint(\"Splitting dataset into train and validation sets...\")\nanswer_dataset = answer_dataset.train_test_split(test_size=0.1, seed=42)\n\nanswer_train_dataset = answer_dataset[\"train\"]\nanswer_val_dataset = answer_dataset[\"test\"]\n\nprint(f\"Answer generation: {len(answer_train_dataset)} training examples, {len(answer_val_dataset)} validation examples\")\n\n# Free up memory\ndel answer_dataset\ngc.collect()\n\n# 5. Load tokenizer and model\nprint(\"Loading tokenizer and model...\")\nmodel_name = \"google/flan-t5-small\"  # Using the small model for faster training\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\n\n# 6. Define tokenization function\ndef tokenize_function(examples, max_input_length=512, max_target_length=128):\n    model_inputs = tokenizer(\n        examples[\"input\"],\n        max_length=max_input_length,\n        padding=\"max_length\",\n        truncation=True\n    )\n    \n    # Tokenize targets\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            examples[\"output\"],\n            max_length=max_target_length,\n            padding=\"max_length\",\n            truncation=True\n        )\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# 7. Tokenize datasets with smaller batch size to reduce memory usage\nprint(\"Tokenizing datasets...\")\nbatch_size = 32\n\nanswer_train_tokenized = answer_train_dataset.map(\n    tokenize_function, \n    batched=True,\n    batch_size=batch_size,\n    remove_columns=[\"input\", \"output\"]\n)\n\nanswer_val_tokenized = answer_val_dataset.map(\n    tokenize_function, \n    batched=True,\n    batch_size=batch_size,\n    remove_columns=[\"input\", \"output\"]\n)\n\n# Free up memory\ndel answer_train_dataset, answer_val_dataset\ngc.collect()\ntorch.cuda.empty_cache()\n\n# 8. Define custom metrics computation function\ndef compute_answer_metrics(eval_pred):\n    predictions, labels = eval_pred\n    # Replace -100 with the pad_token_id\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    \n    # Get the predicted tokens\n    predicted_tokens = np.argmax(predictions, axis=-1)\n    \n    # Decode to get the actual letters - process in batches to save memory\n    batch_size = 32\n    num_examples = len(predicted_tokens)\n    decoded_preds = []\n    decoded_labels = []\n    \n    for i in range(0, num_examples, batch_size):\n        batch_preds = [tokenizer.decode(pred, skip_special_tokens=True) \n                      for pred in predicted_tokens[i:i+batch_size]]\n        batch_labels = [tokenizer.decode(label, skip_special_tokens=True) \n                       for label in labels[i:i+batch_size]]\n        decoded_preds.extend(batch_preds)\n        decoded_labels.extend(batch_labels)\n    \n    # Calculate accuracy\n    correct = sum(1 for pred, label in zip(decoded_preds, decoded_labels) if pred.strip() == label.strip())\n    accuracy = correct / len(decoded_labels) if len(decoded_labels) > 0 else 0\n    \n    # Print just a few examples for debugging\n    print(\"\\nAnswer Generation Examples (Prediction, Reference):\")\n    for i in range(min(3, len(decoded_preds))):\n        print(f\"  {decoded_preds[i]} | {decoded_labels[i]}\")\n    \n    # Return metrics\n    return {\n        \"accuracy\": accuracy,\n        \"exact_match_ratio\": correct / len(decoded_labels) if len(decoded_labels) > 0 else 0,\n    }\n\n# 9. Define training arguments for answer generation\nanswer_training_args = TrainingArguments(\n    output_dir=\"./results/answer_generation\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=2,\n    weight_decay=0.01,\n    save_total_limit=1,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    greater_is_better=True,\n    logging_dir=\"./logs/answer_generation\",\n    logging_steps=100,\n    report_to=[\"tensorboard\"],\n    # Memory optimizations\n    fp16=True if torch.cuda.is_available() else False,  # Use mixed precision if available\n    gradient_accumulation_steps=2,  # Accumulate gradients to simulate larger batch sizes\n    dataloader_num_workers=1,  # Parallelize data loading\n    dataloader_pin_memory=True,  # Speed up data transfer to GPU\n    # Ensure progress bar is shown\n    disable_tqdm=False,\n)\n\n# 10. Define data collator\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer,\n    model=model,\n    padding=\"max_length\",\n    max_length=512,\n    pad_to_multiple_of=8  # Optimize for tensor operations\n)\n\n# 11. Train answer generation model\nprint(\"Initializing answer generation trainer...\")\nanswer_trainer = Trainer(\n    model=model,\n    args=answer_training_args,\n    train_dataset=answer_train_tokenized,\n    eval_dataset=answer_val_tokenized,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_answer_metrics,\n)\n\nprint(\"Training answer generation model...\")\nanswer_trainer.train()\n\n# Get validation results\nprint(\"Evaluating answer generation model...\")\nanswer_eval_results = answer_trainer.evaluate()\nprint(\"\\n\" + \"=\"*50)\nprint(\"ANSWER GENERATION EVALUATION RESULTS\")\nprint(\"=\"*50)\nprint(f\"Accuracy: {answer_eval_results['eval_accuracy']:.4f}\")\nprint(f\"Exact Match: {answer_eval_results['eval_exact_match_ratio']:.4f}\")\nprint(\"=\"*50 + \"\\n\")\n\n# Save model\nprint(\"Saving final model...\")\nanswer_trainer.save_model(\"./results/answer_generation/final_model\")\nprint(\"Model saved successfully!\")\n\n# 12. Memory-efficient inference function for demonstration\ndef generate_answer(context, question, model, tokenizer):\n    # Set model to evaluation mode\n    model.eval()\n    \n    # Get the device\n    device = next(model.parameters()).device\n    \n    # Prepare input\n    answer_input = f\"generate answer: {context} question: {question}\"\n    answer_input_ids = tokenizer(answer_input, return_tensors=\"pt\").input_ids.to(device)\n    \n    print(\"Generating answer letter...\")\n    with torch.no_grad():  # Disable gradient calculation to save memory\n        answer_outputs = model.generate(\n            answer_input_ids, \n            max_length=10,\n            num_beams=4,\n            early_stopping=True\n        )\n    answer_letter = tokenizer.decode(answer_outputs[0], skip_special_tokens=True).strip()\n    print(f\"Generated answer letter: {answer_letter}\")\n    \n    # Clean up GPU memory\n    del answer_outputs, answer_input_ids\n    torch.cuda.empty_cache()\n    \n    return answer_letter\n\n# Optional: Demonstrate the trained model\nprint(\"\\n\" + \"=\"*50)\nprint(\"DEMONSTRATION\")\nprint(\"=\"*50)\ntry:\n    # Sample context and question\n    context = \"The Python programming language was created by Guido van Rossum and first released in 1991. It emphasizes code readability with its notable use of significant whitespace.\"\n    question = \"Who created the Python programming language?\"\n    \n    # Generate answer\n    answer_letter = generate_answer(context, question, model, tokenizer)\n    print(f\"Context: {context}\")\n    print(f\"Question: {question}\")\n    print(f\"Generated Answer: {answer_letter}\")\nexcept Exception as e:\n    print(f\"Error during demonstration: {e}\")\n\nprint(\"\\nTraining of Answer Generation model complete!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-03T08:11:24.223996Z","iopub.execute_input":"2025-03-03T08:11:24.224185Z","iopub.status.idle":"2025-03-03T08:43:55.552208Z","shell.execute_reply.started":"2025-03-03T08:11:24.224167Z","shell.execute_reply":"2025-03-03T08:43:55.550799Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n    DataCollatorForSeq2Seq\n)\nimport torch\nfrom torch.utils.data import Dataset\nimport random\nimport gc\nfrom tqdm.auto import tqdm\n\n# Set random seeds for reproducibility\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\ntorch.cuda.empty_cache()\n\n# Define constants\nMODEL_NAME = \"google/flan-t5-small\"\nMAX_INPUT_LENGTH = 384  # Reduced from 512\nMAX_TARGET_LENGTH = 48  # Reduced from 64\nBATCH_SIZE = 4  # Reduced from 8\nGRADIENT_ACCUMULATION_STEPS = 4  # Increases effective batch size without increasing memory\nLEARNING_RATE = 3e-4\nNUM_EPOCHS = 3\nSAMPLE_RATIO = 0.1  # Reduced from 0.15\nMIXED_PRECISION = \"fp16\"  # Use mixed precision training\n\n# Function to free memory\ndef free_memory():\n    gc.collect()\n    torch.cuda.empty_cache()\n\n# Load the dataset in streaming mode to reduce memory usage\nprint(\"Loading dataset...\")\ndataset = load_dataset(\"zjsd/RedStone-QA-mcq\", split=\"train\", streaming=True)\n\n# Convert to regular dataset for sampling\n# Buffer a smaller amount to not load everything in memory\ndataset = dataset.take(int(1.66e6 * SAMPLE_RATIO * 1.2))  # Buffer slightly more than needed\ndataset = list(dataset)\nrandom.shuffle(dataset)\ndataset = dataset[:int(1.66e6 * SAMPLE_RATIO)]\n\n# Split into train and validation\ntrain_val_split = 0.9\ntrain_size = int(len(dataset) * train_val_split)\n\n# Create train and validation datasets\ntrain_dataset = dataset[:train_size]\nval_dataset = dataset[train_size:train_size + min(2000, len(dataset) - train_size)]  # Limit validation set\n\nprint(f\"Total sampled examples: {len(dataset)}\")\nprint(f\"Training examples: {len(train_dataset)}\")\nprint(f\"Validation examples: {len(val_dataset)}\")\n\n# Free memory\ndel dataset\nfree_memory()\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n\n# Define a processing function for batch tokenization\ndef preprocess_function(examples):\n    batch_size = len(examples)\n    input_texts = []\n    target_texts = []\n    \n    for i in range(batch_size):\n        context = examples[i]['text']\n        question = examples[i]['question']\n        answer = examples[i]['answer'].replace(\"Answer:\", \"\").strip()\n        \n        # Format the input (context + question)\n        input_text = f\"Context: {context} Question: {question}\"\n        input_texts.append(input_text)\n        target_texts.append(answer)\n    \n    # Tokenize inputs\n    model_inputs = tokenizer(\n        input_texts,\n        max_length=MAX_INPUT_LENGTH,\n        padding=\"max_length\",\n        truncation=True\n    )\n    \n    # Tokenize targets\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            target_texts,\n            max_length=MAX_TARGET_LENGTH,\n            padding=\"max_length\",\n            truncation=True\n        )\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    \n    # Replace padding token id with -100 so it's ignored in the loss\n    for i in range(len(model_inputs[\"labels\"])):\n        model_inputs[\"labels\"][i] = [\n            -100 if token == tokenizer.pad_token_id else token \n            for token in model_inputs[\"labels\"][i]\n        ]\n    \n    return model_inputs\n\n# Process data in batches to save memory\nprint(\"Processing training data...\")\nbatch_size = 512\ntrain_processed = []\nfor i in tqdm(range(0, len(train_dataset), batch_size)):\n    batch = train_dataset[i:i+batch_size]\n    processed_batch = preprocess_function(batch)\n    for j in range(len(batch)):\n        train_processed.append({\n            \"input_ids\": processed_batch[\"input_ids\"][j],\n            \"attention_mask\": processed_batch[\"attention_mask\"][j],\n            \"labels\": processed_batch[\"labels\"][j]\n        })\n\nprint(\"Processing validation data...\")\nval_processed = []\nfor i in tqdm(range(0, len(val_dataset), batch_size)):\n    batch = val_dataset[i:i+batch_size]\n    processed_batch = preprocess_function(batch)\n    for j in range(len(batch)):\n        val_processed.append({\n            \"input_ids\": processed_batch[\"input_ids\"][j],\n            \"attention_mask\": processed_batch[\"attention_mask\"][j],\n            \"labels\": processed_batch[\"labels\"][j]\n        })\n\n# Free memory\ndel train_dataset, val_dataset\nfree_memory()\n\n# Create PyTorch datasets\nclass MemoryEfficientDataset(Dataset):\n    def __init__(self, examples):\n        self.examples = examples\n    \n    def __len__(self):\n        return len(self.examples)\n    \n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(self.examples[idx][\"input_ids\"]),\n            \"attention_mask\": torch.tensor(self.examples[idx][\"attention_mask\"]),\n            \"labels\": torch.tensor(self.examples[idx][\"labels\"])\n        }\n\ntrain_dataset = MemoryEfficientDataset(train_processed)\nval_dataset = MemoryEfficientDataset(val_processed)\n\n# Free memory\ndel train_processed, val_processed\nfree_memory()\n\n# Load the model with 8-bit precision to save memory\nprint(\"Loading model...\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n\n# Define training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./answer_generation_model\",\n    overwrite_output_dir=True,\n    num_train_epochs=NUM_EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=1,  # Keep only the best model\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    logging_dir=\"./logs\",\n    logging_steps=50,\n    learning_rate=LEARNING_RATE,\n    weight_decay=0.01,\n    warmup_ratio=0.05,\n    predict_with_generate=False,  # Save memory during evaluation\n    fp16=MIXED_PRECISION == \"fp16\",  # Mixed precision training\n    optim=\"adamw_torch\",\n    report_to=\"none\",  # Disable W&B reporting\n    disable_tqdm=False,  # Enable tqdm progress bar\n)\n\n# Set up the trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True),\n)\n\n# Train the model\nprint(\"Training model...\")\ntrainer.train()\n\n# Save the model\nprint(\"Saving model...\")\nmodel.save_pretrained(\"./answer_generation_model_final\")\ntokenizer.save_pretrained(\"./answer_generation_model_final\")\n\n# Test on a few examples\nprint(\"Testing on some examples...\")\nmodel.eval()\ntest_examples = [\n    val_dataset[i] for i in range(min(3, len(val_dataset)))\n]\n\nfor example in test_examples:\n    input_ids = example[\"input_ids\"].unsqueeze(0).to(model.device)\n    attention_mask = example[\"attention_mask\"].unsqueeze(0).to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            max_length=MAX_TARGET_LENGTH,\n            num_beams=2,  # Reduced for memory efficiency\n            early_stopping=True\n        )\n    \n    input_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n    predicted_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    print(f\"Input: {input_text}\")\n    print(f\"Predicted Answer: {predicted_answer}\")\n    print(\"=\" * 50)\n\nprint(\"Training complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T08:48:44.213065Z","iopub.execute_input":"2025-03-03T08:48:44.213352Z","iopub.status.idle":"2025-03-03T12:09:06.047702Z","shell.execute_reply.started":"2025-03-03T08:48:44.213329Z","shell.execute_reply":"2025-03-03T12:09:06.046918Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n    DataCollatorForSeq2Seq\n)\nimport torch\nfrom torch.utils.data import Dataset\nimport random\nimport gc\nfrom tqdm.auto import tqdm\nimport re\n\n# Set random seeds for reproducibility\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\ntorch.cuda.empty_cache()\n\n# Define constants\nMODEL_NAME = \"google/flan-t5-small\"\nMAX_INPUT_LENGTH = 384\nMAX_TARGET_LENGTH = 96\nBATCH_SIZE = 4\nGRADIENT_ACCUMULATION_STEPS = 4\nLEARNING_RATE = 3e-4\nNUM_EPOCHS = 3\nSAMPLE_RATIO = 0.1\nMIXED_PRECISION = \"fp16\"\n\n# Function to free memory\ndef free_memory():\n    gc.collect()\n    torch.cuda.empty_cache()\n\n# Extract actual answer content and distractors\ndef extract_answers_and_distractors(row):\n    correct_answer = row['answer'].replace(\"Answer:\", \"\").strip()\n    choices = row['choices']\n    \n    # Identify correct answer letter\n    correct_letter = None\n    \n    # If answer is just a letter (A, B, C, D)\n    if re.match(r'^[A-D]$', correct_answer):\n        correct_letter = correct_answer\n    else:\n        # If answer starts with a letter followed by a period/colon (A., A:)\n        match = re.match(r'^([A-D])[:\\.]?\\s*', correct_answer)\n        if match:\n            correct_letter = match.group(1)\n    \n    # If we still don't have a letter, try to infer from content\n    if not correct_letter:\n        correct_content = correct_answer\n        for choice in choices:\n            parts = re.match(r'^([A-D])[.\\s]+(.*)', choice)\n            if parts:\n                letter, content = parts.groups()\n                # If the content matches (approximately) the correct answer\n                if content.strip().lower() in correct_answer.lower() or correct_answer.lower() in content.strip().lower():\n                    correct_letter = letter\n                    correct_content = content.strip()\n                    break\n    \n    # If we still don't have a letter, take a guess based on position\n    if not correct_letter and \"A\" in correct_answer:\n        correct_letter = \"A\"\n    elif not correct_letter and \"B\" in correct_answer:\n        correct_letter = \"B\"\n    elif not correct_letter and \"C\" in correct_answer:\n        correct_letter = \"C\"\n    elif not correct_letter and \"D\" in correct_answer:\n        correct_letter = \"D\"\n    elif not correct_letter and len(choices) > 0:\n        correct_letter = \"A\"  # Default to first option\n    \n    # Extract correct answer content and distractors\n    correct_content = None\n    distractors = []\n    \n    for choice in choices:\n        parts = re.match(r'^([A-D])[.\\s]+(.*)', choice)\n        if parts:\n            letter, content = parts.groups()\n            content = content.strip()\n            if letter == correct_letter:\n                correct_content = content\n            elif content:  # Only add non-empty distractors\n                distractors.append(content)\n    \n    # If we didn't extract content from choices, use the original answer\n    if not correct_content:\n        if re.match(r'^[A-D][:\\.]?\\s+(.+)$', correct_answer):\n            correct_content = re.match(r'^[A-D][:\\.]?\\s+(.+)$', correct_answer).group(1)\n        else:\n            correct_content = correct_answer\n    \n    return correct_content, distractors\n\n# Load the dataset in streaming mode to reduce memory usage\nprint(\"Loading dataset...\")\ndataset = load_dataset(\"zjsd/RedStone-QA-mcq\", split=\"train\", streaming=True)\n\n# Convert to regular dataset for sampling\ndataset = dataset.take(int(1.66e6 * SAMPLE_RATIO * 1.2))\ndataset = list(dataset)\nrandom.shuffle(dataset)\ndataset = dataset[:int(1.66e6 * SAMPLE_RATIO)]\n\n# Process dataset to extract actual answers and distractors\nprint(\"Extracting answers and distractors...\")\nprocessed_dataset = []\nfor row in tqdm(dataset):\n    actual_answer, distractors = extract_answers_and_distractors(row)\n    if actual_answer and distractors:  # Only keep examples with valid answers and distractors\n        row['actual_answer'] = actual_answer\n        row['distractors_list'] = distractors\n        processed_dataset.append(row)\n\n# Free memory\ndel dataset\nfree_memory()\n\n# Split into train and validation\ntrain_val_split = 0.9\ntrain_size = int(len(processed_dataset) * train_val_split)\n\n# Create train and validation datasets\ntrain_dataset = processed_dataset[:train_size]\nval_dataset = processed_dataset[train_size:train_size + min(2000, len(processed_dataset) - train_size)]\n\nprint(f\"Total processed examples: {len(processed_dataset)}\")\nprint(f\"Training examples: {len(train_dataset)}\")\nprint(f\"Validation examples: {len(val_dataset)}\")\n\n# Free memory\ndel processed_dataset\nfree_memory()\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n\n# Define a processing function for batch tokenization\ndef preprocess_function(examples):\n    batch_size = len(examples)\n    input_texts = []\n    target_texts = []\n    \n    for i in range(batch_size):\n        context = examples[i]['text']\n        question = examples[i]['question']\n        answer = examples[i]['actual_answer']\n        \n        # Format distractors as a single string with separators\n        distractors = \" | \".join(examples[i]['distractors_list'])\n        \n        # Format the input (context + question + correct answer)\n        input_text = f\"Context: {context} Question: {question} Correct Answer: {answer}\"\n        input_texts.append(input_text)\n        target_texts.append(distractors)\n    \n    # Tokenize inputs\n    model_inputs = tokenizer(\n        input_texts,\n        max_length=MAX_INPUT_LENGTH,\n        padding=\"max_length\",\n        truncation=True\n    )\n    \n    # Tokenize targets\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            target_texts,\n            max_length=MAX_TARGET_LENGTH,\n            padding=\"max_length\",\n            truncation=True\n        )\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    \n    # Replace padding token id with -100 so it's ignored in the loss\n    for i in range(len(model_inputs[\"labels\"])):\n        model_inputs[\"labels\"][i] = [\n            -100 if token == tokenizer.pad_token_id else token \n            for token in model_inputs[\"labels\"][i]\n        ]\n    \n    return model_inputs\n\n# Process data in batches to save memory\nprint(\"Processing training data...\")\nbatch_size = 512\ntrain_processed = []\nfor i in tqdm(range(0, len(train_dataset), batch_size)):\n    batch = train_dataset[i:i+batch_size]\n    processed_batch = preprocess_function(batch)\n    for j in range(len(batch)):\n        train_processed.append({\n            \"input_ids\": processed_batch[\"input_ids\"][j],\n            \"attention_mask\": processed_batch[\"attention_mask\"][j],\n            \"labels\": processed_batch[\"labels\"][j]\n        })\n\nprint(\"Processing validation data...\")\nval_processed = []\nfor i in tqdm(range(0, len(val_dataset), batch_size)):\n    batch = val_dataset[i:i+batch_size]\n    processed_batch = preprocess_function(batch)\n    for j in range(len(batch)):\n        val_processed.append({\n            \"input_ids\": processed_batch[\"input_ids\"][j],\n            \"attention_mask\": processed_batch[\"attention_mask\"][j],\n            \"labels\": processed_batch[\"labels\"][j]\n        })\n\n# Free memory\ndel train_dataset, val_dataset\nfree_memory()\n\n# Create PyTorch datasets\nclass MemoryEfficientDataset(Dataset):\n    def __init__(self, examples):\n        self.examples = examples\n    \n    def __len__(self):\n        return len(self.examples)\n    \n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(self.examples[idx][\"input_ids\"]),\n            \"attention_mask\": torch.tensor(self.examples[idx][\"attention_mask\"]),\n            \"labels\": torch.tensor(self.examples[idx][\"labels\"])\n        }\n\ntrain_dataset = MemoryEfficientDataset(train_processed)\nval_dataset = MemoryEfficientDataset(val_processed)\n\n# Free memory\ndel train_processed, val_processed\nfree_memory()\n\n# Load the model\nprint(\"Loading model...\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n\n# Define training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./distractor_generation_model\",\n    overwrite_output_dir=True,\n    num_train_epochs=NUM_EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=1,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    logging_dir=\"./logs\",\n    logging_steps=50,\n    learning_rate=LEARNING_RATE,\n    weight_decay=0.01,\n    warmup_ratio=0.05,\n    predict_with_generate=False,\n    fp16=MIXED_PRECISION == \"fp16\",\n    optim=\"adamw_torch\",\n    report_to=\"none\",\n    disable_tqdm=False,\n)\n\n# Set up the trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True),\n)\n\n# Train the model\nprint(\"Training model...\")\ntrainer.train()\n\n# Save the model\nprint(\"Saving model...\")\nmodel.save_pretrained(\"./distractor_generation_model_final\")\ntokenizer.save_pretrained(\"./distractor_generation_model_final\")\n\n# Test on a few examples\nprint(\"Testing on some examples...\")\nmodel.eval()\ntest_examples = [\n    val_dataset[i] for i in range(min(3, len(val_dataset)))\n]\n\nfor example in test_examples:\n    input_ids = example[\"input_ids\"].unsqueeze(0).to(model.device)\n    attention_mask = example[\"attention_mask\"].unsqueeze(0).to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            max_length=MAX_TARGET_LENGTH,\n            num_beams=2,\n            early_stopping=True\n        )\n    \n    input_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n    predicted_distractors = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    print(f\"Input: {input_text}\")\n    print(f\"Predicted Distractors: {predicted_distractors}\")\n    print(\"=\" * 50)\n\nprint(\"Training complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T12:38:37.689231Z","iopub.execute_input":"2025-03-03T12:38:37.689584Z","iopub.status.idle":"2025-03-03T16:18:33.099170Z","shell.execute_reply.started":"2025-03-03T12:38:37.689552Z","shell.execute_reply":"2025-03-03T16:18:33.098435Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Add these imports at the top of your script\nfrom huggingface_hub import login, HfApi\n\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nHF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n\nprint(\"Logging in to Hugging Face Hub...\")\nlogin(token=HF_TOKEN)\n\n# Define your Hugging Face repository name\n# Format: \"username/repository-name\"\nHF_REPO_ID = \"aayeshanakarmi/distractor-generation-redstone-flant5small-2\"  # Replace with your desired repo name\n\n# Save the model to the Hub\nprint(f\"Uploading model to Hugging Face Hub as {HF_REPO_ID}...\")\nmodel.push_to_hub(HF_REPO_ID, use_auth_token=HF_TOKEN)\ntokenizer.push_to_hub(HF_REPO_ID, use_auth_token=HF_TOKEN)\n\n\nprint(f\"Model successfully uploaded to Hugging Face Hub: https://huggingface.co/{HF_REPO_ID}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T16:36:09.624146Z","iopub.execute_input":"2025-03-03T16:36:09.624502Z","iopub.status.idle":"2025-03-03T16:36:22.854002Z","shell.execute_reply.started":"2025-03-03T16:36:09.624472Z","shell.execute_reply":"2025-03-03T16:36:22.853190Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Answer Generation Model ","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n    DataCollatorForSeq2Seq\n)\nimport torch\nfrom torch.utils.data import Dataset\nimport random\nimport gc\nfrom tqdm.auto import tqdm\nimport re\n\n# Set random seeds for reproducibility\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\ntorch.cuda.empty_cache()\n\n# Define constants\nMODEL_NAME = \"google/flan-t5-small\"\nMAX_INPUT_LENGTH = 384\nMAX_TARGET_LENGTH = 48\nBATCH_SIZE = 4\nGRADIENT_ACCUMULATION_STEPS = 4\nLEARNING_RATE = 3e-4\nNUM_EPOCHS = 3\nSAMPLE_RATIO = 0.1\nMIXED_PRECISION = \"fp16\"\n\n# Function to free memory\ndef free_memory():\n    gc.collect()\n    torch.cuda.empty_cache()\n\n# Enhanced function to extract actual answer content (not just A, B, C, D)\ndef extract_actual_answer(row):\n    correct_answer = row['answer'].replace(\"Answer:\", \"\").strip()\n    choices = row['choices']\n    \n    # If answer is just a single letter (A, B, C, D)\n    if re.match(r'^[A-D]$', correct_answer):\n        letter = correct_answer\n        for choice in choices:\n            # Look for the choice that starts with this letter\n            if choice.startswith(letter + \".\") or choice.startswith(letter + \" \") or choice.startswith(letter + \":\"):\n                # Extract everything after the letter and separator\n                content = re.sub(r'^[A-D][.\\s:]+', '', choice).strip()\n                return content\n    \n    # If answer starts with letter followed by period/colon/space (e.g., \"A. text\", \"A: text\", \"A text\")\n    match = re.match(r'^([A-D])[.\\s:]+(.+)$', correct_answer)\n    if match:\n        # Extract the content part after the letter\n        content = match.group(2).strip()\n        if content:\n            return content\n        \n        # If no content after the letter in the answer, find it in choices\n        letter = match.group(1)\n        for choice in choices:\n            if choice.startswith(letter + \".\") or choice.startswith(letter + \" \") or choice.startswith(letter + \":\"):\n                content = re.sub(r'^[A-D][.\\s:]+', '', choice).strip()\n                return content\n    \n    # Handle case where the answer might be the full text that matches one of the choices\n    for choice in choices:\n        # Extract the content part of the choice (removing any leading A., B., etc.)\n        choice_content = re.sub(r'^[A-D][.\\s:]+', '', choice).strip()\n        # If the answer matches this content exactly, return it\n        if correct_answer == choice_content:\n            return correct_answer\n    \n    # If we couldn't match it to a specific choice or extract a letter,\n    # just return the original answer as a fallback\n    return correct_answer\n\n# Load the dataset in streaming mode to reduce memory usage\nprint(\"Loading dataset...\")\ndataset = load_dataset(\"zjsd/RedStone-QA-mcq\", split=\"train\", streaming=True)\n\n# Convert to regular dataset for sampling\ndataset = dataset.take(int(1.66e6 * SAMPLE_RATIO * 1.2))\ndataset = list(dataset)\nrandom.shuffle(dataset)\ndataset = dataset[:int(1.66e6 * SAMPLE_RATIO)]\n\n# Process dataset to extract actual answers\nprint(\"Extracting actual answers...\")\nprocessed_dataset = []\nfor row in tqdm(dataset):\n    actual_answer = extract_actual_answer(row)\n    if actual_answer.strip():  # Only keep examples with non-empty answers\n        row['actual_answer'] = actual_answer\n        processed_dataset.append(row)\n\n# Free memory\ndel dataset\nfree_memory()\n\n# Split into train and validation\ntrain_val_split = 0.9\ntrain_size = int(len(processed_dataset) * train_val_split)\n\n# Create train and validation datasets\ntrain_dataset = processed_dataset[:train_size]\nval_dataset = processed_dataset[train_size:train_size + min(2000, len(processed_dataset) - train_size)]\n\nprint(f\"Total examples with actual answers: {len(processed_dataset)}\")\nprint(f\"Training examples: {len(train_dataset)}\")\nprint(f\"Validation examples: {len(val_dataset)}\")\n\n# Free memory\ndel processed_dataset\nfree_memory()\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n\n# Define a processing function for batch tokenization\ndef preprocess_function(examples):\n    batch_size = len(examples)\n    input_texts = []\n    target_texts = []\n    \n    for i in range(batch_size):\n        context = examples[i]['text']\n        question = examples[i]['question']\n        answer = examples[i]['actual_answer']\n        \n        # Format the input (context + question)\n        input_text = f\"Context: {context} Question: {question}\"\n        input_texts.append(input_text)\n        target_texts.append(answer)\n    \n    # Tokenize inputs\n    model_inputs = tokenizer(\n        input_texts,\n        max_length=MAX_INPUT_LENGTH,\n        padding=\"max_length\",\n        truncation=True\n    )\n    \n    # Tokenize targets\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            target_texts,\n            max_length=MAX_TARGET_LENGTH,\n            padding=\"max_length\",\n            truncation=True\n        )\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    \n    # Replace padding token id with -100 so it's ignored in the loss\n    for i in range(len(model_inputs[\"labels\"])):\n        model_inputs[\"labels\"][i] = [\n            -100 if token == tokenizer.pad_token_id else token \n            for token in model_inputs[\"labels\"][i]\n        ]\n    \n    return model_inputs\n\n# Process data in batches to save memory\nprint(\"Processing training data...\")\nbatch_size = 512\ntrain_processed = []\nfor i in tqdm(range(0, len(train_dataset), batch_size)):\n    batch = train_dataset[i:i+batch_size]\n    processed_batch = preprocess_function(batch)\n    for j in range(len(batch)):\n        train_processed.append({\n            \"input_ids\": processed_batch[\"input_ids\"][j],\n            \"attention_mask\": processed_batch[\"attention_mask\"][j],\n            \"labels\": processed_batch[\"labels\"][j]\n        })\n\nprint(\"Processing validation data...\")\nval_processed = []\nfor i in tqdm(range(0, len(val_dataset), batch_size)):\n    batch = val_dataset[i:i+batch_size]\n    processed_batch = preprocess_function(batch)\n    for j in range(len(batch)):\n        val_processed.append({\n            \"input_ids\": processed_batch[\"input_ids\"][j],\n            \"attention_mask\": processed_batch[\"attention_mask\"][j],\n            \"labels\": processed_batch[\"labels\"][j]\n        })\n\n# Free memory\ndel train_dataset, val_dataset\nfree_memory()\n\n# Create PyTorch datasets\nclass MemoryEfficientDataset(Dataset):\n    def __init__(self, examples):\n        self.examples = examples\n    \n    def __len__(self):\n        return len(self.examples)\n    \n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(self.examples[idx][\"input_ids\"]),\n            \"attention_mask\": torch.tensor(self.examples[idx][\"attention_mask\"]),\n            \"labels\": torch.tensor(self.examples[idx][\"labels\"])\n        }\n\ntrain_dataset = MemoryEfficientDataset(train_processed)\nval_dataset = MemoryEfficientDataset(val_processed)\n\n# Free memory\ndel train_processed, val_processed\nfree_memory()\n\n# Load the model\nprint(\"Loading model...\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n\n# Define training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./answer_generation_model\",\n    overwrite_output_dir=True,\n    num_train_epochs=NUM_EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=1,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    logging_dir=\"./logs\",\n    logging_steps=50,\n    learning_rate=LEARNING_RATE,\n    weight_decay=0.01,\n    warmup_ratio=0.05,\n    predict_with_generate=False,\n    fp16=MIXED_PRECISION == \"fp16\",\n    optim=\"adamw_torch\",\n    report_to=\"none\",\n    disable_tqdm=False,\n)\n\n# Set up the trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True),\n)\n\n# Train the model\nprint(\"Training model...\")\ntrainer.train()\n\n# Save the model\nprint(\"Saving model...\")\nmodel.save_pretrained(\"./answer_generation_model_final\")\ntokenizer.save_pretrained(\"./answer_generation_model_final\")\n\n# Test on a few examples\nprint(\"Testing on some examples...\")\nmodel.eval()\ntest_examples = [\n    val_dataset[i] for i in range(min(3, len(val_dataset)))\n]\n\nfor example in test_examples:\n    input_ids = example[\"input_ids\"].unsqueeze(0).to(model.device)\n    attention_mask = example[\"attention_mask\"].unsqueeze(0).to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            max_length=MAX_TARGET_LENGTH,\n            num_beams=2,\n            early_stopping=True\n        )\n    \n    input_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n    predicted_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    print(f\"Input: {input_text}\")\n    print(f\"Predicted Answer: {predicted_answer}\")\n    print(\"=\" * 50)\n\nprint(\"Training complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T12:18:52.411147Z","iopub.execute_input":"2025-03-04T12:18:52.411502Z","iopub.status.idle":"2025-03-04T15:36:44.864936Z","shell.execute_reply.started":"2025-03-04T12:18:52.411469Z","shell.execute_reply":"2025-03-04T15:36:44.864191Z"}},"outputs":[{"name":"stdout","text":"Loading dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/813 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"616075d34ca14926bf546916fd54329f"}},"metadata":{}},{"name":"stdout","text":"Extracting actual answers...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/166000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4db179706bc34cb68fbf7bfdeff36e3f"}},"metadata":{}},{"name":"stdout","text":"Total examples with actual answers: 165884\nTraining examples: 149295\nValidation examples: 2000\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbe5f179eb5846c6b8c14cc46ce9c2fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df8d19a82527461b9ef5cfc4b23af304"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d6e12525fdb4ae393abd3ba2e9ac24b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cf768f2d45e457eadda34a110941dce"}},"metadata":{}},{"name":"stdout","text":"Processing training data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/292 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f9bb10db4784343a8f8b891f38ac027"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Processing validation data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68c37ebbdadf42a9818bc076600737de"}},"metadata":{}},{"name":"stdout","text":"Loading model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcebe68f28ee41bf8f7769ffaecdfadc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4872058235d485284f913d1ccc7be9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77af03e893a04df99093476835b90856"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-1-afd1c075a049>:250: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"},{"name":"stdout","text":"Training model...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='13995' max='13995' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [13995/13995 3:15:28, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.014300</td>\n      <td>0.011770</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.004900</td>\n      <td>0.013781</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nThere were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n","output_type":"stream"},{"name":"stdout","text":"Saving model...\nTesting on some examples...\nInput: Context: The economic concept of \"opportunity cost\" is most closely associated with which of the following management considerations? A. market structure B. resource scarcity C. product demand D. technology Answer:B Question: The economic concept of \"opportunity cost\" is most closely associated with which of the following management considerations?\nPredicted Answer: resource scarcity\n==================================================\nInput: Context: Python is a _______. A. low level language B. high level language C. machine language D. assembly language Answer:B Question: Python is a _______.\nPredicted Answer: high level language\n==================================================\nInput: Context: I forgot the last digit of a 7-digit telephone number. If 1 randomly dial the final 3 digits after correctly dialing the first four, then what is the chance of dialing the correct number? A. 1/999 B. 1/1001 C. 4/1000 D. 1/1000 Answer:D Question: I forgot the last digit of a 7-digit telephone number. If 1 randomly dial the final 3 digits after correctly dialing the first four, then what is the chance of dialing the correct number?\nPredicted Answer: 1/1000\n==================================================\nTraining complete!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Add these imports at the top of your script\nfrom huggingface_hub import login, HfApi\n\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nHF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n\nprint(\"Logging in to Hugging Face Hub...\")\nlogin(token=HF_TOKEN)\n\n# Define your Hugging Face repository name\n# Format: \"username/repository-name\"\nHF_REPO_ID = \"aayeshanakarmi/mcq-answer-generation-redstone-flant5small-2\"  # Replace with your desired repo name\n\n# Save the model to the Hub\nprint(f\"Uploading model to Hugging Face Hub as {HF_REPO_ID}...\")\nmodel.push_to_hub(HF_REPO_ID, use_auth_token=HF_TOKEN)\ntokenizer.push_to_hub(HF_REPO_ID, use_auth_token=HF_TOKEN)\n\n\nprint(f\"Model successfully uploaded to Hugging Face Hub: https://huggingface.co/{HF_REPO_ID}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T15:39:24.543263Z","iopub.execute_input":"2025-03-04T15:39:24.543603Z","iopub.status.idle":"2025-03-04T15:39:54.741789Z","shell.execute_reply.started":"2025-03-04T15:39:24.543579Z","shell.execute_reply":"2025-03-04T15:39:54.740889Z"}},"outputs":[{"name":"stdout","text":"Logging in to Hugging Face Hub...\nUploading model to Hugging Face Hub as aayeshanakarmi/mcq-answer-generation-redstone-flant5small-2...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:894: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"563a2773abd24dcdbe51ca1985873563"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:894: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f835026005443c588b48116b1debc74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8c1c7917c80462f82af6d482b0dede8"}},"metadata":{}},{"name":"stdout","text":"Model successfully uploaded to Hugging Face Hub: https://huggingface.co/aayeshanakarmi/mcq-answer-generation-redstone-flant5small-2\n","output_type":"stream"}],"execution_count":2}]}