{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset, Dataset\nfrom transformers import (\n    T5ForConditionalGeneration,\n    T5Tokenizer,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForSeq2Seq\n)\nimport numpy as np\nimport gc\nimport os\nfrom tqdm.auto import tqdm\n\n# # Create output directories if they don't exist\n# os.makedirs(\"./results/answer_generation\", exist_ok=True)\n# os.makedirs(\"./logs/answer_generation\", exist_ok=True)\n\nos.makedirs(\"/kaggle/working/results/answer_generation\", exist_ok=True)\nos.makedirs(\"/kaggle/working/logs/answer_generation\", exist_ok=True)\n\n\n# Enable memory optimization for PyTorch\ntorch.cuda.empty_cache()\n\n# 1. Load dataset - load only a subset directly\nprint(\"Loading dataset...\")\ndataset = load_dataset(\"zjsd/RedStone-QA-mcq\", split=f\"train[:{int(0.02*100)}%]\")\nprint(f\"Dataset loaded with {len(dataset)} examples\")\n\n# 2. Define preprocessing function for answer generation\ndef preprocess_for_answer_generation(examples, batch_size=64):\n    \"\"\"Preprocess data for training the model to generate the correct answer letter.\"\"\"\n    inputs = []\n    labels = []\n    \n    for i in range(0, len(examples[\"text\"]), batch_size):\n        batch_texts = examples[\"text\"][i:i+batch_size]\n        batch_questions = examples[\"question\"][i:i+batch_size]\n        batch_answers = examples[\"answer\"][i:i+batch_size]\n        \n        for text, question, answer in zip(batch_texts, batch_questions, batch_answers):\n            combined = f\"generate answer: {text} question: {question}\"\n            inputs.append(combined)\n            # Extract just the letter from \"Answer:X\" format\n            labels.append(answer.replace(\"Answer:\", \"\").strip())\n    \n    return {\n        \"input\": inputs,\n        \"output\": labels\n    }\n\n# 3. Process dataset in chunks to save memory\nprint(\"Preprocessing data for answer generation...\")\nanswer_dataset = Dataset.from_dict(preprocess_for_answer_generation(dataset))\n\n# Free up memory\ndel dataset\ngc.collect()\ntorch.cuda.empty_cache()\n\n# 4. Split dataset into train and validation\nprint(\"Splitting dataset into train and validation sets...\")\nanswer_dataset = answer_dataset.train_test_split(test_size=0.1, seed=42)\n\nanswer_train_dataset = answer_dataset[\"train\"]\nanswer_val_dataset = answer_dataset[\"test\"]\n\nprint(f\"Answer generation: {len(answer_train_dataset)} training examples, {len(answer_val_dataset)} validation examples\")\n\n# Free up memory\ndel answer_dataset\ngc.collect()\n\n# 5. Load tokenizer and model\nprint(\"Loading tokenizer and model...\")\nmodel_name = \"google/flan-t5-small\"  # Using the small model for faster training\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\n\n# 6. Define tokenization function\ndef tokenize_function(examples, max_input_length=512, max_target_length=128):\n    model_inputs = tokenizer(\n        examples[\"input\"],\n        max_length=max_input_length,\n        padding=\"max_length\",\n        truncation=True\n    )\n    \n    # Tokenize targets\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            examples[\"output\"],\n            max_length=max_target_length,\n            padding=\"max_length\",\n            truncation=True\n        )\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# 7. Tokenize datasets with smaller batch size to reduce memory usage\nprint(\"Tokenizing datasets...\")\nbatch_size = 32\n\nanswer_train_tokenized = answer_train_dataset.map(\n    tokenize_function, \n    batched=True,\n    batch_size=batch_size,\n    remove_columns=[\"input\", \"output\"]\n)\n\nanswer_val_tokenized = answer_val_dataset.map(\n    tokenize_function, \n    batched=True,\n    batch_size=batch_size,\n    remove_columns=[\"input\", \"output\"]\n)\n\n# Free up memory\ndel answer_train_dataset, answer_val_dataset\ngc.collect()\ntorch.cuda.empty_cache()\n\n# 8. Define custom metrics computation function\ndef compute_answer_metrics(eval_pred):\n    predictions, labels = eval_pred\n    # Replace -100 with the pad_token_id\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    \n    # Get the predicted tokens\n    predicted_tokens = np.argmax(predictions, axis=-1)\n    \n    # Decode to get the actual letters - process in batches to save memory\n    batch_size = 32\n    num_examples = len(predicted_tokens)\n    decoded_preds = []\n    decoded_labels = []\n    \n    for i in range(0, num_examples, batch_size):\n        batch_preds = [tokenizer.decode(pred, skip_special_tokens=True) \n                      for pred in predicted_tokens[i:i+batch_size]]\n        batch_labels = [tokenizer.decode(label, skip_special_tokens=True) \n                       for label in labels[i:i+batch_size]]\n        decoded_preds.extend(batch_preds)\n        decoded_labels.extend(batch_labels)\n    \n    # Calculate accuracy\n    correct = sum(1 for pred, label in zip(decoded_preds, decoded_labels) if pred.strip() == label.strip())\n    accuracy = correct / len(decoded_labels) if len(decoded_labels) > 0 else 0\n    \n    # Print just a few examples for debugging\n    print(\"\\nAnswer Generation Examples (Prediction, Reference):\")\n    for i in range(min(3, len(decoded_preds))):\n        print(f\"  {decoded_preds[i]} | {decoded_labels[i]}\")\n    \n    # Return metrics\n    return {\n        \"accuracy\": accuracy,\n        \"exact_match_ratio\": correct / len(decoded_labels) if len(decoded_labels) > 0 else 0,\n    }\n\n# 9. Define training arguments for answer generation\nanswer_training_args = TrainingArguments(\n    output_dir=\"./results/answer_generation\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=2,\n    weight_decay=0.01,\n    save_total_limit=1,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    greater_is_better=True,\n    logging_dir=\"./logs/answer_generation\",\n    logging_steps=100,\n    report_to=[\"tensorboard\"],\n    # Memory optimizations\n    fp16=True if torch.cuda.is_available() else False,  # Use mixed precision if available\n    gradient_accumulation_steps=2,  # Accumulate gradients to simulate larger batch sizes\n    dataloader_num_workers=1,  # Parallelize data loading\n    dataloader_pin_memory=True,  # Speed up data transfer to GPU\n    # Ensure progress bar is shown\n    disable_tqdm=False,\n)\n\n# 10. Define data collator\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer,\n    model=model,\n    padding=\"max_length\",\n    max_length=512,\n    pad_to_multiple_of=8  # Optimize for tensor operations\n)\n\n# 11. Train answer generation model\nprint(\"Initializing answer generation trainer...\")\nanswer_trainer = Trainer(\n    model=model,\n    args=answer_training_args,\n    train_dataset=answer_train_tokenized,\n    eval_dataset=answer_val_tokenized,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_answer_metrics,\n)\n\nprint(\"Training answer generation model...\")\nanswer_trainer.train()\n\n# Get validation results\nprint(\"Evaluating answer generation model...\")\nanswer_eval_results = answer_trainer.evaluate()\nprint(\"\\n\" + \"=\"*50)\nprint(\"ANSWER GENERATION EVALUATION RESULTS\")\nprint(\"=\"*50)\nprint(f\"Accuracy: {answer_eval_results['eval_accuracy']:.4f}\")\nprint(f\"Exact Match: {answer_eval_results['eval_exact_match_ratio']:.4f}\")\nprint(\"=\"*50 + \"\\n\")\n\n# Save model\nprint(\"Saving final model...\")\nanswer_trainer.save_model(\"./results/answer_generation/final_model\")\nprint(\"Model saved successfully!\")\n\n# 12. Memory-efficient inference function for demonstration\ndef generate_answer(context, question, model, tokenizer):\n    # Set model to evaluation mode\n    model.eval()\n    \n    # Get the device\n    device = next(model.parameters()).device\n    \n    # Prepare input\n    answer_input = f\"generate answer: {context} question: {question}\"\n    answer_input_ids = tokenizer(answer_input, return_tensors=\"pt\").input_ids.to(device)\n    \n    print(\"Generating answer letter...\")\n    with torch.no_grad():  # Disable gradient calculation to save memory\n        answer_outputs = model.generate(\n            answer_input_ids, \n            max_length=10,\n            num_beams=4,\n            early_stopping=True\n        )\n    answer_letter = tokenizer.decode(answer_outputs[0], skip_special_tokens=True).strip()\n    print(f\"Generated answer letter: {answer_letter}\")\n    \n    # Clean up GPU memory\n    del answer_outputs, answer_input_ids\n    torch.cuda.empty_cache()\n    \n    return answer_letter\n\n# Optional: Demonstrate the trained model\nprint(\"\\n\" + \"=\"*50)\nprint(\"DEMONSTRATION\")\nprint(\"=\"*50)\ntry:\n    # Sample context and question\n    context = \"The Python programming language was created by Guido van Rossum and first released in 1991. It emphasizes code readability with its notable use of significant whitespace.\"\n    question = \"Who created the Python programming language?\"\n    \n    # Generate answer\n    answer_letter = generate_answer(context, question, model, tokenizer)\n    print(f\"Context: {context}\")\n    print(f\"Question: {question}\")\n    print(f\"Generated Answer: {answer_letter}\")\nexcept Exception as e:\n    print(f\"Error during demonstration: {e}\")\n\nprint(\"\\nTraining of Answer Generation model complete!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-03T08:11:24.223996Z","iopub.execute_input":"2025-03-03T08:11:24.224185Z","iopub.status.idle":"2025-03-03T08:43:55.552208Z","shell.execute_reply.started":"2025-03-03T08:11:24.224167Z","shell.execute_reply":"2025-03-03T08:43:55.550799Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n    DataCollatorForSeq2Seq\n)\nimport torch\nfrom torch.utils.data import Dataset\nimport random\nimport gc\nfrom tqdm.auto import tqdm\n\n# Set random seeds for reproducibility\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\ntorch.cuda.empty_cache()\n\n# Define constants\nMODEL_NAME = \"google/flan-t5-small\"\nMAX_INPUT_LENGTH = 384  # Reduced from 512\nMAX_TARGET_LENGTH = 48  # Reduced from 64\nBATCH_SIZE = 4  # Reduced from 8\nGRADIENT_ACCUMULATION_STEPS = 4  # Increases effective batch size without increasing memory\nLEARNING_RATE = 3e-4\nNUM_EPOCHS = 3\nSAMPLE_RATIO = 0.1  # Reduced from 0.15\nMIXED_PRECISION = \"fp16\"  # Use mixed precision training\n\n# Function to free memory\ndef free_memory():\n    gc.collect()\n    torch.cuda.empty_cache()\n\n# Load the dataset in streaming mode to reduce memory usage\nprint(\"Loading dataset...\")\ndataset = load_dataset(\"zjsd/RedStone-QA-mcq\", split=\"train\", streaming=True)\n\n# Convert to regular dataset for sampling\n# Buffer a smaller amount to not load everything in memory\ndataset = dataset.take(int(1.66e6 * SAMPLE_RATIO * 1.2))  # Buffer slightly more than needed\ndataset = list(dataset)\nrandom.shuffle(dataset)\ndataset = dataset[:int(1.66e6 * SAMPLE_RATIO)]\n\n# Split into train and validation\ntrain_val_split = 0.9\ntrain_size = int(len(dataset) * train_val_split)\n\n# Create train and validation datasets\ntrain_dataset = dataset[:train_size]\nval_dataset = dataset[train_size:train_size + min(2000, len(dataset) - train_size)]  # Limit validation set\n\nprint(f\"Total sampled examples: {len(dataset)}\")\nprint(f\"Training examples: {len(train_dataset)}\")\nprint(f\"Validation examples: {len(val_dataset)}\")\n\n# Free memory\ndel dataset\nfree_memory()\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n\n# Define a processing function for batch tokenization\ndef preprocess_function(examples):\n    batch_size = len(examples)\n    input_texts = []\n    target_texts = []\n    \n    for i in range(batch_size):\n        context = examples[i]['text']\n        question = examples[i]['question']\n        answer = examples[i]['answer'].replace(\"Answer:\", \"\").strip()\n        \n        # Format the input (context + question)\n        input_text = f\"Context: {context} Question: {question}\"\n        input_texts.append(input_text)\n        target_texts.append(answer)\n    \n    # Tokenize inputs\n    model_inputs = tokenizer(\n        input_texts,\n        max_length=MAX_INPUT_LENGTH,\n        padding=\"max_length\",\n        truncation=True\n    )\n    \n    # Tokenize targets\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            target_texts,\n            max_length=MAX_TARGET_LENGTH,\n            padding=\"max_length\",\n            truncation=True\n        )\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    \n    # Replace padding token id with -100 so it's ignored in the loss\n    for i in range(len(model_inputs[\"labels\"])):\n        model_inputs[\"labels\"][i] = [\n            -100 if token == tokenizer.pad_token_id else token \n            for token in model_inputs[\"labels\"][i]\n        ]\n    \n    return model_inputs\n\n# Process data in batches to save memory\nprint(\"Processing training data...\")\nbatch_size = 512\ntrain_processed = []\nfor i in tqdm(range(0, len(train_dataset), batch_size)):\n    batch = train_dataset[i:i+batch_size]\n    processed_batch = preprocess_function(batch)\n    for j in range(len(batch)):\n        train_processed.append({\n            \"input_ids\": processed_batch[\"input_ids\"][j],\n            \"attention_mask\": processed_batch[\"attention_mask\"][j],\n            \"labels\": processed_batch[\"labels\"][j]\n        })\n\nprint(\"Processing validation data...\")\nval_processed = []\nfor i in tqdm(range(0, len(val_dataset), batch_size)):\n    batch = val_dataset[i:i+batch_size]\n    processed_batch = preprocess_function(batch)\n    for j in range(len(batch)):\n        val_processed.append({\n            \"input_ids\": processed_batch[\"input_ids\"][j],\n            \"attention_mask\": processed_batch[\"attention_mask\"][j],\n            \"labels\": processed_batch[\"labels\"][j]\n        })\n\n# Free memory\ndel train_dataset, val_dataset\nfree_memory()\n\n# Create PyTorch datasets\nclass MemoryEfficientDataset(Dataset):\n    def __init__(self, examples):\n        self.examples = examples\n    \n    def __len__(self):\n        return len(self.examples)\n    \n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(self.examples[idx][\"input_ids\"]),\n            \"attention_mask\": torch.tensor(self.examples[idx][\"attention_mask\"]),\n            \"labels\": torch.tensor(self.examples[idx][\"labels\"])\n        }\n\ntrain_dataset = MemoryEfficientDataset(train_processed)\nval_dataset = MemoryEfficientDataset(val_processed)\n\n# Free memory\ndel train_processed, val_processed\nfree_memory()\n\n# Load the model with 8-bit precision to save memory\nprint(\"Loading model...\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n\n# Define training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./answer_generation_model\",\n    overwrite_output_dir=True,\n    num_train_epochs=NUM_EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=1,  # Keep only the best model\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    logging_dir=\"./logs\",\n    logging_steps=50,\n    learning_rate=LEARNING_RATE,\n    weight_decay=0.01,\n    warmup_ratio=0.05,\n    predict_with_generate=False,  # Save memory during evaluation\n    fp16=MIXED_PRECISION == \"fp16\",  # Mixed precision training\n    optim=\"adamw_torch\",\n    report_to=\"none\",  # Disable W&B reporting\n    disable_tqdm=False,  # Enable tqdm progress bar\n)\n\n# Set up the trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True),\n)\n\n# Train the model\nprint(\"Training model...\")\ntrainer.train()\n\n# Save the model\nprint(\"Saving model...\")\nmodel.save_pretrained(\"./answer_generation_model_final\")\ntokenizer.save_pretrained(\"./answer_generation_model_final\")\n\n# Test on a few examples\nprint(\"Testing on some examples...\")\nmodel.eval()\ntest_examples = [\n    val_dataset[i] for i in range(min(3, len(val_dataset)))\n]\n\nfor example in test_examples:\n    input_ids = example[\"input_ids\"].unsqueeze(0).to(model.device)\n    attention_mask = example[\"attention_mask\"].unsqueeze(0).to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            max_length=MAX_TARGET_LENGTH,\n            num_beams=2,  # Reduced for memory efficiency\n            early_stopping=True\n        )\n    \n    input_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n    predicted_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    print(f\"Input: {input_text}\")\n    print(f\"Predicted Answer: {predicted_answer}\")\n    print(\"=\" * 50)\n\nprint(\"Training complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T08:48:44.213065Z","iopub.execute_input":"2025-03-03T08:48:44.213352Z","iopub.status.idle":"2025-03-03T12:09:06.047702Z","shell.execute_reply.started":"2025-03-03T08:48:44.213329Z","shell.execute_reply":"2025-03-03T12:09:06.046918Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n    DataCollatorForSeq2Seq\n)\nimport torch\nfrom torch.utils.data import Dataset\nimport random\nimport gc\nfrom tqdm.auto import tqdm\nimport re\n\n# Set random seeds for reproducibility\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\ntorch.cuda.empty_cache()\n\n# Define constants\nMODEL_NAME = \"google/flan-t5-small\"\nMAX_INPUT_LENGTH = 384\nMAX_TARGET_LENGTH = 96\nBATCH_SIZE = 4\nGRADIENT_ACCUMULATION_STEPS = 4\nLEARNING_RATE = 3e-4\nNUM_EPOCHS = 3\nSAMPLE_RATIO = 0.1\nMIXED_PRECISION = \"fp16\"\n\n# Function to free memory\ndef free_memory():\n    gc.collect()\n    torch.cuda.empty_cache()\n\n# Extract actual answer content and distractors\ndef extract_answers_and_distractors(row):\n    correct_answer = row['answer'].replace(\"Answer:\", \"\").strip()\n    choices = row['choices']\n    \n    # Identify correct answer letter\n    correct_letter = None\n    \n    # If answer is just a letter (A, B, C, D)\n    if re.match(r'^[A-D]$', correct_answer):\n        correct_letter = correct_answer\n    else:\n        # If answer starts with a letter followed by a period/colon (A., A:)\n        match = re.match(r'^([A-D])[:\\.]?\\s*', correct_answer)\n        if match:\n            correct_letter = match.group(1)\n    \n    # If we still don't have a letter, try to infer from content\n    if not correct_letter:\n        correct_content = correct_answer\n        for choice in choices:\n            parts = re.match(r'^([A-D])[.\\s]+(.*)', choice)\n            if parts:\n                letter, content = parts.groups()\n                # If the content matches (approximately) the correct answer\n                if content.strip().lower() in correct_answer.lower() or correct_answer.lower() in content.strip().lower():\n                    correct_letter = letter\n                    correct_content = content.strip()\n                    break\n    \n    # If we still don't have a letter, take a guess based on position\n    if not correct_letter and \"A\" in correct_answer:\n        correct_letter = \"A\"\n    elif not correct_letter and \"B\" in correct_answer:\n        correct_letter = \"B\"\n    elif not correct_letter and \"C\" in correct_answer:\n        correct_letter = \"C\"\n    elif not correct_letter and \"D\" in correct_answer:\n        correct_letter = \"D\"\n    elif not correct_letter and len(choices) > 0:\n        correct_letter = \"A\"  # Default to first option\n    \n    # Extract correct answer content and distractors\n    correct_content = None\n    distractors = []\n    \n    for choice in choices:\n        parts = re.match(r'^([A-D])[.\\s]+(.*)', choice)\n        if parts:\n            letter, content = parts.groups()\n            content = content.strip()\n            if letter == correct_letter:\n                correct_content = content\n            elif content:  # Only add non-empty distractors\n                distractors.append(content)\n    \n    # If we didn't extract content from choices, use the original answer\n    if not correct_content:\n        if re.match(r'^[A-D][:\\.]?\\s+(.+)$', correct_answer):\n            correct_content = re.match(r'^[A-D][:\\.]?\\s+(.+)$', correct_answer).group(1)\n        else:\n            correct_content = correct_answer\n    \n    return correct_content, distractors\n\n# Load the dataset in streaming mode to reduce memory usage\nprint(\"Loading dataset...\")\ndataset = load_dataset(\"zjsd/RedStone-QA-mcq\", split=\"train\", streaming=True)\n\n# Convert to regular dataset for sampling\ndataset = dataset.take(int(1.66e6 * SAMPLE_RATIO * 1.2))\ndataset = list(dataset)\nrandom.shuffle(dataset)\ndataset = dataset[:int(1.66e6 * SAMPLE_RATIO)]\n\n# Process dataset to extract actual answers and distractors\nprint(\"Extracting answers and distractors...\")\nprocessed_dataset = []\nfor row in tqdm(dataset):\n    actual_answer, distractors = extract_answers_and_distractors(row)\n    if actual_answer and distractors:  # Only keep examples with valid answers and distractors\n        row['actual_answer'] = actual_answer\n        row['distractors_list'] = distractors\n        processed_dataset.append(row)\n\n# Free memory\ndel dataset\nfree_memory()\n\n# Split into train and validation\ntrain_val_split = 0.9\ntrain_size = int(len(processed_dataset) * train_val_split)\n\n# Create train and validation datasets\ntrain_dataset = processed_dataset[:train_size]\nval_dataset = processed_dataset[train_size:train_size + min(2000, len(processed_dataset) - train_size)]\n\nprint(f\"Total processed examples: {len(processed_dataset)}\")\nprint(f\"Training examples: {len(train_dataset)}\")\nprint(f\"Validation examples: {len(val_dataset)}\")\n\n# Free memory\ndel processed_dataset\nfree_memory()\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n\n# Define a processing function for batch tokenization\ndef preprocess_function(examples):\n    batch_size = len(examples)\n    input_texts = []\n    target_texts = []\n    \n    for i in range(batch_size):\n        context = examples[i]['text']\n        question = examples[i]['question']\n        answer = examples[i]['actual_answer']\n        \n        # Format distractors as a single string with separators\n        distractors = \" | \".join(examples[i]['distractors_list'])\n        \n        # Format the input (context + question + correct answer)\n        input_text = f\"Context: {context} Question: {question} Correct Answer: {answer}\"\n        input_texts.append(input_text)\n        target_texts.append(distractors)\n    \n    # Tokenize inputs\n    model_inputs = tokenizer(\n        input_texts,\n        max_length=MAX_INPUT_LENGTH,\n        padding=\"max_length\",\n        truncation=True\n    )\n    \n    # Tokenize targets\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            target_texts,\n            max_length=MAX_TARGET_LENGTH,\n            padding=\"max_length\",\n            truncation=True\n        )\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    \n    # Replace padding token id with -100 so it's ignored in the loss\n    for i in range(len(model_inputs[\"labels\"])):\n        model_inputs[\"labels\"][i] = [\n            -100 if token == tokenizer.pad_token_id else token \n            for token in model_inputs[\"labels\"][i]\n        ]\n    \n    return model_inputs\n\n# Process data in batches to save memory\nprint(\"Processing training data...\")\nbatch_size = 512\ntrain_processed = []\nfor i in tqdm(range(0, len(train_dataset), batch_size)):\n    batch = train_dataset[i:i+batch_size]\n    processed_batch = preprocess_function(batch)\n    for j in range(len(batch)):\n        train_processed.append({\n            \"input_ids\": processed_batch[\"input_ids\"][j],\n            \"attention_mask\": processed_batch[\"attention_mask\"][j],\n            \"labels\": processed_batch[\"labels\"][j]\n        })\n\nprint(\"Processing validation data...\")\nval_processed = []\nfor i in tqdm(range(0, len(val_dataset), batch_size)):\n    batch = val_dataset[i:i+batch_size]\n    processed_batch = preprocess_function(batch)\n    for j in range(len(batch)):\n        val_processed.append({\n            \"input_ids\": processed_batch[\"input_ids\"][j],\n            \"attention_mask\": processed_batch[\"attention_mask\"][j],\n            \"labels\": processed_batch[\"labels\"][j]\n        })\n\n# Free memory\ndel train_dataset, val_dataset\nfree_memory()\n\n# Create PyTorch datasets\nclass MemoryEfficientDataset(Dataset):\n    def __init__(self, examples):\n        self.examples = examples\n    \n    def __len__(self):\n        return len(self.examples)\n    \n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(self.examples[idx][\"input_ids\"]),\n            \"attention_mask\": torch.tensor(self.examples[idx][\"attention_mask\"]),\n            \"labels\": torch.tensor(self.examples[idx][\"labels\"])\n        }\n\ntrain_dataset = MemoryEfficientDataset(train_processed)\nval_dataset = MemoryEfficientDataset(val_processed)\n\n# Free memory\ndel train_processed, val_processed\nfree_memory()\n\n# Load the model\nprint(\"Loading model...\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n\n# Define training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./distractor_generation_model\",\n    overwrite_output_dir=True,\n    num_train_epochs=NUM_EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=1,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    logging_dir=\"./logs\",\n    logging_steps=50,\n    learning_rate=LEARNING_RATE,\n    weight_decay=0.01,\n    warmup_ratio=0.05,\n    predict_with_generate=False,\n    fp16=MIXED_PRECISION == \"fp16\",\n    optim=\"adamw_torch\",\n    report_to=\"none\",\n    disable_tqdm=False,\n)\n\n# Set up the trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True),\n)\n\n# Train the model\nprint(\"Training model...\")\ntrainer.train()\n\n# Save the model\nprint(\"Saving model...\")\nmodel.save_pretrained(\"./distractor_generation_model_final\")\ntokenizer.save_pretrained(\"./distractor_generation_model_final\")\n\n# Test on a few examples\nprint(\"Testing on some examples...\")\nmodel.eval()\ntest_examples = [\n    val_dataset[i] for i in range(min(3, len(val_dataset)))\n]\n\nfor example in test_examples:\n    input_ids = example[\"input_ids\"].unsqueeze(0).to(model.device)\n    attention_mask = example[\"attention_mask\"].unsqueeze(0).to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            max_length=MAX_TARGET_LENGTH,\n            num_beams=2,\n            early_stopping=True\n        )\n    \n    input_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n    predicted_distractors = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    print(f\"Input: {input_text}\")\n    print(f\"Predicted Distractors: {predicted_distractors}\")\n    print(\"=\" * 50)\n\nprint(\"Training complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T12:38:37.689231Z","iopub.execute_input":"2025-03-03T12:38:37.689584Z","iopub.status.idle":"2025-03-03T16:18:33.099170Z","shell.execute_reply.started":"2025-03-03T12:38:37.689552Z","shell.execute_reply":"2025-03-03T16:18:33.098435Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Add these imports at the top of your script\nfrom huggingface_hub import login, HfApi\n\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nHF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n\nprint(\"Logging in to Hugging Face Hub...\")\nlogin(token=HF_TOKEN)\n\n# Define your Hugging Face repository name\n# Format: \"username/repository-name\"\nHF_REPO_ID = \"aayeshanakarmi/distractor-generation-redstone-flant5small-2\"  # Replace with your desired repo name\n\n# Save the model to the Hub\nprint(f\"Uploading model to Hugging Face Hub as {HF_REPO_ID}...\")\nmodel.push_to_hub(HF_REPO_ID, use_auth_token=HF_TOKEN)\ntokenizer.push_to_hub(HF_REPO_ID, use_auth_token=HF_TOKEN)\n\n\nprint(f\"Model successfully uploaded to Hugging Face Hub: https://huggingface.co/{HF_REPO_ID}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T16:36:09.624146Z","iopub.execute_input":"2025-03-03T16:36:09.624502Z","iopub.status.idle":"2025-03-03T16:36:22.854002Z","shell.execute_reply.started":"2025-03-03T16:36:09.624472Z","shell.execute_reply":"2025-03-03T16:36:22.853190Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Answer Generation Model ","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n    DataCollatorForSeq2Seq\n)\nimport torch\nfrom torch.utils.data import Dataset\nimport random\nimport gc\nfrom tqdm.auto import tqdm\nimport re\n\n# Set random seeds for reproducibility\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\ntorch.cuda.empty_cache()\n\n# Define constants\nMODEL_NAME = \"google/flan-t5-small\"\nMAX_INPUT_LENGTH = 384\nMAX_TARGET_LENGTH = 48\nBATCH_SIZE = 4\nGRADIENT_ACCUMULATION_STEPS = 4\nLEARNING_RATE = 3e-4\nNUM_EPOCHS = 3\nSAMPLE_RATIO = 0.1\nMIXED_PRECISION = \"fp16\"\n\n# Function to free memory\ndef free_memory():\n    gc.collect()\n    torch.cuda.empty_cache()\n\n# Enhanced function to extract actual answer content (not just A, B, C, D)\ndef extract_actual_answer(row):\n    correct_answer = row['answer'].replace(\"Answer:\", \"\").strip()\n    choices = row['choices']\n    \n    # If answer is just a single letter (A, B, C, D)\n    if re.match(r'^[A-D]$', correct_answer):\n        letter = correct_answer\n        for choice in choices:\n            # Look for the choice that starts with this letter\n            if choice.startswith(letter + \".\") or choice.startswith(letter + \" \") or choice.startswith(letter + \":\"):\n                # Extract everything after the letter and separator\n                content = re.sub(r'^[A-D][.\\s:]+', '', choice).strip()\n                return content\n    \n    # If answer starts with letter followed by period/colon/space (e.g., \"A. text\", \"A: text\", \"A text\")\n    match = re.match(r'^([A-D])[.\\s:]+(.+)$', correct_answer)\n    if match:\n        # Extract the content part after the letter\n        content = match.group(2).strip()\n        if content:\n            return content\n        \n        # If no content after the letter in the answer, find it in choices\n        letter = match.group(1)\n        for choice in choices:\n            if choice.startswith(letter + \".\") or choice.startswith(letter + \" \") or choice.startswith(letter + \":\"):\n                content = re.sub(r'^[A-D][.\\s:]+', '', choice).strip()\n                return content\n    \n    # Handle case where the answer might be the full text that matches one of the choices\n    for choice in choices:\n        # Extract the content part of the choice (removing any leading A., B., etc.)\n        choice_content = re.sub(r'^[A-D][.\\s:]+', '', choice).strip()\n        # If the answer matches this content exactly, return it\n        if correct_answer == choice_content:\n            return correct_answer\n    \n    # If we couldn't match it to a specific choice or extract a letter,\n    # just return the original answer as a fallback\n    return correct_answer\n\n# Load the dataset in streaming mode to reduce memory usage\nprint(\"Loading dataset...\")\ndataset = load_dataset(\"zjsd/RedStone-QA-mcq\", split=\"train\", streaming=True)\n\n# Convert to regular dataset for sampling\ndataset = dataset.take(int(1.66e6 * SAMPLE_RATIO * 1.2))\ndataset = list(dataset)\nrandom.shuffle(dataset)\ndataset = dataset[:int(1.66e6 * SAMPLE_RATIO)]\n\n# Process dataset to extract actual answers\nprint(\"Extracting actual answers...\")\nprocessed_dataset = []\nfor row in tqdm(dataset):\n    actual_answer = extract_actual_answer(row)\n    if actual_answer.strip():  # Only keep examples with non-empty answers\n        row['actual_answer'] = actual_answer\n        processed_dataset.append(row)\n\n# Free memory\ndel dataset\nfree_memory()\n\n# Split into train and validation\ntrain_val_split = 0.9\ntrain_size = int(len(processed_dataset) * train_val_split)\n\n# Create train and validation datasets\ntrain_dataset = processed_dataset[:train_size]\nval_dataset = processed_dataset[train_size:train_size + min(2000, len(processed_dataset) - train_size)]\n\nprint(f\"Total examples with actual answers: {len(processed_dataset)}\")\nprint(f\"Training examples: {len(train_dataset)}\")\nprint(f\"Validation examples: {len(val_dataset)}\")\n\n# Free memory\ndel processed_dataset\nfree_memory()\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n\n# Define a processing function for batch tokenization\ndef preprocess_function(examples):\n    batch_size = len(examples)\n    input_texts = []\n    target_texts = []\n    \n    for i in range(batch_size):\n        context = examples[i]['text']\n        question = examples[i]['question']\n        answer = examples[i]['actual_answer']\n        \n        # Format the input (context + question)\n        input_text = f\"Context: {context} Question: {question}\"\n        input_texts.append(input_text)\n        target_texts.append(answer)\n    \n    # Tokenize inputs\n    model_inputs = tokenizer(\n        input_texts,\n        max_length=MAX_INPUT_LENGTH,\n        padding=\"max_length\",\n        truncation=True\n    )\n    \n    # Tokenize targets\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            target_texts,\n            max_length=MAX_TARGET_LENGTH,\n            padding=\"max_length\",\n            truncation=True\n        )\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    \n    # Replace padding token id with -100 so it's ignored in the loss\n    for i in range(len(model_inputs[\"labels\"])):\n        model_inputs[\"labels\"][i] = [\n            -100 if token == tokenizer.pad_token_id else token \n            for token in model_inputs[\"labels\"][i]\n        ]\n    \n    return model_inputs\n\n# Process data in batches to save memory\nprint(\"Processing training data...\")\nbatch_size = 512\ntrain_processed = []\nfor i in tqdm(range(0, len(train_dataset), batch_size)):\n    batch = train_dataset[i:i+batch_size]\n    processed_batch = preprocess_function(batch)\n    for j in range(len(batch)):\n        train_processed.append({\n            \"input_ids\": processed_batch[\"input_ids\"][j],\n            \"attention_mask\": processed_batch[\"attention_mask\"][j],\n            \"labels\": processed_batch[\"labels\"][j]\n        })\n\nprint(\"Processing validation data...\")\nval_processed = []\nfor i in tqdm(range(0, len(val_dataset), batch_size)):\n    batch = val_dataset[i:i+batch_size]\n    processed_batch = preprocess_function(batch)\n    for j in range(len(batch)):\n        val_processed.append({\n            \"input_ids\": processed_batch[\"input_ids\"][j],\n            \"attention_mask\": processed_batch[\"attention_mask\"][j],\n            \"labels\": processed_batch[\"labels\"][j]\n        })\n\n# Free memory\ndel train_dataset, val_dataset\nfree_memory()\n\n# Create PyTorch datasets\nclass MemoryEfficientDataset(Dataset):\n    def __init__(self, examples):\n        self.examples = examples\n    \n    def __len__(self):\n        return len(self.examples)\n    \n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(self.examples[idx][\"input_ids\"]),\n            \"attention_mask\": torch.tensor(self.examples[idx][\"attention_mask\"]),\n            \"labels\": torch.tensor(self.examples[idx][\"labels\"])\n        }\n\ntrain_dataset = MemoryEfficientDataset(train_processed)\nval_dataset = MemoryEfficientDataset(val_processed)\n\n# Free memory\ndel train_processed, val_processed\nfree_memory()\n\n# Load the model\nprint(\"Loading model...\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n\n# Define training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./answer_generation_model\",\n    overwrite_output_dir=True,\n    num_train_epochs=NUM_EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=1,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    logging_dir=\"./logs\",\n    logging_steps=50,\n    learning_rate=LEARNING_RATE,\n    weight_decay=0.01,\n    warmup_ratio=0.05,\n    predict_with_generate=False,\n    fp16=MIXED_PRECISION == \"fp16\",\n    optim=\"adamw_torch\",\n    report_to=\"none\",\n    disable_tqdm=False,\n)\n\n# Set up the trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True),\n)\n\n# Train the model\nprint(\"Training model...\")\ntrainer.train()\n\n# Save the model\nprint(\"Saving model...\")\nmodel.save_pretrained(\"./answer_generation_model_final\")\ntokenizer.save_pretrained(\"./answer_generation_model_final\")\n\n# Test on a few examples\nprint(\"Testing on some examples...\")\nmodel.eval()\ntest_examples = [\n    val_dataset[i] for i in range(min(3, len(val_dataset)))\n]\n\nfor example in test_examples:\n    input_ids = example[\"input_ids\"].unsqueeze(0).to(model.device)\n    attention_mask = example[\"attention_mask\"].unsqueeze(0).to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            max_length=MAX_TARGET_LENGTH,\n            num_beams=2,\n            early_stopping=True\n        )\n    \n    input_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n    predicted_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    print(f\"Input: {input_text}\")\n    print(f\"Predicted Answer: {predicted_answer}\")\n    print(\"=\" * 50)\n\nprint(\"Training complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T12:18:52.411147Z","iopub.execute_input":"2025-03-04T12:18:52.411502Z","iopub.status.idle":"2025-03-04T15:36:44.864936Z","shell.execute_reply.started":"2025-03-04T12:18:52.411469Z","shell.execute_reply":"2025-03-04T15:36:44.864191Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Add these imports at the top of your script\nfrom huggingface_hub import login, HfApi\n\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nHF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n\nprint(\"Logging in to Hugging Face Hub...\")\nlogin(token=HF_TOKEN)\n\n# Define your Hugging Face repository name\n# Format: \"username/repository-name\"\nHF_REPO_ID = \"aayeshanakarmi/mcq-answer-generation-redstone-flant5small-2\"  # Replace with your desired repo name\n\n# Save the model to the Hub\nprint(f\"Uploading model to Hugging Face Hub as {HF_REPO_ID}...\")\nmodel.push_to_hub(HF_REPO_ID, use_auth_token=HF_TOKEN)\ntokenizer.push_to_hub(HF_REPO_ID, use_auth_token=HF_TOKEN)\n\n\nprint(f\"Model successfully uploaded to Hugging Face Hub: https://huggingface.co/{HF_REPO_ID}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T15:39:24.543263Z","iopub.execute_input":"2025-03-04T15:39:24.543603Z","iopub.status.idle":"2025-03-04T15:39:54.741789Z","shell.execute_reply.started":"2025-03-04T15:39:24.543579Z","shell.execute_reply":"2025-03-04T15:39:54.740889Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inferencing","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nimport random\n\nclass MCQGenerator:\n    def __init__(self):\n        # Load the answer generation model\n        print(\"Loading answer generation model...\")\n        self.answer_model = AutoModelForSeq2SeqLM.from_pretrained(\"aayeshanakarmi/mcq-answer-generation-redstone-flant5small-2\")\n        self.answer_tokenizer = AutoTokenizer.from_pretrained(\"aayeshanakarmi/mcq-answer-generation-redstone-flant5small-2\")\n        \n        # Load the distractor generation model\n        print(\"Loading distractor generation model...\")\n        self.distractor_model = AutoModelForSeq2SeqLM.from_pretrained(\"aayeshanakarmi/distractor-generation-redstone-flant5small-2\")\n        self.distractor_tokenizer = AutoTokenizer.from_pretrained(\"aayeshanakarmi/distractor-generation-redstone-flant5small-2\")\n        \n        # Set models to evaluation mode\n        self.answer_model.eval()\n        self.distractor_model.eval()\n        \n        # Move models to GPU if available\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.answer_model.to(self.device)\n        self.distractor_model.to(self.device)\n        \n        print(f\"Models loaded and running on {self.device}\")\n    \n    def generate_answer(self, context, question, max_length=48):\n        \"\"\"Generate the correct answer based on context and question\"\"\"\n        # Format input as expected by the answer model\n        input_text = f\"Context: {context} Question: {question}\"\n        \n        # Tokenize input\n        input_ids = self.answer_tokenizer(input_text, return_tensors=\"pt\", max_length=384, \n                                          truncation=True).input_ids.to(self.device)\n        \n        # Generate answer\n        with torch.no_grad():\n            output_ids = self.answer_model.generate(\n                input_ids=input_ids,\n                max_length=max_length,\n                num_beams=4,\n                early_stopping=True\n            )\n        \n        # Decode the output\n        answer = self.answer_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n        return answer.strip()\n    \n    def generate_distractors(self, context, question, answer, max_length=96, num_distractors=3):\n        \"\"\"Generate distractors based on context, question and correct answer\"\"\"\n        # Format input as expected by the distractor model\n        input_text = f\"Context: {context} Question: {question} Correct Answer: {answer}\"\n        \n        # Tokenize input\n        input_ids = self.distractor_tokenizer(input_text, return_tensors=\"pt\", max_length=384, \n                                             truncation=True).input_ids.to(self.device)\n        \n        # Generate distractors\n        with torch.no_grad():\n            output_ids = self.distractor_model.generate(\n                input_ids=input_ids,\n                max_length=max_length,\n                num_beams=4,\n                do_sample=True,\n                temperature=0.8,\n                early_stopping=True\n            )\n        \n        # Decode the output\n        distractors_text = self.distractor_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n        \n        # Split the distractors - they were joined with | during training\n        distractors = [d.strip() for d in distractors_text.split(\"|\") if d.strip()]\n        \n        # Ensure we have unique distractors that are different from the answer\n        unique_distractors = []\n        for d in distractors:\n            if d.lower() != answer.lower() and d not in unique_distractors:\n                unique_distractors.append(d)\n                if len(unique_distractors) >= num_distractors:\n                    break\n        \n        # If we don't have enough distractors, generate some basic alternatives\n        while len(unique_distractors) < num_distractors:\n            fallback = f\"Alternative answer {len(unique_distractors) + 1}\"\n            if fallback not in unique_distractors and fallback.lower() != answer.lower():\n                unique_distractors.append(fallback)\n        \n        return unique_distractors\n    \n    def generate_mcq(self, context, question, shuffle=True):\n        \"\"\"Generate a complete MCQ with context, question, and options\"\"\"\n        # Generate the correct answer\n        correct_answer = self.generate_answer(context, question)\n        \n        # Generate distractors\n        distractors = self.generate_distractors(context, question, correct_answer)\n        \n        # Create options (correct answer + distractors)\n        options = [correct_answer] + distractors\n        \n        # Shuffle options if requested\n        correct_idx = 0\n        if shuffle:\n            correct_idx = random.randint(0, len(options) - 1)\n            shuffled_options = options.copy()\n            # Move correct answer to the randomly chosen position\n            shuffled_options[0], shuffled_options[correct_idx] = shuffled_options[correct_idx], shuffled_options[0]\n            options = shuffled_options\n        \n        # Format as MCQ\n        option_labels = [\"A\", \"B\", \"C\", \"D\"][:len(options)]\n        formatted_options = [f\"{label}. {option}\" for label, option in zip(option_labels, options)]\n        \n        # Identify the correct answer label\n        correct_label = option_labels[correct_idx]\n        \n        # Create the complete MCQ\n        mcq = {\n            \"context\": context,\n            \"question\": question,\n            \"options\": formatted_options,\n            \"correct_answer\": f\"{correct_label}. {options[correct_idx]}\",\n            \"correct_label\": correct_label\n        }\n        \n        return mcq\n    \n    def format_mcq_as_text(self, mcq):\n        \"\"\"Format the MCQ as a readable text\"\"\"\n        text = f\"Context:\\n{mcq['context']}\\n\\n\"\n        text += f\"Question:\\n{mcq['question']}\\n\\n\"\n        text += \"Options:\\n\"\n        for option in mcq['options']:\n            text += f\"{option}\\n\"\n        text += f\"\\nCorrect Answer: {mcq['correct_label']}\"\n        return text\n\n\n# Example usage\ndef generate_example_mcq():\n    # Example context and question\n    context = \"\"\"The water cycle, also known as the hydrologic cycle, describes the continuous movement of water on, above, and below the surface of the Earth. Water can change states among liquid, vapor, and ice at various places in the water cycle. Although the balance of water on Earth remains fairly constant over time, individual water molecules can come and go. The water moves from one reservoir to another, such as from river to ocean, or from the ocean to the atmosphere, by the physical processes of evaporation, condensation, precipitation, infiltration, surface runoff, and subsurface flow. In doing so, the water goes through different forms: liquid, solid (ice) and vapor.\"\"\"\n    \n    question = \"What causes water to move from the ocean to the atmosphere in the water cycle?\"\n    \n    # Initialize the MCQ generator\n    generator = MCQGenerator()\n    \n    # Generate MCQ\n    mcq = generator.generate_mcq(context, question)\n    \n    # Format as text\n    formatted_mcq = generator.format_mcq_as_text(mcq)\n    \n    return formatted_mcq\n\n# Run the example if the script is executed directly\nif __name__ == \"__main__\":\n    print(generate_example_mcq())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T11:13:31.035852Z","iopub.execute_input":"2025-03-06T11:13:31.036137Z","iopub.status.idle":"2025-03-06T11:14:33.448306Z","shell.execute_reply.started":"2025-03-06T11:13:31.036079Z","shell.execute_reply":"2025-03-06T11:14:33.447325Z"}},"outputs":[{"name":"stdout","text":"Loading answer generation model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bd9b3ce94534bf9a4c495cd713a0ae3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6855beb69cea4571a4b1b7daf59b39ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d5b37d6427348179b1a600fa65cd01f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/20.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1db8b18f9fa4defb9cb428319b5be01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56b5ced26de34de0af248e46763e34cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a117705582e459f9cf491c96d69a725"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f22f5696639247f384ce8d2227fb664f"}},"metadata":{}},{"name":"stdout","text":"Loading distractor generation model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"309632be541b49adb2d67017a128b702"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59529629196f4798a74c4a0067c103d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8472f8e06e1741d0ab9420a652617a05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/20.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccf47a2b2855425da9de4413cec72b54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edcd99c6e49947df9654cd82b9d7a445"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce35e75d50e04c57a8c7d1dc8629ad88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a72a17bbec44419db69078a15d974f86"}},"metadata":{}},{"name":"stdout","text":"Models loaded and running on cpu\nContext:\nThe water cycle, also known as the hydrologic cycle, describes the continuous movement of water on, above, and below the surface of the Earth. Water can change states among liquid, vapor, and ice at various places in the water cycle. Although the balance of water on Earth remains fairly constant over time, individual water molecules can come and go. The water moves from one reservoir to another, such as from river to ocean, or from the ocean to the atmosphere, by the physical processes of evaporation, condensation, precipitation, infiltration, surface runoff, and subsurface flow. In doing so, the water goes through different forms: liquid, solid (ice) and vapor.\n\nQuestion:\nWhat causes water to move from the ocean to the atmosphere in the water cycle?\n\nOptions:\nA. Alternative answer 3\nB. In doing so, the water goes through different forms: liquid, solid (ice) and vapor.\nC. Alternative answer 2\nD. evaporation, condensation, precipitation, infiltration, surface runoff, and subsurface flow.\n\nCorrect Answer: D\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nimport random\n\nclass MCQGenerator:\n    def __init__(self):\n        # Load answer generation model\n        print(\"Loading answer generation model...\")\n        self.answer_model_id = \"aayeshanakarmi/mcq-answer-generation-redstone-flant5small-2\"\n        self.answer_model = AutoModelForSeq2SeqLM.from_pretrained(self.answer_model_id)\n        self.answer_tokenizer = AutoTokenizer.from_pretrained(self.answer_model_id)\n        \n        # Load distractor generation model\n        print(\"Loading distractor generation model...\")\n        self.distractor_model_id = \"aayeshanakarmi/distractor-generation-redstone-flant5small-2\"\n        self.distractor_model = AutoModelForSeq2SeqLM.from_pretrained(self.distractor_model_id)\n        self.distractor_tokenizer = AutoTokenizer.from_pretrained(self.distractor_model_id)\n        \n        # Move models to GPU if available\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.answer_model.to(self.device)\n        self.distractor_model.to(self.device)\n        \n        # Set models to evaluation mode\n        self.answer_model.eval()\n        self.distractor_model.eval()\n        \n        print(f\"Models loaded and ready on {self.device}\")\n    \n    def generate_answer(self, context, question):\n        \"\"\"Generate the correct answer using the answer generation model\"\"\"\n        input_text = f\"Context: {context} Question: {question}\"\n        \n        # Tokenize input for answer model\n        inputs = self.answer_tokenizer(\n            input_text, \n            return_tensors=\"pt\", \n            max_length=384, \n            truncation=True\n        ).to(self.device)\n        \n        # Generate answer\n        with torch.no_grad():\n            outputs = self.answer_model.generate(\n                input_ids=inputs[\"input_ids\"],\n                attention_mask=inputs[\"attention_mask\"],\n                max_length=48,\n                num_beams=3,\n                early_stopping=True\n            )\n        \n        # Decode the answer\n        answer = self.answer_tokenizer.decode(outputs[0], skip_special_tokens=True)\n        return answer.strip()\n    \n    def generate_distractors(self, context, question, answer):\n        \"\"\"Generate distractors using the distractor generation model\"\"\"\n        input_text = f\"Context: {context} Question: {question} Correct Answer: {answer}\"\n        \n        # Tokenize input for distractor model\n        inputs = self.distractor_tokenizer(\n            input_text, \n            return_tensors=\"pt\", \n            max_length=384, \n            truncation=True\n        ).to(self.device)\n        \n        # Generate distractors\n        with torch.no_grad():\n            outputs = self.distractor_model.generate(\n                input_ids=inputs[\"input_ids\"],\n                attention_mask=inputs[\"attention_mask\"],\n                max_length=96,\n                num_beams=3,\n                early_stopping=True\n            )\n        \n        # Decode the distractors\n        distractors_text = self.distractor_tokenizer.decode(outputs[0], skip_special_tokens=True)\n        \n        # Split distractors by the separator used during training\n        distractors = [d.strip() for d in distractors_text.split(\"|\")]\n        \n        # Filter out empty distractors and remove duplicates\n        distractors = [d for d in distractors if d and d.lower() != answer.lower()]\n        \n        # Return up to 3 distractors (to make 4 total options with the correct answer)\n        return distractors[:3]\n    \n    def format_mcq(self, question, answer, distractors):\n        \"\"\"Format the MCQ with options A, B, C, D\"\"\"\n        # Combine answer and distractors\n        all_options = [answer] + distractors\n        \n        # Shuffle options to randomize the correct answer position\n        random.shuffle(all_options)\n        \n        # Find the position of the correct answer\n        correct_option_idx = all_options.index(answer)\n        correct_option_letter = chr(65 + correct_option_idx)  # A, B, C, or D\n        \n        # Format the question and options\n        formatted_question = question + \"\\n\"\n        for i, option in enumerate(all_options):\n            option_letter = chr(65 + i)  # A, B, C, or D\n            formatted_question += f\"{option_letter}. {option}\\n\"\n        \n        return {\n            \"question\": question,\n            \"options\": all_options,\n            \"formatted_mcq\": formatted_question,\n            \"correct_answer\": answer,\n            \"correct_option\": correct_option_letter\n        }\n    \n    def generate_mcq(self, context, question):\n        \"\"\"Generate a complete MCQ from context and question\"\"\"\n        # Generate the correct answer\n        answer = self.generate_answer(context, question)\n        print(f\"Generated answer: {answer}\")\n        \n        # Generate distractors\n        distractors = self.generate_distractors(context, question, answer)\n        print(f\"Generated distractors: {distractors}\")\n        \n        # Format as MCQ\n        mcq = self.format_mcq(question, answer, distractors)\n        \n        return mcq\n\n\n# Example usage\nif __name__ == \"__main__\":\n    # Create MCQ generator\n    generator = MCQGenerator()\n    \n    # Example context and question\n    context = \"\"\"Neural networks are a set of algorithms, modeled loosely after the human brain, that are designed to recognize patterns. They interpret sensory data through a kind of machine perception, labeling or clustering raw input. The patterns they recognize are numerical, contained in vectors, into which all real-world data, be it images, sound, text or time series, must be translated. Neural networks help us cluster and classify data. You can think of them as a clustering and classification layer on top of the data you store and manage. They help to group unlabeled data according to similarities among the example inputs, and they classify data when they have a labeled dataset to train on.\"\"\"\n    \n    question = \"What is the primary function of neural networks?\"\n    \n    # Generate MCQ\n    mcq = generator.generate_mcq(context, question)\n    \n    # Print the results\n    print(\"\\n--- Generated MCQ ---\")\n    print(mcq[\"formatted_mcq\"])\n    print(f\"Correct answer: {mcq['correct_option']} - {mcq['correct_answer']}\")\n    \n    # Another example\n    print(\"\\n--- Another Example ---\")\n    context2 = \"\"\"The water cycle, also known as the hydrologic cycle, describes the continuous movement of water on, above, and below the surface of the Earth. Water can change states among liquid, vapor, and ice at various points in the cycle. Although the balance of water on Earth remains fairly constant over time, individual water molecules can move around the globe. The water cycle involves the following processes: evaporation, transpiration, condensation, precipitation, and collection.\"\"\"\n    \n    question2 = \"What happens during the condensation phase of the water cycle?\"\n    \n    mcq2 = generator.generate_mcq(context2, question2)\n    \n    print(mcq2[\"formatted_mcq\"])\n    print(f\"Correct answer: {mcq2['correct_option']} - {mcq2['correct_answer']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T11:20:15.661362Z","iopub.execute_input":"2025-03-06T11:20:15.661756Z","iopub.status.idle":"2025-03-06T11:20:23.471932Z","shell.execute_reply.started":"2025-03-06T11:20:15.661727Z","shell.execute_reply":"2025-03-06T11:20:23.470813Z"}},"outputs":[{"name":"stdout","text":"Loading answer generation model...\nLoading distractor generation model...\nModels loaded and ready on cpu\nGenerated answer: Neural networks help us cluster and classify data. You can think of them as a clustering and classification layer on top of the data you store and manage.\nGenerated distractors: ['Neural networks help us cluster and classify data according to similarities among the example inputs, and they classify data when they have a labeled dataset to train on.']\n\n--- Generated MCQ ---\nWhat is the primary function of neural networks?\nA. Neural networks help us cluster and classify data according to similarities among the example inputs, and they classify data when they have a labeled dataset to train on.\nB. Neural networks help us cluster and classify data. You can think of them as a clustering and classification layer on top of the data you store and manage.\n\nCorrect answer: B - Neural networks help us cluster and classify data. You can think of them as a clustering and classification layer on top of the data you store and manage.\n\n--- Another Example ---\nGenerated answer: evaporation, transpiration, condensation, precipitation, and collection.\nGenerated distractors: ['Although the balance of water on Earth remains fairly constant over time, individual water molecules can move around the globe. The water cycle involves the following processes: evaporation, transpiration, condensation, precipitation, and collection.']\nWhat happens during the condensation phase of the water cycle?\nA. evaporation, transpiration, condensation, precipitation, and collection.\nB. Although the balance of water on Earth remains fairly constant over time, individual water molecules can move around the globe. The water cycle involves the following processes: evaporation, transpiration, condensation, precipitation, and collection.\n\nCorrect answer: A - evaporation, transpiration, condensation, precipitation, and collection.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from transformers import T5Tokenizer, T5ForConditionalGeneration\nimport torch\nimport os\n# from dotenv import load_dotenv\n\n# # Load environment variables from .env file (if needed)\n# load_dotenv()\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nHF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n\n\nclass SimpleMCQGenerator:\n    def __init__(self):\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model_name = \"aayeshanakarmi/Flan-T5-Small-Test1MCQ-Quizard-5\"\n        self.auth_token = HF_TOKEN\n        \n        print(f\"Using device: {self.device}\")\n        \n        # Initialize tokenizer and model\n        self.tokenizer = T5Tokenizer.from_pretrained(self.model_name, use_auth_token=self.auth_token)\n        self.model = T5ForConditionalGeneration.from_pretrained(self.model_name, use_auth_token=self.auth_token).to(self.device)\n\n    def generate_mcq(self, question, context, max_length=128):\n        \"\"\"\n        Generate a multiple-choice question based on the provided question and context.\n        \n        Args:\n            question (str): The question to generate MCQs for\n            context (str): The context/passage related to the question\n            max_length (int): Maximum length of the generated output\n            \n        Returns:\n            dict: Dictionary containing the correct answer and distractors\n        \"\"\"\n        input_text = f\"Generate a multiple-choice question (MCQ) based on the given question and context. Ensure the output includes one correct answer and three unique incorrect answers (distractors). Question: {question} Context: {context}\"\n        \n        inputs = self.tokenizer(\n            input_text, \n            return_tensors=\"pt\", \n            max_length=512, \n            truncation=True\n        ).to(self.device)\n        \n        outputs = self.model.generate(\n            inputs[\"input_ids\"], \n            max_length=max_length, \n            num_beams=5, \n            early_stopping=True\n        )\n        \n        raw_output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n        \n        try:\n            parts = raw_output.split('|')\n            correct_answer = parts[0].replace('Correct Answer:', '').strip()\n            distractors = [opt.strip() for opt in parts[1].replace('Incorrect Answers:', '').split(',')]\n            \n            # Ensure we have exactly 3 unique distractors\n            unique_distractors = list(set(distractors))[:3]\n            while len(unique_distractors) < 3:\n                unique_distractors.append(f\"Option {len(unique_distractors) + 1}\")\n            \n            return {\n                'question': question,\n                'context': context,\n                'correct_answer': correct_answer,\n                'distractors': unique_distractors,\n                'raw_output': raw_output  # Include raw output for debugging\n            }\n        except Exception as e:\n            print(f\"Error parsing MCQ output: {e}\")\n            return {\n                'question': question,\n                'context': context,\n                'correct_answer': raw_output,\n                'distractors': [f\"Option {i+1}\" for i in range(3)],\n                'raw_output': raw_output,\n                'error': str(e)\n            }\n\n# Example usage in a Jupyter notebook cell:\n\n# Create the generator\nmcq_gen = SimpleMCQGenerator()\n\n# Example context and question\ncontext = \"The mitochondria is the powerhouse of the cell. It produces energy in the form of ATP through cellular respiration.\"\nquestion = \"What is the function of mitochondria in a cell?\"\n\n# Generate MCQ\nmcq_result = mcq_gen.generate_mcq(question, context)\n\n# Display the result\nprint(f\"Question: {question}\")\nprint(f\"Correct Answer: {mcq_result['correct_answer']}\")\nprint(\"Distractors:\")\nfor i, distractor in enumerate(mcq_result['distractors']):\n    print(f\"  {i+1}. {distractor}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T11:35:47.538055Z","iopub.execute_input":"2025-03-06T11:35:47.538470Z","iopub.status.idle":"2025-03-06T11:36:03.166437Z","shell.execute_reply.started":"2025-03-06T11:35:47.538441Z","shell.execute_reply":"2025-03-06T11:36:03.165300Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1895: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/20.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b699331206a40c0aaeb7429e8c56887"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13e0a19013a64b67a0480a5f81f07941"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/2.59k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77075f35aca842b189b6bb234eff0a05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d78863328324426296dedebb5c1caa4a"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:3491: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5a4c6ad3084427886c845875a15c747"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"361a51ad547b4a57aa0d51909bdca793"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab7f29c498e64aecb0f9ab576237f12d"}},"metadata":{}},{"name":"stdout","text":"Question: What is the function of mitochondria in a cell?\nCorrect Answer: Producing energy through cellular respiration\nDistractors:\n  1. To store energy\n  2. Option 2\n  3. Option 3\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"potsawee/t5-large-generation-race-Distractor\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"potsawee/t5-large-generation-race-Distractor\")\n\ncontext = r\"\"\"\nWorld number one Novak Djokovic says he is hoping for a \"positive decision\" to allow him \nto play at Indian Wells and the Miami Open next month. The United States has extended \nits requirement for international visitors to be vaccinated against Covid-19. Proof of vaccination \nwill be required to enter the country until at least 10 April, but the Serbian has previously \nsaid he is unvaccinated. The 35-year-old has applied for special permission to enter the country. \nIndian Wells and the Miami Open - two of the most prestigious tournaments on the tennis calendar \noutside the Grand Slams - start on 6 and 20 March respectively. Djokovic says he will return to \nthe ATP tour in Dubai next week after claiming a record-extending 10th Australian Open title \nand a record-equalling 22nd Grand Slam men's title last month.\"\"\".replace(\"\\n\", \"\")\nquestion = \"What is the best title for the passage?\"\nanswer = \"Djokovic's application for special permission to enter the United States\"\n\ninput_text = \" \".join([question, tokenizer.sep_token, answer, tokenizer.sep_token, context])\ninputs = tokenizer(input_text, return_tensors=\"pt\")\noutputs = model.generate(**inputs, max_new_tokens=128)\ndistractors = tokenizer.decode(outputs[0], skip_special_tokens=False)\ndistractors = distractors.replace(tokenizer.pad_token, \"\").replace(tokenizer.eos_token, \"\")\ndistractors = [y.strip() for y in distractors.split(tokenizer.sep_token)]\nprint(distractors)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T12:55:57.225918Z","iopub.execute_input":"2025-03-06T12:55:57.226321Z","iopub.status.idle":"2025-03-06T12:59:10.208650Z","shell.execute_reply.started":"2025-03-06T12:55:57.226280Z","shell.execute_reply":"2025-03-06T12:59:10.206613Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.35k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8a7055f82de428aab1d6d6eb196fdc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5903f11cf9ca490ba622349104270ee4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e01776996b0a4800932291b12fd4af4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"610072124a8c4d77841cdd18e778467b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.23k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc767745a3304aebb754f33f20687b9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.48k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8134c22568c4745a57498e1d8836b7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f17e974c2cc4f9380ce8f7ed880fd0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/142 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d38e9293ed247c5824a7c72e5613c0c"}},"metadata":{}},{"name":"stdout","text":"['The United States has extended its requirement for international visitors to be vaccinated against Covid-19', \"Djokovic's return to the ATP tour in Dubai\", \"Djokovic's hope for a positive decision to allow him to play at Indian Wells and the Miami Open\"]\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\n# Load tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(\"potsawee/t5-large-generation-race-Distractor\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"potsawee/t5-large-generation-race-Distractor\")\n\n# Define multiple context, question, and answer tuples\ndata_samples = [\n    (\n        \"\"\"World number one Novak Djokovic says he is hoping for a \"positive decision\" to allow him \n        to play at Indian Wells and the Miami Open next month. The United States has extended \n        its requirement for international visitors to be vaccinated against Covid-19. Proof of vaccination \n        will be required to enter the country until at least 10 April, but the Serbian has previously \n        said he is unvaccinated. The 35-year-old has applied for special permission to enter the country. \n        Indian Wells and the Miami Open - two of the most prestigious tournaments on the tennis calendar \n        outside the Grand Slams - start on 6 and 20 March respectively. Djokovic says he will return to \n        the ATP tour in Dubai next week after claiming a record-extending 10th Australian Open title \n        and a record-equalling 22nd Grand Slam men's title last month.\"\"\".replace(\"\\n\", \"\"),\n        \"What is the best title for the passage?\",\n        \"Djokovic's application for special permission to enter the United States\"\n    ),\n    (\n        \"\"\"NASA has announced plans to launch a new space telescope that will search for habitable \n        exoplanets. The telescope, named the Habitable Worlds Observatory, will focus on detecting \n        Earth-like planets in the habitable zones of their stars. Scientists hope this mission will \n        provide valuable insights into the possibility of life beyond our solar system. The telescope \n        is expected to launch in the early 2040s and will build upon the findings of previous missions \n        like the James Webb Space Telescope.\"\"\".replace(\"\\n\", \"\"),\n        \"What is NASA's new telescope called?\",\n        \"Habitable Worlds Observatory\"\n    )\n]\n\n# Process each data sample\nfor context, question, answer in data_samples:\n    input_text = \" \".join([question, tokenizer.sep_token, answer, tokenizer.sep_token, context])\n    inputs = tokenizer(input_text, return_tensors=\"pt\")\n    outputs = model.generate(**inputs, max_new_tokens=128)\n    distractors = tokenizer.decode(outputs[0], skip_special_tokens=False)\n    \n    # Clean up distractors\n    distractors = distractors.replace(tokenizer.pad_token, \"\").replace(tokenizer.eos_token, \"\")\n    distractors = [y.strip() for y in distractors.split(tokenizer.sep_token)]\n\n    # Print results\n    print(f\"Question: {question}\")\n    print(f\"Answer: {answer}\")\n    print(f\"Distractors: {distractors}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T13:01:18.237051Z","iopub.execute_input":"2025-03-06T13:01:18.237514Z","iopub.status.idle":"2025-03-06T13:01:30.974323Z","shell.execute_reply.started":"2025-03-06T13:01:18.237466Z","shell.execute_reply":"2025-03-06T13:01:30.973159Z"}},"outputs":[{"name":"stdout","text":"Question: What is the best title for the passage?\nAnswer: Djokovic's application for special permission to enter the United States\nDistractors: ['The United States has extended its requirement for international visitors to be vaccinated against Covid-19', \"Djokovic's return to the ATP tour in Dubai\", \"Djokovic's hope for a positive decision to allow him to play at Indian Wells and the Miami Open\"]\n\nQuestion: What is NASA's new telescope called?\nAnswer: Habitable Worlds Observatory\nDistractors: ['Habitable Planets Observatory', 'Habitable Planets', 'Habitable Planets']\n\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport sys\nimport random\nimport time\nfrom datetime import datetime\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, get_linear_schedule_with_warmup\nfrom datasets import load_dataset\nfrom tqdm.notebook import tqdm\n\n# === Configuration ===\n# Model parameters\nmodel_name = \"google/flan-t5-base\"\nsave_dir = \"./model_weights/\"\nrun_name = f\"flan-t5-base-race-distractor-generation\"\n\n# Training parameters\nlearning_rate = 3e-5\nbatch_size = 4\nnum_workers = 2\nnum_epochs = 5\nmax_length = 512\nvalid_steps = 2000\nwarmup_steps = 500\nweight_decay = 0.01\ngradient_accumulation_steps = 4  # For effective batch size of 16\n\n# Create save directory if it doesn't exist\nos.makedirs(save_dir, exist_ok=True)\n\n# Set random seeds for reproducibility\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\n\n# Check for GPU availability\ntorch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {torch_device}\")\n\n# Load tokenizer and add special tokens\ntokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=max_length)\nif tokenizer.sep_token is None:\n    tokenizer.add_special_tokens({\"sep_token\": \"<sep>\"})\n    print(f\"Added special token: {tokenizer.sep_token}\")\nelse:\n    print(f\"Using existing sep_token: {tokenizer.sep_token}\")\n\n\n# === Dataset Class ===\nclass RaceDistractorGeneration(Dataset):\n    def __init__(self, tokenizer, data_split, shuffle_distractors=False):\n        \"\"\"\n        Dataset class for distractor generation using RACE dataset\n        - input: question <sep> answer <sep> article\n        - output: distractor1 <sep> distractor2 <sep> distractor3\n        \"\"\"\n        # Load RACE dataset\n        self.data = load_dataset(\"race\", \"all\", split=data_split)\n        self.tokenizer = tokenizer\n        self.shuffle_distractors = shuffle_distractors\n        \n        # Create mapping for answer options\n        self.label_mapping = {label: i for i, label in enumerate([\"A\", \"B\", \"C\", \"D\"])}\n        self.all_labels = [0, 1, 2, 3]\n        \n        print(f\"RACE Distractor Generation Dataset Initialized - {data_split} split: {len(self.data)} examples\")\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        try:\n            example = self.data[idx]\n            \n            # Extract data from example\n            question = example[\"question\"]\n            context = example[\"article\"]\n            options = example[\"options\"]\n            \n            # Find correct answer\n            label_example = example[\"answer\"]\n            answer_i = self.label_mapping[label_example]\n            answer = options[answer_i]\n            \n            # Get distractors (incorrect options)\n            distractor_ids = [i for i in self.all_labels if i != answer_i]\n            if self.shuffle_distractors:\n                random.shuffle(distractor_ids)\n            distractors = [options[i] for i in distractor_ids]\n            \n            # Format input and output\n            input_text = f\"{question} {tokenizer.sep_token} {answer} {tokenizer.sep_token} {context}\"\n            output_text = f\"{distractors[0]} {tokenizer.sep_token} {distractors[1]} {tokenizer.sep_token} {distractors[2]}\"\n            \n            return {'input': input_text, 'output': output_text}\n        except Exception as e:\n            print(f\"Error processing example {idx}: {e}\")\n            # Return empty strings as fallback\n            return {'input': '', 'output': ''}\n\n\n# === Collate Function ===\ndef collate_fn(batch):\n    \"\"\"\n    Collate function for DataLoader\n    \"\"\"\n    # Filter out None values\n    batch = [item for item in batch if item['input'] != '' and item['output'] != '']\n    \n    if len(batch) == 0:\n        return None\n    \n    # Extract input and output sequences\n    input_sequences = [item['input'] for item in batch]\n    output_sequences = [item['output'] for item in batch]\n    \n    # Tokenize inputs\n    input_encoding = tokenizer(\n        input_sequences,\n        padding=\"longest\",\n        max_length=max_length,\n        truncation=True,\n        return_tensors=\"pt\",\n    )\n    \n    # Tokenize outputs\n    target_encoding = tokenizer(\n        output_sequences,\n        padding=\"longest\",\n        max_length=max_length,\n        truncation=True,\n    )\n    \n    # Convert to tensor and replace padding token ids with -100 (ignored by loss)\n    labels = torch.tensor(target_encoding.input_ids)\n    labels[labels == tokenizer.pad_token_id] = -100\n    \n    return {\n        'input_ids': input_encoding.input_ids,\n        'attention_mask': input_encoding.attention_mask,\n        'labels': labels,\n    }\n\n\n# === Training Function ===\ndef train():\n    # Create datasets\n    train_data = RaceDistractorGeneration(\n        tokenizer=tokenizer,\n        data_split=\"train\",\n        shuffle_distractors=True,\n    )\n    \n    valid_data = RaceDistractorGeneration(\n        tokenizer=tokenizer,\n        data_split=\"validation\",\n        shuffle_distractors=False,\n    )\n    \n    # Create data loaders\n    train_loader = DataLoader(\n        train_data,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        shuffle=True,\n        collate_fn=collate_fn,\n    )\n    \n    valid_loader = DataLoader(\n        valid_data,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        shuffle=False,\n        collate_fn=collate_fn,\n    )\n    \n    # Load model\n    print(f\"Loading {model_name}...\")\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n    \n    # Resize token embeddings if we added new tokens\n    model.resize_token_embeddings(len(tokenizer))\n    \n    # Move model to device\n    if torch_device == \"cuda\":\n        model.to(torch_device)\n    \n    # Print model parameters\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"Total parameters: {total_params:,}\")\n    print(f\"Trainable parameters: {trainable_params:,}\")\n    \n    # Setup optimizer\n    optimizer = optim.AdamW(\n        model.parameters(),\n        lr=learning_rate,\n        weight_decay=weight_decay,\n    )\n    \n    # Calculate total training steps\n    total_steps = len(train_loader) * num_epochs // gradient_accumulation_steps\n    \n    # Setup learning rate scheduler\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=warmup_steps,\n        num_training_steps=total_steps\n    )\n    \n    # Training loop\n    print(\"Starting training...\")\n    model.train()\n    best_val_loss = float('inf')\n    early_stop_counter = 0\n    training_step = 0\n    \n    start_time = time.time()\n    \n    for epoch in range(num_epochs):\n        epoch_loss = 0\n        steps_in_epoch = 0\n        \n        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n        progress_bar = tqdm(train_loader, desc=f\"Training\")\n        \n        for batch in progress_bar:\n            if batch is None:\n                continue\n            \n            # Move batch to device\n            input_ids = batch['input_ids'].to(torch_device)\n            attention_mask = batch['attention_mask'].to(torch_device)\n            labels = batch['labels'].to(torch_device)\n            \n            # Forward pass\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=labels\n            )\n            \n            # Calculate loss\n            loss = outputs.loss / gradient_accumulation_steps\n            epoch_loss += loss.item() * gradient_accumulation_steps\n            steps_in_epoch += 1\n            \n            # Backward pass\n            loss.backward()\n            \n            # Update weights every gradient_accumulation_steps\n            if (training_step + 1) % gradient_accumulation_steps == 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                optimizer.step()\n                scheduler.step()\n                optimizer.zero_grad()\n            \n            # Update progress bar\n            progress_bar.set_postfix({\n                'loss': f\"{loss.item() * gradient_accumulation_steps:.4f}\",\n                'step': training_step\n            })\n            \n            training_step += 1\n            \n            # Validation\n            if training_step % valid_steps == 0:\n                # Save checkpoint\n                checkpoint_path = f\"{save_dir}/{run_name}-step{training_step}.pt\"\n                torch.save({\n                    'epoch': epoch,\n                    'step': training_step,\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'scheduler_state_dict': scheduler.state_dict(),\n                }, checkpoint_path)\n                print(f\"\\nSaved checkpoint to {checkpoint_path}\")\n                \n                # Validate\n                model.eval()\n                val_loss = validate(model, valid_loader)\n                print(f\"Validation Loss: {val_loss:.4f}\")\n                model.train()\n                \n                # Early stopping\n                if val_loss < best_val_loss:\n                    early_stop_counter = 0\n                    best_val_loss = val_loss\n                    best_model_path = f\"{save_dir}/{run_name}-best.pt\"\n                    torch.save({\n                        'epoch': epoch,\n                        'step': training_step,\n                        'model_state_dict': model.state_dict(),\n                        'optimizer_state_dict': optimizer.state_dict(),\n                        'scheduler_state_dict': scheduler.state_dict(),\n                        'validation_loss': val_loss,\n                    }, best_model_path)\n                    print(f\"New best model saved at {best_model_path}\")\n                else:\n                    early_stop_counter += 1\n                    print(f\"Validation loss did not improve. Early stopping counter: {early_stop_counter}/3\")\n                    if early_stop_counter >= 3:\n                        print(\"Early stopping triggered.\")\n                        return\n        \n        # Epoch completed\n        avg_epoch_loss = epoch_loss / steps_in_epoch\n        print(f\"Epoch {epoch + 1} completed. Average loss: {avg_epoch_loss:.4f}\")\n    \n    # Training completed\n    final_model_path = f\"{save_dir}/{run_name}-final.pt\"\n    torch.save({\n        'epoch': num_epochs,\n        'step': training_step,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'scheduler_state_dict': scheduler.state_dict(),\n    }, final_model_path)\n    print(f\"Training completed. Final model saved at {final_model_path}\")\n    \n    total_time = time.time() - start_time\n    print(f\"Total training time: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n\n\n# === Validation Function ===\ndef validate(model, dataloader):\n    \"\"\"\n    Validate the model on the validation dataset\n    \"\"\"\n    model.eval()\n    total_loss = 0\n    num_batches = 0\n    \n    with torch.no_grad():\n        progress_bar = tqdm(dataloader, desc=\"Validating\")\n        for batch in progress_bar:\n            if batch is None:\n                continue\n            \n            # Move batch to device\n            input_ids = batch['input_ids'].to(torch_device)\n            attention_mask = batch['attention_mask'].to(torch_device)\n            labels = batch['labels'].to(torch_device)\n            \n            # Forward pass\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=labels\n            )\n            \n            # Calculate loss\n            loss = outputs.loss.item()\n            total_loss += loss\n            num_batches += 1\n            \n            # Update progress bar\n            progress_bar.set_postfix({'loss': f\"{loss:.4f}\"})\n    \n    # Calculate average loss\n    avg_loss = total_loss / max(num_batches, 1)\n    return avg_loss\n\n\n# === Generation Function ===\ndef generate_distractors(model, text, num_return_sequences=1):\n    \"\"\"\n    Generate distractors for a given question, answer, and context\n    \"\"\"\n    model.eval()\n    \n    # Tokenize input\n    inputs = tokenizer(\n        text,\n        return_tensors=\"pt\",\n        max_length=max_length,\n        truncation=True\n    )\n    \n    # Move to device\n    inputs = {k: v.to(torch_device) for k, v in inputs.items()}\n    \n    # Generate\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_length=max_length,\n            num_return_sequences=num_return_sequences,\n            num_beams=4,\n            temperature=1.0,\n            top_k=50,\n            top_p=0.95,\n            do_sample=True,\n            early_stopping=True\n        )\n    \n    # Decode outputs\n    generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n    \n    return generated_texts\n\n\n# === Main Function ===\ndef main():\n    \"\"\"\n    Main function to run the training process\n    \"\"\"\n    print(\"Starting finetuning process...\")\n    print(f\"Model: {model_name}\")\n    print(f\"Device: {torch_device}\")\n    print(f\"Learning rate: {learning_rate}\")\n    print(f\"Batch size: {batch_size} (effective: {batch_size * gradient_accumulation_steps})\")\n    print(f\"Gradient accumulation steps: {gradient_accumulation_steps}\")\n    print(f\"Number of epochs: {num_epochs}\")\n    print(f\"Maximum sequence length: {max_length}\")\n    print(f\"Validation steps: {valid_steps}\")\n    print(f\"Warmup steps: {warmup_steps}\")\n    \n    # Start training\n    train()\n    \n    print(\"Training complete!\")\n\n\n# === Testing Function ===\ndef test_model(model_path):\n    \"\"\"\n    Test the trained model on custom examples\n    \"\"\"\n    # Load model\n    model = AutoModelForSeq2SeqLM","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T03:54:39.286864Z","iopub.execute_input":"2025-03-07T03:54:39.287162Z","iopub.status.idle":"2025-03-07T03:54:39.569954Z","shell.execute_reply.started":"2025-03-07T03:54:39.287137Z","shell.execute_reply":"2025-03-07T03:54:39.569046Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nAdded special token: <sep>\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Kaggle Notebook Setup\n# FLAN-T5-Base Finetuning on RACE Dataset for Distractor Generation\n\n# Check if GPU is available\n!nvidia-smi\n\n# Install required packages (if not already installed)\n!pip install -q transformers datasets tqdm\n\n# Set up wandb for experiment tracking (optional)\n# !pip install wandb\n# import wandb\n# wandb.login()\n\n# Import necessary libraries\nimport os\nimport sys\nimport random\nimport time\nfrom datetime import datetime\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, get_linear_schedule_with_warmup\nfrom datasets import load_dataset\nfrom tqdm.notebook import tqdm\n\n# Set up the output directory\nos.makedirs(\"./model_weights\", exist_ok=True)\n\n# Set random seed for reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\n# Now copy and paste the code from the previous artifacts here\n\n# After pasting all the code, run this to execute it\nif __name__ == \"__main__\":\n    print(\"=\"*50)\n    print(\"FLAN-T5-Base Finetuning on RACE Dataset\")\n    print(\"=\"*50)\n    \n    print(\"\\n=== FLAN-T5 Distractor Generation Tools ===\")\n    print(\"1. Train the model\")\n    print(\"2. Evaluate on test set\")\n    print(\"3. Interactive distractor generation\")\n    print(\"4. Exit\")\n    \n\n    main()\n    evaluate_on_test_set()\n    interactive_distractor_generation()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T03:54:43.380646Z","iopub.execute_input":"2025-03-07T03:54:43.380952Z","iopub.status.idle":"2025-03-07T06:30:17.418123Z","shell.execute_reply.started":"2025-03-07T03:54:43.380927Z","shell.execute_reply":"2025-03-07T06:30:17.416655Z"}},"outputs":[{"name":"stdout","text":"Fri Mar  7 03:54:43 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   76C    P0             35W /   70W |    1183MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   40C    P8              9W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n+-----------------------------------------------------------------------------------------+\n==================================================\nFLAN-T5-Base Finetuning on RACE Dataset\n==================================================\n\n=== FLAN-T5 Distractor Generation Tools ===\n1. Train the model\n2. Evaluate on test set\n3. Interactive distractor generation\n4. Exit\nStarting finetuning process...\nModel: google/flan-t5-base\nDevice: cuda\nLearning rate: 3e-05\nBatch size: 4 (effective: 16)\nGradient accumulation steps: 4\nNumber of epochs: 5\nMaximum sequence length: 512\nValidation steps: 2000\nWarmup steps: 500\nRACE Distractor Generation Dataset Initialized - train split: 87866 examples\nRACE Distractor Generation Dataset Initialized - validation split: 4887 examples\nLoading google/flan-t5-base...\nTotal parameters: 247,536,384\nTrainable parameters: 247,536,384\nStarting training...\n\nEpoch 1/5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/21967 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"424a2b7b01f8403f8121ac4067551ae0"}},"metadata":{}},{"name":"stdout","text":"\nSaved checkpoint to ./model_weights//flan-t5-base-race-distractor-generation-step2000.pt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/1222 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34601272a6c541be8d677e393a8cf9e0"}},"metadata":{}},{"name":"stdout","text":"Validation Loss: 1.9140\nNew best model saved at ./model_weights//flan-t5-base-race-distractor-generation-best.pt\n\nSaved checkpoint to ./model_weights//flan-t5-base-race-distractor-generation-step4000.pt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/1222 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed87ee335f5f47c4910ce2f6a838c8c5"}},"metadata":{}},{"name":"stdout","text":"Validation Loss: 1.8089\nNew best model saved at ./model_weights//flan-t5-base-race-distractor-generation-best.pt\n\nSaved checkpoint to ./model_weights//flan-t5-base-race-distractor-generation-step6000.pt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/1222 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b67526daab0477293152ad847a54cf8"}},"metadata":{}},{"name":"stdout","text":"Validation Loss: 1.7825\nNew best model saved at ./model_weights//flan-t5-base-race-distractor-generation-best.pt\n\nSaved checkpoint to ./model_weights//flan-t5-base-race-distractor-generation-step8000.pt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/1222 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20c8698c75c14d9992645b369833b51b"}},"metadata":{}},{"name":"stdout","text":"Validation Loss: 1.7585\nNew best model saved at ./model_weights//flan-t5-base-race-distractor-generation-best.pt\n\nSaved checkpoint to ./model_weights//flan-t5-base-race-distractor-generation-step10000.pt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/1222 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec1c111ce03a4692b782101823eac9d4"}},"metadata":{}},{"name":"stdout","text":"Validation Loss: 1.7456\nNew best model saved at ./model_weights//flan-t5-base-race-distractor-generation-best.pt\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    849\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 850\u001b[0;31m             _save(\n\u001b[0m\u001b[1;32m    851\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1113\u001b[0m             \u001b[0;31m# Now that it is on the CPU we can directly copy it into the zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:778] . PytorchStreamWriter failed writing file data/281: file write failed","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-3a364f8e33f6>\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mevaluate_on_test_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0minteractive_distractor_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-62f6e6152328>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training complete!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-62f6e6152328>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    275\u001b[0m                 \u001b[0;31m# Save checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0mcheckpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{save_dir}/{run_name}-step{training_step}.pt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 torch.save({\n\u001b[0m\u001b[1;32m    278\u001b[0m                     \u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                     \u001b[0;34m'step'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m             _save(\n\u001b[1;32m    851\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_end_of_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_stream\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:603] . unexpected pos 1060654976 vs 1060654864"],"ename":"RuntimeError","evalue":"[enforce fail at inline_container.cc:603] . unexpected pos 1060654976 vs 1060654864","output_type":"error"}],"execution_count":7},{"cell_type":"markdown","source":"# Optimized training","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, get_linear_schedule_with_warmup\nfrom tqdm.auto import tqdm\nimport gc\nimport logging\nfrom datetime import datetime\n\n# Setup logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S'\n)\nlogger = logging.getLogger(__name__)\n\n# Configuration\nclass Config:\n    # Model settings\n    model_name = \"google/flan-t5-base\"\n    max_length = 512\n    \n    # Training settings\n    batch_size = 4\n    gradient_accumulation_steps = 4  # Effectively gives batch size of 16\n    learning_rate = 2e-5\n    weight_decay = 0.01\n    num_epochs = 3\n    warmup_ratio = 0.1\n    \n    # Mixed precision\n    use_mixed_precision = True\n    \n    # Checkpointing settings\n    save_steps = 500\n    eval_steps = 500\n    \n    # Output directories\n    model_dir = \"./model_weights\"\n    run_name = f\"flan-t5-base-race-distractor-{datetime.now().strftime('%Y%m%d_%H%M')}\"\n    \n    # System settings\n    seed = 42\n    num_workers = 2\n    \nconfig = Config()\n\n# Create model directory if it doesn't exist\nos.makedirs(config.model_dir, exist_ok=True)\n\n# Set seeds for reproducibility\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nset_seed(config.seed)\n\n# Dataset class\nclass RaceDistractorGeneration(Dataset):\n    def __init__(self, tokenizer, data_split, shuffle_distractors=False):\n        \"\"\"\n        Task: Generate distractors based on the question, correct answer, and article\n        \n        Args:\n            tokenizer: Tokenizer for encoding inputs and outputs\n            data_split: 'train', 'validation', or 'test'\n            shuffle_distractors: Whether to shuffle the order of distractors during training\n        \"\"\"\n        logger.info(f\"Loading RACE dataset for {data_split} split\")\n        data = load_dataset(\"race\", \"all\", split=data_split)\n        self.data = data\n        self.tokenizer = tokenizer\n        self.separator = \" <sep> \"\n        self.label_mapping = {label: i for i, label in enumerate([\"A\", \"B\", \"C\", \"D\"])}\n        self.all_labels = [0, 1, 2, 3]\n        self.shuffle_distractors = shuffle_distractors\n        logger.info(f\"RaceDistractorGeneration loaded with {len(self.data)} examples\")\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        try:\n            example = self.data[idx]\n            question = example[\"question\"]\n            context = example[\"article\"]\n            options = example[\"options\"]\n            label_example = example[\"answer\"]\n            answer_i = self.label_mapping[label_example]\n            answer = options[answer_i]\n            \n            # Get distractor indices (all options except the correct answer)\n            distractor_ids = [i for i in self.all_labels if i != answer_i]\n            if self.shuffle_distractors:\n                random.shuffle(distractor_ids)\n            \n            # Get the actual distractors\n            distractors = [options[i] for i in distractor_ids]\n            \n            # Prepare input and output\n            input_text = f\"Generate distractors: Question: {question}{self.separator}Answer: {answer}{self.separator}Context: {context}\"\n            output_text = f\"{distractors[0]}{self.separator}{distractors[1]}{self.separator}{distractors[2]}\"\n            \n            return {\n                \"input\": input_text,\n                \"output\": output_text\n            }\n        except Exception as e:\n            logger.warning(f\"Error processing item {idx}: {e}\")\n            # Return a simple example as fallback\n            return {\n                \"input\": \"Error processing item\",\n                \"output\": \"Error\"\n            }\n\n# Collate function for data loader\ndef collate_fn(batch):\n    input_texts = [item[\"input\"] for item in batch]\n    output_texts = [item[\"output\"] for item in batch]\n    \n    # Tokenize inputs\n    input_encodings = tokenizer(\n        input_texts,\n        padding=\"longest\",\n        max_length=config.max_length,\n        truncation=True,\n        return_tensors=\"pt\",\n    )\n    \n    # Tokenize outputs\n    output_encodings = tokenizer(\n        output_texts,\n        padding=\"longest\",\n        max_length=config.max_length,\n        truncation=True,\n        return_tensors=\"pt\",\n    )\n    \n    # Replace padding token id's with -100 so they're ignored in loss calculation\n    labels = output_encodings.input_ids.clone()\n    labels[labels == tokenizer.pad_token_id] = -100\n    \n    return {\n        \"input_ids\": input_encodings.input_ids,\n        \"attention_mask\": input_encodings.attention_mask,\n        \"labels\": labels,\n    }\n\n# Smart model checkpoint saver to handle disk space issues\nclass CheckpointSaver:\n    def __init__(self, save_dir, run_name, max_checkpoints=3):\n        self.save_dir = save_dir\n        self.run_name = run_name\n        self.max_checkpoints = max_checkpoints\n        self.checkpoint_paths = []\n        \n    def save_checkpoint(self, model, optimizer, scheduler, scaler, epoch, step, loss):\n        # Create checkpoint\n        checkpoint = {\n            'epoch': epoch,\n            'step': step,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n            'scaler_state_dict': scaler.state_dict() if scaler else None,\n            'loss': loss,\n        }\n        \n        # Create filename\n        checkpoint_path = f\"{self.save_dir}/{self.run_name}-step{step}.pt\"\n        \n        # Try to save checkpoint\n        try:\n            # Use a temporary file first\n            temp_path = f\"{checkpoint_path}.tmp\"\n            torch.save(checkpoint, temp_path)\n            \n            # Rename to final path if successful\n            if os.path.exists(temp_path):\n                os.rename(temp_path, checkpoint_path)\n                logger.info(f\"Checkpoint saved: {checkpoint_path}\")\n                \n                # Add to list of checkpoints\n                self.checkpoint_paths.append(checkpoint_path)\n                \n                # Remove old checkpoints if exceeding max_checkpoints\n                if len(self.checkpoint_paths) > self.max_checkpoints:\n                    old_checkpoint = self.checkpoint_paths.pop(0)\n                    if os.path.exists(old_checkpoint):\n                        os.remove(old_checkpoint)\n                        logger.info(f\"Removed old checkpoint: {old_checkpoint}\")\n                        \n                return True\n            else:\n                logger.error(f\"Failed to create temporary checkpoint file\")\n                return False\n                \n        except Exception as e:\n            logger.error(f\"Error saving checkpoint: {e}\")\n            \n            # Try saving a smaller checkpoint as fallback\n            try:\n                smaller_checkpoint = {\n                    'epoch': epoch,\n                    'step': step,\n                    'model_state_dict': model.state_dict(),\n                }\n                fallback_path = f\"{self.save_dir}/{self.run_name}-step{step}-fallback.pt\"\n                torch.save(smaller_checkpoint, fallback_path)\n                logger.info(f\"Fallback checkpoint saved: {fallback_path}\")\n                return True\n            except Exception as e2:\n                logger.error(f\"Failed to save fallback checkpoint: {e2}\")\n                return False\n\n# Function to free up memory\ndef free_memory():\n    gc.collect()\n    torch.cuda.empty_cache()\n\n# Initialize tokenizer\ntokenizer = AutoTokenizer.from_pretrained(config.model_name)\n# Add separator token if not in vocabulary\nif \"<sep>\" not in tokenizer.get_vocab():\n    tokenizer.add_special_tokens({\"sep_token\": \"<sep>\"})\n\n# Training function\ndef train():\n    # Check if CUDA is available\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    logger.info(f\"Using device: {device}\")\n    \n    # Load datasets\n    train_dataset = RaceDistractorGeneration(\n        tokenizer=tokenizer,\n        data_split=\"train\",\n        shuffle_distractors=True\n    )\n    \n    valid_dataset = RaceDistractorGeneration(\n        tokenizer=tokenizer,\n        data_split=\"validation\",\n        shuffle_distractors=False\n    )\n    \n    # Create data loaders\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=config.batch_size,\n        shuffle=True,\n        num_workers=config.num_workers,\n        collate_fn=collate_fn,\n        pin_memory=True,\n        drop_last=True\n    )\n    \n    valid_loader = DataLoader(\n        valid_dataset,\n        batch_size=config.batch_size,\n        shuffle=False,\n        num_workers=config.num_workers,\n        collate_fn=collate_fn,\n        pin_memory=True\n    )\n    \n    # Load model\n    logger.info(f\"Loading model: {config.model_name}\")\n    model = AutoModelForSeq2SeqLM.from_pretrained(config.model_name)\n    \n    # Resize embeddings if needed due to added tokens\n    model.resize_token_embeddings(len(tokenizer))\n    \n    # Move model to device\n    model.to(device)\n    \n    # Initialize optimizer\n    optimizer = optim.AdamW(\n        model.parameters(),\n        lr=config.learning_rate,\n        weight_decay=config.weight_decay\n    )\n    \n    # Calculate total training steps\n    total_steps = len(train_loader) * config.num_epochs // config.gradient_accumulation_steps\n    warmup_steps = int(total_steps * config.warmup_ratio)\n    \n    # Initialize scheduler\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=warmup_steps,\n        num_training_steps=total_steps\n    )\n    \n    # Initialize gradient scaler for mixed precision\n    scaler = GradScaler() if config.use_mixed_precision else None\n    \n    # Initialize checkpoint saver\n    checkpoint_saver = CheckpointSaver(config.model_dir, config.run_name)\n    \n    # Training loop\n    logger.info(\"Starting training\")\n    best_valid_loss = float(\"inf\")\n    no_improvement_count = 0\n    training_step = 0\n    \n    for epoch in range(config.num_epochs):\n        model.train()\n        epoch_loss = 0\n        \n        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config.num_epochs}\")\n        optimizer.zero_grad()\n        \n        for step, batch in enumerate(progress_bar):\n            # Move batch to device\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"labels\"].to(device)\n            \n            # Forward pass with mixed precision if enabled\n            if config.use_mixed_precision:\n                with autocast():\n                    outputs = model(\n                        input_ids=input_ids,\n                        attention_mask=attention_mask,\n                        labels=labels\n                    )\n                    loss = outputs.loss / config.gradient_accumulation_steps\n                \n                # Backward pass with gradient scaling\n                scaler.scale(loss).backward()\n                \n                if (step + 1) % config.gradient_accumulation_steps == 0:\n                    scaler.unscale_(optimizer)\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                    scaler.step(optimizer)\n                    scaler.update()\n                    scheduler.step()\n                    optimizer.zero_grad()\n                    \n                    # Update training step\n                    training_step += 1\n            else:\n                # Standard training without mixed precision\n                outputs = model(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    labels=labels\n                )\n                loss = outputs.loss / config.gradient_accumulation_steps\n                loss.backward()\n                \n                if (step + 1) % config.gradient_accumulation_steps == 0:\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                    optimizer.step()\n                    scheduler.step()\n                    optimizer.zero_grad()\n                    \n                    # Update training step\n                    training_step += 1\n            \n            # Update progress bar\n            epoch_loss += loss.item() * config.gradient_accumulation_steps\n            progress_bar.set_postfix({\"loss\": f\"{loss.item() * config.gradient_accumulation_steps:.4f}\"})\n            \n            # Evaluate and save checkpoint at specified intervals\n            if training_step > 0 and training_step % config.eval_steps == 0:\n                valid_loss = evaluate(model, valid_loader, device)\n                logger.info(f\"Step {training_step} - Validation Loss: {valid_loss:.4f}\")\n                \n                model.train()\n                \n                # Save checkpoint\n                if training_step % config.save_steps == 0:\n                    success = checkpoint_saver.save_checkpoint(\n                        model, optimizer, scheduler, scaler, epoch, training_step, valid_loss\n                    )\n                    if not success:\n                        logger.warning(\"Failed to save checkpoint, continuing training\")\n                \n                # Early stopping check\n                if valid_loss < best_valid_loss:\n                    best_valid_loss = valid_loss\n                    no_improvement_count = 0\n                    \n                    # Save best model\n                    logger.info(f\"New best validation loss: {best_valid_loss:.4f}\")\n                    try:\n                        best_model_path = f\"{config.model_dir}/{config.run_name}-best.pt\"\n                        torch.save(model.state_dict(), best_model_path)\n                        logger.info(f\"Best model saved: {best_model_path}\")\n                    except Exception as e:\n                        logger.error(f\"Failed to save best model: {e}\")\n                else:\n                    no_improvement_count += 1\n                    logger.info(f\"No improvement for {no_improvement_count} evaluations\")\n                    \n                    if no_improvement_count >= 3:\n                        logger.info(\"Early stopping triggered\")\n                        return\n        \n        # End of epoch\n        avg_epoch_loss = epoch_loss / len(train_loader)\n        logger.info(f\"Epoch {epoch+1}/{config.num_epochs} - Average Loss: {avg_epoch_loss:.4f}\")\n        \n        # Free memory at the end of each epoch\n        free_memory()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T03:17:56.976328Z","iopub.execute_input":"2025-03-07T03:17:56.976606Z","iopub.status.idle":"2025-03-07T03:18:06.692763Z","shell.execute_reply.started":"2025-03-07T03:17:56.976586Z","shell.execute_reply":"2025-03-07T03:18:06.692077Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fceb750fa1f4022be2712f97969a2d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2c9c94a7a4b41e4b8c78a3b922ca128"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36e6607243904604844cddce24cac960"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9ab709ac77b4f3b8186998acc1aa0b7"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# Evaluation function\ndef evaluate(model, dataloader, device):\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"labels\"].to(device)\n            \n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=labels\n            )\n            \n            total_loss += outputs.loss.item()\n    \n    return total_loss / len(dataloader)\n\n# Main function to run the entire training process\ndef main():\n    logger.info(\"Initializing training process\")\n    \n    # Set seed for reproducibility\n    set_seed(config.seed)\n    \n    try:\n        # Start training\n        logger.info(f\"Starting training with run name: {config.run_name}\")\n        train()\n        \n        # Save final model\n        try:\n            final_model_path = f\"{config.model_dir}/{config.run_name}-final.pt\"\n            model = AutoModelForSeq2SeqLM.from_pretrained(config.model_name)\n            model.resize_token_embeddings(len(tokenizer))\n            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n            model.to(device)\n            \n            # Try to load best model if it exists\n            best_model_path = f\"{config.model_dir}/{config.run_name}-best.pt\"\n            if os.path.exists(best_model_path):\n                logger.info(f\"Loading best model from {best_model_path}\")\n                model.load_state_dict(torch.load(best_model_path, map_location=device))\n            \n            # Save the model\n            model.save_pretrained(final_model_path)\n            tokenizer.save_pretrained(final_model_path)\n            logger.info(f\"Final model saved to {final_model_path}\")\n        except Exception as e:\n            logger.error(f\"Error saving final model: {e}\")\n    \n    except KeyboardInterrupt:\n        logger.info(\"Training interrupted by user\")\n    except Exception as e:\n        logger.error(f\"Training failed with error: {e}\")\n    finally:\n        # Clean up resources\n        free_memory()\n        logger.info(\"Training process completed\")\n\n# Run main function if script is executed directly\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T03:18:06.693734Z","iopub.execute_input":"2025-03-07T03:18:06.694191Z","iopub.status.idle":"2025-03-07T03:46:38.848735Z","shell.execute_reply.started":"2025-03-07T03:18:06.694158Z","shell.execute_reply":"2025-03-07T03:46:38.847993Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/11.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d55ff55b8af4bc4bdcf985f2898e3ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/2.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4b61f326dc043908ea6c93dac05f05b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/37.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d6dac02113849e3a419847e90a010f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/2.05M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49f4dd1a05f64d40a81394fb134eb732"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4934 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee71892c881747898ae1438cbd76ce40"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/87866 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42dcaf239f40427087ae2d370fdd76b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/4887 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c287555a5b44de095e60bd0ed0fc51f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c038dee65793439bae6bb19b9c3502d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee1561ba0dec4e9bb67a53c2105d3c02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae81afdbf7204b5d901957724289214e"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-2-9cf23aa02ca0>:305: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler() if config.use_mixed_precision else None\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/3:   0%|          | 0/21966 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c610e80eb7544ab18931d6472250ed95"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-2-9cf23aa02ca0>:331: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n<ipython-input-3-bc6e1a356a9d>:45: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(best_model_path, map_location=device))\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# TRIAL 101","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nimport random\nimport time\nimport shutil\nfrom datetime import datetime\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, get_linear_schedule_with_warmup\nfrom datasets import load_dataset\nfrom tqdm.notebook import tqdm\n\n# === Configuration ===\n# Model parameters\nmodel_name = \"google/flan-t5-base\"\nsave_dir = \"./model_weights/\"\nrun_name = f\"flan-t5-base-race-distractor-generation\"\n\n# Training parameters\nlearning_rate = 3e-5\nbatch_size = 4\nnum_workers = 2\nnum_epochs = 5\nmax_length = 512\nvalid_steps = 2000\nwarmup_steps = 500\nweight_decay = 0.01\ngradient_accumulation_steps = 4  # For effective batch size of 16\n\n# Checkpoint handling\nsave_full_checkpoint = False  # Set to True only for final model\ncheckpoint_every_epoch = False  # Set to False to only save at validation steps\n\n# Create save directory if it doesn't exist\nos.makedirs(save_dir, exist_ok=True)\n\n# Set random seeds for reproducibility\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\n\n# Check for GPU availability\ntorch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Using device: {torch_device}\")\n\n# Load tokenizer and add special tokens\ntokenizer = AutoTokenizer.from_pretrained(model_name, model_max_length=max_length)\nif tokenizer.sep_token is None:\n    tokenizer.add_special_tokens({\"sep_token\": \"<sep>\"})\n    print(f\"Added special token: {tokenizer.sep_token}\")\nelse:\n    print(f\"Using existing sep_token: {tokenizer.sep_token}\")\n\n\n# === Dataset Class ===\nclass RaceDistractorGeneration(Dataset):\n    def __init__(self, tokenizer, data_split, shuffle_distractors=False):\n        \"\"\"\n        Dataset class for distractor generation using RACE dataset\n        - input: question <sep> answer <sep> article\n        - output: distractor1 <sep> distractor2 <sep> distractor3\n        \"\"\"\n        # Load RACE dataset\n        self.data = load_dataset(\"race\", \"all\", split=data_split)\n        self.tokenizer = tokenizer\n        self.shuffle_distractors = shuffle_distractors\n        \n        # Create mapping for answer options\n        self.label_mapping = {label: i for i, label in enumerate([\"A\", \"B\", \"C\", \"D\"])}\n        self.all_labels = [0, 1, 2, 3]\n        \n        print(f\"RACE Distractor Generation Dataset Initialized - {data_split} split: {len(self.data)} examples\")\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        try:\n            example = self.data[idx]\n            \n            # Extract data from example\n            question = example[\"question\"]\n            context = example[\"article\"]\n            options = example[\"options\"]\n            \n            # Find correct answer\n            label_example = example[\"answer\"]\n            answer_i = self.label_mapping[label_example]\n            answer = options[answer_i]\n            \n            # Get distractors (incorrect options)\n            distractor_ids = [i for i in self.all_labels if i != answer_i]\n            if self.shuffle_distractors:\n                random.shuffle(distractor_ids)\n            distractors = [options[i] for i in distractor_ids]\n            \n            # Format input and output\n            input_text = f\"{question} {tokenizer.sep_token} {answer} {tokenizer.sep_token} {context}\"\n            output_text = f\"{distractors[0]} {tokenizer.sep_token} {distractors[1]} {tokenizer.sep_token} {distractors[2]}\"\n            \n            return {'input': input_text, 'output': output_text}\n        except Exception as e:\n            print(f\"Error processing example {idx}: {e}\")\n            # Return empty strings as fallback\n            return {'input': '', 'output': ''}\n\n\n# === Collate Function ===\ndef collate_fn(batch):\n    \"\"\"\n    Collate function for DataLoader\n    \"\"\"\n    # Filter out None values\n    batch = [item for item in batch if item['input'] != '' and item['output'] != '']\n    \n    if len(batch) == 0:\n        return None\n    \n    # Extract input and output sequences\n    input_sequences = [item['input'] for item in batch]\n    output_sequences = [item['output'] for item in batch]\n    \n    # Tokenize inputs\n    input_encoding = tokenizer(\n        input_sequences,\n        padding=\"longest\",\n        max_length=max_length,\n        truncation=True,\n        return_tensors=\"pt\",\n    )\n    \n    # Tokenize outputs\n    target_encoding = tokenizer(\n        output_sequences,\n        padding=\"longest\",\n        max_length=max_length,\n        truncation=True,\n    )\n    \n    # Convert to tensor and replace padding token ids with -100 (ignored by loss)\n    labels = torch.tensor(target_encoding.input_ids)\n    labels[labels == tokenizer.pad_token_id] = -100\n    \n    return {\n        'input_ids': input_encoding.input_ids,\n        'attention_mask': input_encoding.attention_mask,\n        'labels': labels,\n    }\n\n\n# === Training Function ===\ndef train():\n    # Check disk space\n    total, used, free = shutil.disk_usage(\"/\")\n    print(f\"Disk space: Total={total // (2**30)}GB, Used={used // (2**30)}GB, Free={free // (2**30)}GB\")\n    if free < 5 * (2**30):  # Less than 5GB free\n        print(\"WARNING: Low disk space. Consider cleaning up or using smaller checkpoints.\")\n    \n    # Create datasets\n    train_data = RaceDistractorGeneration(\n        tokenizer=tokenizer,\n        data_split=\"train\",\n        shuffle_distractors=True,\n    )\n    \n    valid_data = RaceDistractorGeneration(\n        tokenizer=tokenizer,\n        data_split=\"validation\",\n        shuffle_distractors=False,\n    )\n    \n    # Create data loaders\n    train_loader = DataLoader(\n        train_data,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        shuffle=True,\n        collate_fn=collate_fn,\n    )\n    \n    valid_loader = DataLoader(\n        valid_data,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        shuffle=False,\n        collate_fn=collate_fn,\n    )\n    \n    # Load model\n    print(f\"Loading {model_name}...\")\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n    \n    # Resize token embeddings if we added new tokens\n    model.resize_token_embeddings(len(tokenizer))\n    \n    # Move model to device\n    if torch_device == \"cuda\":\n        model.to(torch_device)\n    \n    # Print model parameters\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"Total parameters: {total_params:,}\")\n    print(f\"Trainable parameters: {trainable_params:,}\")\n    \n    # Setup optimizer\n    optimizer = optim.AdamW(\n        model.parameters(),\n        lr=learning_rate,\n        weight_decay=weight_decay,\n    )\n    \n    # Calculate total training steps\n    total_steps = len(train_loader) * num_epochs // gradient_accumulation_steps\n    \n    # Setup learning rate scheduler\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=warmup_steps,\n        num_training_steps=total_steps\n    )\n    \n    # Training loop\n    print(\"Starting training...\")\n    model.train()\n    best_val_loss = float('inf')\n    early_stop_counter = 0\n    training_step = 0\n    \n    start_time = time.time()\n    \n    for epoch in range(num_epochs):\n        epoch_loss = 0\n        steps_in_epoch = 0\n        \n        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n        progress_bar = tqdm(train_loader, desc=f\"Training\")\n        \n        for batch in progress_bar:\n            if batch is None:\n                continue\n            \n            # Move batch to device\n            input_ids = batch['input_ids'].to(torch_device)\n            attention_mask = batch['attention_mask'].to(torch_device)\n            labels = batch['labels'].to(torch_device)\n            \n            # Forward pass\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=labels\n            )\n            \n            # Calculate loss\n            loss = outputs.loss / gradient_accumulation_steps\n            epoch_loss += loss.item() * gradient_accumulation_steps\n            steps_in_epoch += 1\n            \n            # Backward pass\n            loss.backward()\n            \n            # Update weights every gradient_accumulation_steps\n            if (training_step + 1) % gradient_accumulation_steps == 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                optimizer.step()\n                scheduler.step()\n                optimizer.zero_grad()\n            \n            # Update progress bar\n            progress_bar.set_postfix({\n                'loss': f\"{loss.item() * gradient_accumulation_steps:.4f}\",\n                'step': training_step\n            })\n            \n            training_step += 1\n            \n            # Validation\n            if training_step % valid_steps == 0:\n                # Clean up previous checkpoint files (except best)\n                for f in os.listdir(save_dir):\n                    if f.startswith(run_name) and f.endswith(\".pt\") and \"best\" not in f:\n                        try:\n                            os.remove(os.path.join(save_dir, f))\n                            print(f\"Removed old checkpoint: {f}\")\n                        except:\n                            pass\n                \n                # Save checkpoint more efficiently\n                checkpoint_path = f\"{save_dir}/{run_name}-step{training_step}.pt\"\n                try:\n                    # Save only model weights instead of full state\n                    torch.save(model.state_dict(), checkpoint_path)\n                    print(f\"\\nSaved model weights to {checkpoint_path}\")\n                except RuntimeError as e:\n                    print(f\"ERROR saving checkpoint: {e}\")\n                    print(\"Continuing without saving checkpoint...\")\n                \n                # Validate\n                model.eval()\n                val_loss = validate(model, valid_loader)\n                print(f\"Validation Loss: {val_loss:.4f}\")\n                model.train()\n                \n                # Early stopping\n                if val_loss < best_val_loss:\n                    early_stop_counter = 0\n                    best_val_loss = val_loss\n                    best_model_path = f\"{save_dir}/{run_name}-best.pt\"\n                    try:\n                        # Save only model weights for best model\n                        torch.save(model.state_dict(), best_model_path)\n                        print(f\"New best model saved at {best_model_path}\")\n                    except RuntimeError as e:\n                        print(f\"ERROR saving best model: {e}\")\n                else:\n                    early_stop_counter += 1\n                    print(f\"Validation loss did not improve. Early stopping counter: {early_stop_counter}/3\")\n                    if early_stop_counter >= 3:\n                        print(\"Early stopping triggered.\")\n                        return\n        \n        # Epoch completed\n        avg_epoch_loss = epoch_loss / steps_in_epoch\n        print(f\"Epoch {epoch + 1} completed. Average loss: {avg_epoch_loss:.4f}\")\n        \n        # Save epoch checkpoint if enabled\n        if checkpoint_every_epoch:\n            epoch_model_path = f\"{save_dir}/{run_name}-epoch{epoch+1}.pt\"\n            try:\n                torch.save(model.state_dict(), epoch_model_path)\n                print(f\"Epoch checkpoint saved at {epoch_model_path}\")\n            except RuntimeError as e:\n                print(f\"ERROR saving epoch checkpoint: {e}\")\n    \n    # Training completed\n    final_model_path = f\"{save_dir}/{run_name}-final.pt\"\n    try:\n        if save_full_checkpoint:\n            torch.save({\n                'epoch': num_epochs,\n                'step': training_step,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'scheduler_state_dict': scheduler.state_dict(),\n            }, final_model_path)\n        else:\n            torch.save(model.state_dict(), final_model_path)\n        print(f\"Training completed. Final model saved at {final_model_path}\")\n    except RuntimeError as e:\n        print(f\"ERROR saving final model: {e}\")\n    \n    total_time = time.time() - start_time\n    print(f\"Total training time: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n\n\n# === Validation Function ===\ndef validate(model, dataloader):\n    \"\"\"\n    Validate the model on the validation dataset\n    \"\"\"\n    model.eval()\n    total_loss = 0\n    num_batches = 0\n    \n    with torch.no_grad():\n        progress_bar = tqdm(dataloader, desc=\"Validating\")\n        for batch in progress_bar:\n            if batch is None:\n                continue\n            \n            # Move batch to device\n            input_ids = batch['input_ids'].to(torch_device)\n            attention_mask = batch['attention_mask'].to(torch_device)\n            labels = batch['labels'].to(torch_device)\n            \n            # Forward pass\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=labels\n            )\n            \n            # Calculate loss\n            loss = outputs.loss.item()\n            total_loss += loss\n            num_batches += 1\n            \n            # Update progress bar\n            progress_bar.set_postfix({'loss': f\"{loss:.4f}\"})\n    \n    # Calculate average loss\n    avg_loss = total_loss / max(num_batches, 1)\n    return avg_loss\n\n\n# === Generation Function ===\ndef generate_distractors(model, text, num_return_sequences=1):\n    \"\"\"\n    Generate distractors for a given question, answer, and context\n    \"\"\"\n    model.eval()\n    \n    # Tokenize input\n    inputs = tokenizer(\n        text,\n        return_tensors=\"pt\",\n        max_length=max_length,\n        truncation=True\n    )\n    \n    # Move to device\n    inputs = {k: v.to(torch_device) for k, v in inputs.items()}\n    \n    # Generate\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_length=max_length,\n            num_return_sequences=num_return_sequences,\n            num_beams=4,\n            temperature=1.0,\n            top_k=50,\n            top_p=0.95,\n            do_sample=True,\n            early_stopping=True\n        )\n    \n    # Decode outputs\n    generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n    \n    return generated_texts\n\n\n# === Evaluation Function ===\ndef evaluate_on_test_set():\n    print(\"Evaluating on test set...\")\n    # Load best model\n    best_model_path = f\"{save_dir}/{run_name}-best.pt\"\n    if not os.path.exists(best_model_path):\n        print(f\"Best model not found at {best_model_path}\")\n        return\n    \n    try:\n        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n        model.load_state_dict(torch.load(best_model_path))\n        model.to(torch_device)\n    except Exception as e:\n        print(f\"Error loading model: {e}\")\n        return\n    \n    # Create test dataset\n    test_data = RaceDistractorGeneration(\n        tokenizer=tokenizer,\n        data_split=\"test\",\n        shuffle_distractors=False,\n    )\n    \n    test_loader = DataLoader(\n        test_data,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        shuffle=False,\n        collate_fn=collate_fn,\n    )\n    \n    # Evaluate\n    test_loss = validate(model, test_loader)\n    print(f\"Test Loss: {test_loss:.4f}\")\n    \n    # Generate examples\n    print(\"\\nExample generations:\")\n    for i in range(min(3, len(test_data))):\n        example = test_data[i]\n        input_text = example['input']\n        reference = example['output']\n        \n        generated = generate_distractors(model, input_text)\n        \n        print(f\"\\nInput: {input_text}\")\n        print(f\"Reference: {reference}\")\n        print(f\"Generated: {generated[0]}\")\n\n\n# === Interactive Generation Function ===\ndef interactive_distractor_generation():\n    print(\"\\nInteractive distractor generation mode\")\n    print(\"Loading best model...\")\n    \n    best_model_path = f\"{save_dir}/{run_name}-best.pt\"\n    if not os.path.exists(best_model_path):\n        print(f\"Best model not found at {best_model_path}\")\n        return\n    \n    try:\n        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n        model.load_state_dict(torch.load(best_model_path))\n        model.to(torch_device)\n    except Exception as e:\n        print(f\"Error loading model: {e}\")\n        return\n    \n    print(\"\\nEnter 'q' to quit\")\n    while True:\n        question = input(\"\\nEnter question: \")\n        if question.lower() == 'q':\n            break\n            \n        answer = input(\"Enter correct answer: \")\n        if answer.lower() == 'q':\n            break\n            \n        context = input(\"Enter context: \")\n        if context.lower() == 'q':\n            break\n        \n        input_text = f\"{question} {tokenizer.sep_token} {answer} {tokenizer.sep_token} {context}\"\n        generated = generate_distractors(model, input_text, num_return_sequences=3)\n        \n        print(\"\\nGenerated distractors:\")\n        for i, distractors in enumerate(generated):\n            print(f\"Option {i+1}: {distractors}\")\n\n\n# === Main Function ===\ndef main():\n    \"\"\"\n    Main function to run the training process\n    \"\"\"\n    print(\"Starting finetuning process...\")\n    print(f\"Model: {model_name}\")\n    print(f\"Device: {torch_device}\")\n    print(f\"Learning rate: {learning_rate}\")\n    print(f\"Batch size: {batch_size} (effective: {batch_size * gradient_accumulation_steps})\")\n    print(f\"Gradient accumulation steps: {gradient_accumulation_steps}\")\n    print(f\"Number of epochs: {num_epochs}\")\n    print(f\"Maximum sequence length: {max_length}\")\n    print(f\"Validation steps: {valid_steps}\")\n    print(f\"Warmup steps: {warmup_steps}\")\n    \n    # Start training\n    train()\n    \n    print(\"Training complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T06:37:25.801517Z","iopub.execute_input":"2025-03-07T06:37:25.801887Z","iopub.status.idle":"2025-03-07T06:37:26.018985Z","shell.execute_reply.started":"2025-03-07T06:37:25.801859Z","shell.execute_reply":"2025-03-07T06:37:26.018128Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nAdded special token: <sep>\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    try:\n        print(\"=\"*50)\n        print(\"FLAN-T5-Base Finetuning on RACE Dataset\")\n        print(\"=\"*50)\n        \n        print(\"\\n=== FLAN-T5 Distractor Generation Tools ===\")\n        main()\n        evaluate_on_test_set()\n        interactive_distractor_generation()\n\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        import traceback\n        traceback.print_exc()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-07T06:37:36.975569Z","iopub.execute_input":"2025-03-07T06:37:36.976015Z","execution_failed":"2025-03-07T15:17:11.785Z"}},"outputs":[{"name":"stdout","text":"==================================================\nFLAN-T5-Base Finetuning on RACE Dataset\n==================================================\n\n=== FLAN-T5 Distractor Generation Tools ===\nStarting finetuning process...\nModel: google/flan-t5-base\nDevice: cuda\nLearning rate: 3e-05\nBatch size: 4 (effective: 16)\nGradient accumulation steps: 4\nNumber of epochs: 5\nMaximum sequence length: 512\nValidation steps: 2000\nWarmup steps: 500\nDisk space: Total=8062GB, Used=6171GB, Free=1891GB\nRACE Distractor Generation Dataset Initialized - train split: 87866 examples\nRACE Distractor Generation Dataset Initialized - validation split: 4887 examples\nLoading google/flan-t5-base...\nTotal parameters: 247,536,384\nTrainable parameters: 247,536,384\nStarting training...\n\nEpoch 1/5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/21967 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48f44986152740f7829aeb8f748886bc"}},"metadata":{}},{"name":"stdout","text":"Removed old checkpoint: flan-t5-base-race-distractor-generation-step2000.pt\nRemoved old checkpoint: flan-t5-base-race-distractor-generation-step6000.pt\nRemoved old checkpoint: flan-t5-base-race-distractor-generation-step12000.pt\nRemoved old checkpoint: flan-t5-base-race-distractor-generation-step10000.pt\nRemoved old checkpoint: flan-t5-base-race-distractor-generation-step8000.pt\nRemoved old checkpoint: flan-t5-base-race-distractor-generation-step4000.pt\n\nSaved model weights to ./model_weights//flan-t5-base-race-distractor-generation-step2000.pt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/1222 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f4278a7ecce4cbd97dffcbdc21d6622"}},"metadata":{}},{"name":"stdout","text":"Validation Loss: 1.9140\nNew best model saved at ./model_weights//flan-t5-base-race-distractor-generation-best.pt\nRemoved old checkpoint: flan-t5-base-race-distractor-generation-step2000.pt\n\nSaved model weights to ./model_weights//flan-t5-base-race-distractor-generation-step4000.pt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/1222 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a0cdc7a5c0f4c308d0749d82a87fd4c"}},"metadata":{}},{"name":"stdout","text":"Validation Loss: 1.8089\nNew best model saved at ./model_weights//flan-t5-base-race-distractor-generation-best.pt\nRemoved old checkpoint: flan-t5-base-race-distractor-generation-step4000.pt\n\nSaved model weights to ./model_weights//flan-t5-base-race-distractor-generation-step6000.pt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/1222 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea5fd088f27d4f888e83231b3bf0e779"}},"metadata":{}},{"name":"stdout","text":"Validation Loss: 1.7825\nNew best model saved at ./model_weights//flan-t5-base-race-distractor-generation-best.pt\nRemoved old checkpoint: flan-t5-base-race-distractor-generation-step6000.pt\n\nSaved model weights to ./model_weights//flan-t5-base-race-distractor-generation-step8000.pt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/1222 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4582dbcb42cf41489270d1ed33ecbe5e"}},"metadata":{}},{"name":"stdout","text":"Validation Loss: 1.7585\nNew best model saved at ./model_weights//flan-t5-base-race-distractor-generation-best.pt\nRemoved old checkpoint: flan-t5-base-race-distractor-generation-step8000.pt\n\nSaved model weights to ./model_weights//flan-t5-base-race-distractor-generation-step10000.pt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/1222 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"530db94f2d874d048ef5dfc8347bda26"}},"metadata":{}},{"name":"stdout","text":"Validation Loss: 1.7456\nNew best model saved at ./model_weights//flan-t5-base-race-distractor-generation-best.pt\nRemoved old checkpoint: flan-t5-base-race-distractor-generation-step10000.pt\n\nSaved model weights to ./model_weights//flan-t5-base-race-distractor-generation-step12000.pt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/1222 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d009fb5960ad4709858a3b1ab3795992"}},"metadata":{}},{"name":"stdout","text":"Validation Loss: 1.7299\nNew best model saved at ./model_weights//flan-t5-base-race-distractor-generation-best.pt\nRemoved old checkpoint: flan-t5-base-race-distractor-generation-step12000.pt\n\nSaved model weights to ./model_weights//flan-t5-base-race-distractor-generation-step14000.pt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/1222 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d22ec1a7b034392850d89596f32a13b"}},"metadata":{}},{"name":"stdout","text":"Validation Loss: 1.7256\nNew best model saved at ./model_weights//flan-t5-base-race-distractor-generation-best.pt\nRemoved old checkpoint: flan-t5-base-race-distractor-generation-step14000.pt\n\nSaved model weights to ./model_weights//flan-t5-base-race-distractor-generation-step16000.pt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/1222 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71e2566fec0b4f4b94fb0b649b03c2ea"}},"metadata":{}},{"name":"stdout","text":"Validation Loss: 1.7177\nNew best model saved at ./model_weights//flan-t5-base-race-distractor-generation-best.pt\nRemoved old checkpoint: flan-t5-base-race-distractor-generation-step16000.pt\n\nSaved model weights to ./model_weights//flan-t5-base-race-distractor-generation-step18000.pt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/1222 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b601c67b7b774b2e9059e600f52ed4d9"}},"metadata":{}},{"name":"stdout","text":"Validation Loss: 1.7139\nNew best model saved at ./model_weights//flan-t5-base-race-distractor-generation-best.pt\nRemoved old checkpoint: flan-t5-base-race-distractor-generation-step18000.pt\n\nSaved model weights to ./model_weights//flan-t5-base-race-distractor-generation-step20000.pt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/1222 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"221de5d6dfc84c989c4f24d36fb8c416"}},"metadata":{}},{"name":"stdout","text":"Validation Loss: 1.7050\nNew best model saved at ./model_weights//flan-t5-base-race-distractor-generation-best.pt\nEpoch 1 completed. Average loss: 2.0825\n\nEpoch 2/5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/21967 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0da69b9019ce4b21828ba07653b73e0e"}},"metadata":{}},{"name":"stdout","text":"Removed old checkpoint: flan-t5-base-race-distractor-generation-step20000.pt\n\nSaved model weights to ./model_weights//flan-t5-base-race-distractor-generation-step22000.pt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/1222 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"695d330c9aaf458595e1a982cbd23d72"}},"metadata":{}},{"name":"stdout","text":"Validation Loss: 1.7015\nNew best model saved at ./model_weights//flan-t5-base-race-distractor-generation-best.pt\nRemoved old checkpoint: flan-t5-base-race-distractor-generation-step22000.pt\n\nSaved model weights to ./model_weights//flan-t5-base-race-distractor-generation-step24000.pt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/1222 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"224b61ac21284438a90759cfd5e2500b"}},"metadata":{}},{"name":"stdout","text":"Validation Loss: 1.6989\nNew best model saved at ./model_weights//flan-t5-base-race-distractor-generation-best.pt\nRemoved old checkpoint: flan-t5-base-race-distractor-generation-step24000.pt\n\nSaved model weights to ./model_weights//flan-t5-base-race-distractor-generation-step26000.pt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/1222 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8c59eeb670c4c0e9f5d22e7f353e3f4"}},"metadata":{}},{"name":"stdout","text":"Validation Loss: 1.6975\nNew best model saved at ./model_weights//flan-t5-base-race-distractor-generation-best.pt\nRemoved old checkpoint: flan-t5-base-race-distractor-generation-step26000.pt\n\nSaved model weights to ./model_weights//flan-t5-base-race-distractor-generation-step28000.pt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/1222 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c2c02ff8e6640808b53dc3eea24ea38"}},"metadata":{}},{"name":"stdout","text":"Validation Loss: 1.6939\nNew best model saved at ./model_weights//flan-t5-base-race-distractor-generation-best.pt\nRemoved old checkpoint: flan-t5-base-race-distractor-generation-step28000.pt\n\nSaved model weights to ./model_weights//flan-t5-base-race-distractor-generation-step30000.pt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/1222 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e315aee66b044f8874a7d963e56b984"}},"metadata":{}},{"name":"stdout","text":"Validation Loss: 1.6920\nNew best model saved at ./model_weights//flan-t5-base-race-distractor-generation-best.pt\nRemoved old checkpoint: flan-t5-base-race-distractor-generation-step30000.pt\n\nSaved model weights to ./model_weights//flan-t5-base-race-distractor-generation-step32000.pt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/1222 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ff5c15553dc4de5a9da1e717c4bb1eb"}},"metadata":{}},{"name":"stdout","text":"Validation Loss: 1.6884\nNew best model saved at ./model_weights//flan-t5-base-race-distractor-generation-best.pt\nRemoved old checkpoint: flan-t5-base-race-distractor-generation-step32000.pt\n\nSaved model weights to ./model_weights//flan-t5-base-race-distractor-generation-step34000.pt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/1222 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e40c5d92d95e42a2b4413791d3a8fa88"}},"metadata":{}},{"name":"stdout","text":"Validation Loss: 1.6862\nNew best model saved at ./model_weights//flan-t5-base-race-distractor-generation-best.pt\nRemoved old checkpoint: flan-t5-base-race-distractor-generation-step34000.pt\n\nSaved model weights to ./model_weights//flan-t5-base-race-distractor-generation-step36000.pt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/1222 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec39861f747c4bf4b43fdd6429ce02e2"}},"metadata":{}},{"name":"stdout","text":"Validation Loss: 1.6829\nNew best model saved at ./model_weights//flan-t5-base-race-distractor-generation-best.pt\nRemoved old checkpoint: flan-t5-base-race-distractor-generation-step36000.pt\n\nSaved model weights to ./model_weights//flan-t5-base-race-distractor-generation-step38000.pt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validating:   0%|          | 0/1222 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"616ef625eba54eaf8276f72be0f1f1f7"}},"metadata":{}},{"name":"stdout","text":"Validation Loss: 1.6804\nNew best model saved at ./model_weights//flan-t5-base-race-distractor-generation-best.pt\n","output_type":"stream"}],"execution_count":null}]}