{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset, Dataset\nfrom transformers import (\n    T5ForConditionalGeneration,\n    T5Tokenizer,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForSeq2Seq\n)\nimport numpy as np\nimport gc\nimport os\nfrom tqdm.auto import tqdm\n\n# # Create output directories if they don't exist\n# os.makedirs(\"./results/answer_generation\", exist_ok=True)\n# os.makedirs(\"./logs/answer_generation\", exist_ok=True)\n\nos.makedirs(\"/kaggle/working/results/answer_generation\", exist_ok=True)\nos.makedirs(\"/kaggle/working/logs/answer_generation\", exist_ok=True)\n\n\n# Enable memory optimization for PyTorch\ntorch.cuda.empty_cache()\n\n# 1. Load dataset - load only a subset directly\nprint(\"Loading dataset...\")\ndataset = load_dataset(\"zjsd/RedStone-QA-mcq\", split=f\"train[:{int(0.02*100)}%]\")\nprint(f\"Dataset loaded with {len(dataset)} examples\")\n\n# 2. Define preprocessing function for answer generation\ndef preprocess_for_answer_generation(examples, batch_size=64):\n    \"\"\"Preprocess data for training the model to generate the correct answer letter.\"\"\"\n    inputs = []\n    labels = []\n    \n    for i in range(0, len(examples[\"text\"]), batch_size):\n        batch_texts = examples[\"text\"][i:i+batch_size]\n        batch_questions = examples[\"question\"][i:i+batch_size]\n        batch_answers = examples[\"answer\"][i:i+batch_size]\n        \n        for text, question, answer in zip(batch_texts, batch_questions, batch_answers):\n            combined = f\"generate answer: {text} question: {question}\"\n            inputs.append(combined)\n            # Extract just the letter from \"Answer:X\" format\n            labels.append(answer.replace(\"Answer:\", \"\").strip())\n    \n    return {\n        \"input\": inputs,\n        \"output\": labels\n    }\n\n# 3. Process dataset in chunks to save memory\nprint(\"Preprocessing data for answer generation...\")\nanswer_dataset = Dataset.from_dict(preprocess_for_answer_generation(dataset))\n\n# Free up memory\ndel dataset\ngc.collect()\ntorch.cuda.empty_cache()\n\n# 4. Split dataset into train and validation\nprint(\"Splitting dataset into train and validation sets...\")\nanswer_dataset = answer_dataset.train_test_split(test_size=0.1, seed=42)\n\nanswer_train_dataset = answer_dataset[\"train\"]\nanswer_val_dataset = answer_dataset[\"test\"]\n\nprint(f\"Answer generation: {len(answer_train_dataset)} training examples, {len(answer_val_dataset)} validation examples\")\n\n# Free up memory\ndel answer_dataset\ngc.collect()\n\n# 5. Load tokenizer and model\nprint(\"Loading tokenizer and model...\")\nmodel_name = \"google/flan-t5-small\"  # Using the small model for faster training\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name)\n\n# 6. Define tokenization function\ndef tokenize_function(examples, max_input_length=512, max_target_length=128):\n    model_inputs = tokenizer(\n        examples[\"input\"],\n        max_length=max_input_length,\n        padding=\"max_length\",\n        truncation=True\n    )\n    \n    # Tokenize targets\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            examples[\"output\"],\n            max_length=max_target_length,\n            padding=\"max_length\",\n            truncation=True\n        )\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\n# 7. Tokenize datasets with smaller batch size to reduce memory usage\nprint(\"Tokenizing datasets...\")\nbatch_size = 32\n\nanswer_train_tokenized = answer_train_dataset.map(\n    tokenize_function, \n    batched=True,\n    batch_size=batch_size,\n    remove_columns=[\"input\", \"output\"]\n)\n\nanswer_val_tokenized = answer_val_dataset.map(\n    tokenize_function, \n    batched=True,\n    batch_size=batch_size,\n    remove_columns=[\"input\", \"output\"]\n)\n\n# Free up memory\ndel answer_train_dataset, answer_val_dataset\ngc.collect()\ntorch.cuda.empty_cache()\n\n# 8. Define custom metrics computation function\ndef compute_answer_metrics(eval_pred):\n    predictions, labels = eval_pred\n    # Replace -100 with the pad_token_id\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    \n    # Get the predicted tokens\n    predicted_tokens = np.argmax(predictions, axis=-1)\n    \n    # Decode to get the actual letters - process in batches to save memory\n    batch_size = 32\n    num_examples = len(predicted_tokens)\n    decoded_preds = []\n    decoded_labels = []\n    \n    for i in range(0, num_examples, batch_size):\n        batch_preds = [tokenizer.decode(pred, skip_special_tokens=True) \n                      for pred in predicted_tokens[i:i+batch_size]]\n        batch_labels = [tokenizer.decode(label, skip_special_tokens=True) \n                       for label in labels[i:i+batch_size]]\n        decoded_preds.extend(batch_preds)\n        decoded_labels.extend(batch_labels)\n    \n    # Calculate accuracy\n    correct = sum(1 for pred, label in zip(decoded_preds, decoded_labels) if pred.strip() == label.strip())\n    accuracy = correct / len(decoded_labels) if len(decoded_labels) > 0 else 0\n    \n    # Print just a few examples for debugging\n    print(\"\\nAnswer Generation Examples (Prediction, Reference):\")\n    for i in range(min(3, len(decoded_preds))):\n        print(f\"  {decoded_preds[i]} | {decoded_labels[i]}\")\n    \n    # Return metrics\n    return {\n        \"accuracy\": accuracy,\n        \"exact_match_ratio\": correct / len(decoded_labels) if len(decoded_labels) > 0 else 0,\n    }\n\n# 9. Define training arguments for answer generation\nanswer_training_args = TrainingArguments(\n    output_dir=\"./results/answer_generation\",\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=2,\n    weight_decay=0.01,\n    save_total_limit=1,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    greater_is_better=True,\n    logging_dir=\"./logs/answer_generation\",\n    logging_steps=100,\n    report_to=[\"tensorboard\"],\n    # Memory optimizations\n    fp16=True if torch.cuda.is_available() else False,  # Use mixed precision if available\n    gradient_accumulation_steps=2,  # Accumulate gradients to simulate larger batch sizes\n    dataloader_num_workers=1,  # Parallelize data loading\n    dataloader_pin_memory=True,  # Speed up data transfer to GPU\n    # Ensure progress bar is shown\n    disable_tqdm=False,\n)\n\n# 10. Define data collator\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer,\n    model=model,\n    padding=\"max_length\",\n    max_length=512,\n    pad_to_multiple_of=8  # Optimize for tensor operations\n)\n\n# 11. Train answer generation model\nprint(\"Initializing answer generation trainer...\")\nanswer_trainer = Trainer(\n    model=model,\n    args=answer_training_args,\n    train_dataset=answer_train_tokenized,\n    eval_dataset=answer_val_tokenized,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_answer_metrics,\n)\n\nprint(\"Training answer generation model...\")\nanswer_trainer.train()\n\n# Get validation results\nprint(\"Evaluating answer generation model...\")\nanswer_eval_results = answer_trainer.evaluate()\nprint(\"\\n\" + \"=\"*50)\nprint(\"ANSWER GENERATION EVALUATION RESULTS\")\nprint(\"=\"*50)\nprint(f\"Accuracy: {answer_eval_results['eval_accuracy']:.4f}\")\nprint(f\"Exact Match: {answer_eval_results['eval_exact_match_ratio']:.4f}\")\nprint(\"=\"*50 + \"\\n\")\n\n# Save model\nprint(\"Saving final model...\")\nanswer_trainer.save_model(\"./results/answer_generation/final_model\")\nprint(\"Model saved successfully!\")\n\n# 12. Memory-efficient inference function for demonstration\ndef generate_answer(context, question, model, tokenizer):\n    # Set model to evaluation mode\n    model.eval()\n    \n    # Get the device\n    device = next(model.parameters()).device\n    \n    # Prepare input\n    answer_input = f\"generate answer: {context} question: {question}\"\n    answer_input_ids = tokenizer(answer_input, return_tensors=\"pt\").input_ids.to(device)\n    \n    print(\"Generating answer letter...\")\n    with torch.no_grad():  # Disable gradient calculation to save memory\n        answer_outputs = model.generate(\n            answer_input_ids, \n            max_length=10,\n            num_beams=4,\n            early_stopping=True\n        )\n    answer_letter = tokenizer.decode(answer_outputs[0], skip_special_tokens=True).strip()\n    print(f\"Generated answer letter: {answer_letter}\")\n    \n    # Clean up GPU memory\n    del answer_outputs, answer_input_ids\n    torch.cuda.empty_cache()\n    \n    return answer_letter\n\n# Optional: Demonstrate the trained model\nprint(\"\\n\" + \"=\"*50)\nprint(\"DEMONSTRATION\")\nprint(\"=\"*50)\ntry:\n    # Sample context and question\n    context = \"The Python programming language was created by Guido van Rossum and first released in 1991. It emphasizes code readability with its notable use of significant whitespace.\"\n    question = \"Who created the Python programming language?\"\n    \n    # Generate answer\n    answer_letter = generate_answer(context, question, model, tokenizer)\n    print(f\"Context: {context}\")\n    print(f\"Question: {question}\")\n    print(f\"Generated Answer: {answer_letter}\")\nexcept Exception as e:\n    print(f\"Error during demonstration: {e}\")\n\nprint(\"\\nTraining of Answer Generation model complete!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-03T08:11:24.223996Z","iopub.execute_input":"2025-03-03T08:11:24.224185Z","iopub.status.idle":"2025-03-03T08:43:55.552208Z","shell.execute_reply.started":"2025-03-03T08:11:24.224167Z","shell.execute_reply":"2025-03-03T08:43:55.550799Z"}},"outputs":[{"name":"stdout","text":"Loading dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/813 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef15b3a676334dfbacf6451fa4b832e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"02f2eecfda5642699cec7306db349dd7.parquet:   0%|          | 0.00/41.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"702ac6f2a9174b9cbab3b463f957dfb5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"23afc2c9c15646b5b42c3c1fd8191bb3.parquet:   0%|          | 0.00/41.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f1dd0d429d04d05bc0f2839b0d99f4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"3e76396d09c3438c82927f7147f79e4e.parquet:   0%|          | 0.00/43.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30776a5af05b40fa8241fa174c1305fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"4c027dcad52241f596b09eb8c6c893a9.parquet:   0%|          | 0.00/45.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9926f6181444fe39926705d4aeb23ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"69e90546dac941528cdd07d49b93b140.parquet:   0%|          | 0.00/41.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"637e25dc93c24456b22fc6c50248ffe3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"a2687c7645db40708ea8db9371a0e703.parquet:   0%|          | 0.00/42.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"024bc9a3317a4bfab383d9aee417be93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"cc8885cabdc8437b93220777f74d0c3f.parquet:   0%|          | 0.00/43.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14690b9ef19343e098c195cb807464e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"ea94289cf49746818839e882f248f13d.parquet:   0%|          | 0.00/8.93M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f26fd31e61246fb92b950fee5c272d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"f10233c4ea29412b9a07e7ca8aedd169.parquet:   0%|          | 0.00/41.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d314a5cdef9c4eb0a29c40060110beea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"f458d85ec1e747cf8b47ac222a46d8c2.parquet:   0%|          | 0.00/42.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d18bf31bb0064acf9bc05cf7b1289b90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1661603 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a87ca2cd7d84124888edc32360bb0ba"}},"metadata":{}},{"name":"stdout","text":"Dataset loaded with 33232 examples\nPreprocessing data for answer generation...\nSplitting dataset into train and validation sets...\nAnswer generation: 29908 training examples, 3324 validation examples\nLoading tokenizer and model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b78051109d2548daa24f7b916ca22df9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b042bf9590e5440ea5f54725dc659a73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bc92936274343648df4f0d2f6fda4e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e12e7b6a839e4a28a4a09edb06b0e8f9"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f75007c2dd2463c842141a4c8f393c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7ab7fccc52f4dd3b909cbbc77b2147a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96d2e8ae95b64e1098995ec91753eed7"}},"metadata":{}},{"name":"stdout","text":"Tokenizing datasets...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/29908 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c89dcebe7e3c42d1a0951bccf9a0c6e0"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3324 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebc27117011d4be98fe7dcbc9d5d1561"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-1-c709f8ddc725>:200: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  answer_trainer = Trainer(\n","output_type":"stream"},{"name":"stdout","text":"Initializing answer generation trainer...\nTraining answer generation model...\n","output_type":"stream"},{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1871' max='3738' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1871/3738 30:15 < 30:13, 1.03 it/s, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>\n    <div>\n      \n      <progress value='10' max='416' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 10/416 00:03 < 02:21, 2.87 it/s]\n    </div>\n    "},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-c709f8ddc725>\u001b[0m in \u001b[0;36m<cell line: 211>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training answer generation model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m \u001b[0manswer_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;31m# Get validation results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2164\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2165\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2166\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2615\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2616\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2618\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mDebugOption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPU_METRICS_DEBUG\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time)\u001b[0m\n\u001b[1;32m   3045\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3046\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_evaluate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3047\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3048\u001b[0m             \u001b[0mis_new_best_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_determine_best_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   2999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3000\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_scheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3001\u001b[0;31m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3002\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4050\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4051\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   4052\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4053\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4270\u001b[0m                 \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4271\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_eval_metrics\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdescription\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Prediction\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4272\u001b[0;31m                     \u001b[0mall_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4273\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4274\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, tensors)\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensors\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_nested_concat\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_nested_concat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnested_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    132\u001b[0m         ), f\"Expected `tensors` and `new_tensors` to have the same type but found {type(tensors)} and {type(new_tensors)}.\"\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch_pad_and_concatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    132\u001b[0m         ), f\"Expected `tensors` and `new_tensors` to have the same type but found {type(tensors)} and {type(new_tensors)}.\"\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch_pad_and_concatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_concat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch_pad_and_concatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         return type(tensors)(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer_pt_utils.py\u001b[0m in \u001b[0;36mtorch_pad_and_concatenate\u001b[0;34m(tensor1, tensor2, padding_index)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtensor1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtensor2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# Let's figure out the new shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 5.39 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.81 GiB is free. Process 3053 has 10.93 GiB memory in use. Of the allocated memory 6.37 GiB is allocated by PyTorch, and 4.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 5.39 GiB. GPU 0 has a total capacity of 14.74 GiB of which 3.81 GiB is free. Process 3053 has 10.93 GiB memory in use. Of the allocated memory 6.37 GiB is allocated by PyTorch, and 4.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n    DataCollatorForSeq2Seq\n)\nimport torch\nfrom torch.utils.data import Dataset\nimport random\nimport gc\nfrom tqdm.auto import tqdm\n\n# Set random seeds for reproducibility\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\ntorch.cuda.empty_cache()\n\n# Define constants\nMODEL_NAME = \"google/flan-t5-small\"\nMAX_INPUT_LENGTH = 384  # Reduced from 512\nMAX_TARGET_LENGTH = 48  # Reduced from 64\nBATCH_SIZE = 4  # Reduced from 8\nGRADIENT_ACCUMULATION_STEPS = 4  # Increases effective batch size without increasing memory\nLEARNING_RATE = 3e-4\nNUM_EPOCHS = 3\nSAMPLE_RATIO = 0.1  # Reduced from 0.15\nMIXED_PRECISION = \"fp16\"  # Use mixed precision training\n\n# Function to free memory\ndef free_memory():\n    gc.collect()\n    torch.cuda.empty_cache()\n\n# Load the dataset in streaming mode to reduce memory usage\nprint(\"Loading dataset...\")\ndataset = load_dataset(\"zjsd/RedStone-QA-mcq\", split=\"train\", streaming=True)\n\n# Convert to regular dataset for sampling\n# Buffer a smaller amount to not load everything in memory\ndataset = dataset.take(int(1.66e6 * SAMPLE_RATIO * 1.2))  # Buffer slightly more than needed\ndataset = list(dataset)\nrandom.shuffle(dataset)\ndataset = dataset[:int(1.66e6 * SAMPLE_RATIO)]\n\n# Split into train and validation\ntrain_val_split = 0.9\ntrain_size = int(len(dataset) * train_val_split)\n\n# Create train and validation datasets\ntrain_dataset = dataset[:train_size]\nval_dataset = dataset[train_size:train_size + min(2000, len(dataset) - train_size)]  # Limit validation set\n\nprint(f\"Total sampled examples: {len(dataset)}\")\nprint(f\"Training examples: {len(train_dataset)}\")\nprint(f\"Validation examples: {len(val_dataset)}\")\n\n# Free memory\ndel dataset\nfree_memory()\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n\n# Define a processing function for batch tokenization\ndef preprocess_function(examples):\n    batch_size = len(examples)\n    input_texts = []\n    target_texts = []\n    \n    for i in range(batch_size):\n        context = examples[i]['text']\n        question = examples[i]['question']\n        answer = examples[i]['answer'].replace(\"Answer:\", \"\").strip()\n        \n        # Format the input (context + question)\n        input_text = f\"Context: {context} Question: {question}\"\n        input_texts.append(input_text)\n        target_texts.append(answer)\n    \n    # Tokenize inputs\n    model_inputs = tokenizer(\n        input_texts,\n        max_length=MAX_INPUT_LENGTH,\n        padding=\"max_length\",\n        truncation=True\n    )\n    \n    # Tokenize targets\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            target_texts,\n            max_length=MAX_TARGET_LENGTH,\n            padding=\"max_length\",\n            truncation=True\n        )\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    \n    # Replace padding token id with -100 so it's ignored in the loss\n    for i in range(len(model_inputs[\"labels\"])):\n        model_inputs[\"labels\"][i] = [\n            -100 if token == tokenizer.pad_token_id else token \n            for token in model_inputs[\"labels\"][i]\n        ]\n    \n    return model_inputs\n\n# Process data in batches to save memory\nprint(\"Processing training data...\")\nbatch_size = 512\ntrain_processed = []\nfor i in tqdm(range(0, len(train_dataset), batch_size)):\n    batch = train_dataset[i:i+batch_size]\n    processed_batch = preprocess_function(batch)\n    for j in range(len(batch)):\n        train_processed.append({\n            \"input_ids\": processed_batch[\"input_ids\"][j],\n            \"attention_mask\": processed_batch[\"attention_mask\"][j],\n            \"labels\": processed_batch[\"labels\"][j]\n        })\n\nprint(\"Processing validation data...\")\nval_processed = []\nfor i in tqdm(range(0, len(val_dataset), batch_size)):\n    batch = val_dataset[i:i+batch_size]\n    processed_batch = preprocess_function(batch)\n    for j in range(len(batch)):\n        val_processed.append({\n            \"input_ids\": processed_batch[\"input_ids\"][j],\n            \"attention_mask\": processed_batch[\"attention_mask\"][j],\n            \"labels\": processed_batch[\"labels\"][j]\n        })\n\n# Free memory\ndel train_dataset, val_dataset\nfree_memory()\n\n# Create PyTorch datasets\nclass MemoryEfficientDataset(Dataset):\n    def __init__(self, examples):\n        self.examples = examples\n    \n    def __len__(self):\n        return len(self.examples)\n    \n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(self.examples[idx][\"input_ids\"]),\n            \"attention_mask\": torch.tensor(self.examples[idx][\"attention_mask\"]),\n            \"labels\": torch.tensor(self.examples[idx][\"labels\"])\n        }\n\ntrain_dataset = MemoryEfficientDataset(train_processed)\nval_dataset = MemoryEfficientDataset(val_processed)\n\n# Free memory\ndel train_processed, val_processed\nfree_memory()\n\n# Load the model with 8-bit precision to save memory\nprint(\"Loading model...\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n\n# Define training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./answer_generation_model\",\n    overwrite_output_dir=True,\n    num_train_epochs=NUM_EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=1,  # Keep only the best model\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    logging_dir=\"./logs\",\n    logging_steps=50,\n    learning_rate=LEARNING_RATE,\n    weight_decay=0.01,\n    warmup_ratio=0.05,\n    predict_with_generate=False,  # Save memory during evaluation\n    fp16=MIXED_PRECISION == \"fp16\",  # Mixed precision training\n    optim=\"adamw_torch\",\n    report_to=\"none\",  # Disable W&B reporting\n    disable_tqdm=False,  # Enable tqdm progress bar\n)\n\n# Set up the trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True),\n)\n\n# Train the model\nprint(\"Training model...\")\ntrainer.train()\n\n# Save the model\nprint(\"Saving model...\")\nmodel.save_pretrained(\"./answer_generation_model_final\")\ntokenizer.save_pretrained(\"./answer_generation_model_final\")\n\n# Test on a few examples\nprint(\"Testing on some examples...\")\nmodel.eval()\ntest_examples = [\n    val_dataset[i] for i in range(min(3, len(val_dataset)))\n]\n\nfor example in test_examples:\n    input_ids = example[\"input_ids\"].unsqueeze(0).to(model.device)\n    attention_mask = example[\"attention_mask\"].unsqueeze(0).to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            max_length=MAX_TARGET_LENGTH,\n            num_beams=2,  # Reduced for memory efficiency\n            early_stopping=True\n        )\n    \n    input_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n    predicted_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    print(f\"Input: {input_text}\")\n    print(f\"Predicted Answer: {predicted_answer}\")\n    print(\"=\" * 50)\n\nprint(\"Training complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T08:48:44.213065Z","iopub.execute_input":"2025-03-03T08:48:44.213352Z","iopub.status.idle":"2025-03-03T12:09:06.047702Z","shell.execute_reply.started":"2025-03-03T08:48:44.213329Z","shell.execute_reply":"2025-03-03T12:09:06.046918Z"}},"outputs":[{"name":"stdout","text":"Loading dataset...\nTotal sampled examples: 166000\nTraining examples: 149400\nValidation examples: 2000\nProcessing training data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/292 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b556db6a4a654d7fa5f4563701a6c0ed"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Processing validation data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"856b1d9e6f774d65a3701729646a8b44"}},"metadata":{}},{"name":"stdout","text":"Loading model...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-2-6c99bc4a7172>:195: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"},{"name":"stdout","text":"Training model...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py:657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='14004' max='14004' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [14004/14004 3:18:29, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nThere were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n","output_type":"stream"},{"name":"stdout","text":"Saving model...\nTesting on some examples...\nInput: Context: The primary distinction between the short term scheduler and the long term scheduler is __________ A. The length of their queues B. The type of processes they schedule C. The frequency of their execution D. None of the mentioned Answer:C Question: The primary distinction between the short term scheduler and the long term scheduler is __________\nPredicted Answer: C\n==================================================\nInput: Context: Consider the situation that the transaction 'P' holds shared key lock X. Also, other transaction 'Q' requests for shared key lock X, then : A. request will be immediately granted B. the deadlock situation is created C. request will be rejected after some time D. request will be granted at it is released by P. Answer:A Question: Consider the situation that the transaction 'P' holds shared key lock X. Also, other transaction 'Q' requests for shared key lock X, then :\nPredicted Answer: A\n==================================================\nInput: Context: The following wounds will heal without a scar formation: A. Abrasion B. Laceration C. Chop wound D. Stab wound Answer:A Question: The following wounds will heal without a scar formation:\nPredicted Answer: A\n==================================================\nTraining complete!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n    DataCollatorForSeq2Seq\n)\nimport torch\nfrom torch.utils.data import Dataset\nimport random\nimport gc\nfrom tqdm.auto import tqdm\nimport re\n\n# Set random seeds for reproducibility\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\ntorch.cuda.empty_cache()\n\n# Define constants\nMODEL_NAME = \"google/flan-t5-small\"\nMAX_INPUT_LENGTH = 384\nMAX_TARGET_LENGTH = 96\nBATCH_SIZE = 4\nGRADIENT_ACCUMULATION_STEPS = 4\nLEARNING_RATE = 3e-4\nNUM_EPOCHS = 3\nSAMPLE_RATIO = 0.1\nMIXED_PRECISION = \"fp16\"\n\n# Function to free memory\ndef free_memory():\n    gc.collect()\n    torch.cuda.empty_cache()\n\n# Extract actual answer content and distractors\ndef extract_answers_and_distractors(row):\n    correct_answer = row['answer'].replace(\"Answer:\", \"\").strip()\n    choices = row['choices']\n    \n    # Identify correct answer letter\n    correct_letter = None\n    \n    # If answer is just a letter (A, B, C, D)\n    if re.match(r'^[A-D]$', correct_answer):\n        correct_letter = correct_answer\n    else:\n        # If answer starts with a letter followed by a period/colon (A., A:)\n        match = re.match(r'^([A-D])[:\\.]?\\s*', correct_answer)\n        if match:\n            correct_letter = match.group(1)\n    \n    # If we still don't have a letter, try to infer from content\n    if not correct_letter:\n        correct_content = correct_answer\n        for choice in choices:\n            parts = re.match(r'^([A-D])[.\\s]+(.*)', choice)\n            if parts:\n                letter, content = parts.groups()\n                # If the content matches (approximately) the correct answer\n                if content.strip().lower() in correct_answer.lower() or correct_answer.lower() in content.strip().lower():\n                    correct_letter = letter\n                    correct_content = content.strip()\n                    break\n    \n    # If we still don't have a letter, take a guess based on position\n    if not correct_letter and \"A\" in correct_answer:\n        correct_letter = \"A\"\n    elif not correct_letter and \"B\" in correct_answer:\n        correct_letter = \"B\"\n    elif not correct_letter and \"C\" in correct_answer:\n        correct_letter = \"C\"\n    elif not correct_letter and \"D\" in correct_answer:\n        correct_letter = \"D\"\n    elif not correct_letter and len(choices) > 0:\n        correct_letter = \"A\"  # Default to first option\n    \n    # Extract correct answer content and distractors\n    correct_content = None\n    distractors = []\n    \n    for choice in choices:\n        parts = re.match(r'^([A-D])[.\\s]+(.*)', choice)\n        if parts:\n            letter, content = parts.groups()\n            content = content.strip()\n            if letter == correct_letter:\n                correct_content = content\n            elif content:  # Only add non-empty distractors\n                distractors.append(content)\n    \n    # If we didn't extract content from choices, use the original answer\n    if not correct_content:\n        if re.match(r'^[A-D][:\\.]?\\s+(.+)$', correct_answer):\n            correct_content = re.match(r'^[A-D][:\\.]?\\s+(.+)$', correct_answer).group(1)\n        else:\n            correct_content = correct_answer\n    \n    return correct_content, distractors\n\n# Load the dataset in streaming mode to reduce memory usage\nprint(\"Loading dataset...\")\ndataset = load_dataset(\"zjsd/RedStone-QA-mcq\", split=\"train\", streaming=True)\n\n# Convert to regular dataset for sampling\ndataset = dataset.take(int(1.66e6 * SAMPLE_RATIO * 1.2))\ndataset = list(dataset)\nrandom.shuffle(dataset)\ndataset = dataset[:int(1.66e6 * SAMPLE_RATIO)]\n\n# Process dataset to extract actual answers and distractors\nprint(\"Extracting answers and distractors...\")\nprocessed_dataset = []\nfor row in tqdm(dataset):\n    actual_answer, distractors = extract_answers_and_distractors(row)\n    if actual_answer and distractors:  # Only keep examples with valid answers and distractors\n        row['actual_answer'] = actual_answer\n        row['distractors_list'] = distractors\n        processed_dataset.append(row)\n\n# Free memory\ndel dataset\nfree_memory()\n\n# Split into train and validation\ntrain_val_split = 0.9\ntrain_size = int(len(processed_dataset) * train_val_split)\n\n# Create train and validation datasets\ntrain_dataset = processed_dataset[:train_size]\nval_dataset = processed_dataset[train_size:train_size + min(2000, len(processed_dataset) - train_size)]\n\nprint(f\"Total processed examples: {len(processed_dataset)}\")\nprint(f\"Training examples: {len(train_dataset)}\")\nprint(f\"Validation examples: {len(val_dataset)}\")\n\n# Free memory\ndel processed_dataset\nfree_memory()\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n\n# Define a processing function for batch tokenization\ndef preprocess_function(examples):\n    batch_size = len(examples)\n    input_texts = []\n    target_texts = []\n    \n    for i in range(batch_size):\n        context = examples[i]['text']\n        question = examples[i]['question']\n        answer = examples[i]['actual_answer']\n        \n        # Format distractors as a single string with separators\n        distractors = \" | \".join(examples[i]['distractors_list'])\n        \n        # Format the input (context + question + correct answer)\n        input_text = f\"Context: {context} Question: {question} Correct Answer: {answer}\"\n        input_texts.append(input_text)\n        target_texts.append(distractors)\n    \n    # Tokenize inputs\n    model_inputs = tokenizer(\n        input_texts,\n        max_length=MAX_INPUT_LENGTH,\n        padding=\"max_length\",\n        truncation=True\n    )\n    \n    # Tokenize targets\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            target_texts,\n            max_length=MAX_TARGET_LENGTH,\n            padding=\"max_length\",\n            truncation=True\n        )\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    \n    # Replace padding token id with -100 so it's ignored in the loss\n    for i in range(len(model_inputs[\"labels\"])):\n        model_inputs[\"labels\"][i] = [\n            -100 if token == tokenizer.pad_token_id else token \n            for token in model_inputs[\"labels\"][i]\n        ]\n    \n    return model_inputs\n\n# Process data in batches to save memory\nprint(\"Processing training data...\")\nbatch_size = 512\ntrain_processed = []\nfor i in tqdm(range(0, len(train_dataset), batch_size)):\n    batch = train_dataset[i:i+batch_size]\n    processed_batch = preprocess_function(batch)\n    for j in range(len(batch)):\n        train_processed.append({\n            \"input_ids\": processed_batch[\"input_ids\"][j],\n            \"attention_mask\": processed_batch[\"attention_mask\"][j],\n            \"labels\": processed_batch[\"labels\"][j]\n        })\n\nprint(\"Processing validation data...\")\nval_processed = []\nfor i in tqdm(range(0, len(val_dataset), batch_size)):\n    batch = val_dataset[i:i+batch_size]\n    processed_batch = preprocess_function(batch)\n    for j in range(len(batch)):\n        val_processed.append({\n            \"input_ids\": processed_batch[\"input_ids\"][j],\n            \"attention_mask\": processed_batch[\"attention_mask\"][j],\n            \"labels\": processed_batch[\"labels\"][j]\n        })\n\n# Free memory\ndel train_dataset, val_dataset\nfree_memory()\n\n# Create PyTorch datasets\nclass MemoryEfficientDataset(Dataset):\n    def __init__(self, examples):\n        self.examples = examples\n    \n    def __len__(self):\n        return len(self.examples)\n    \n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(self.examples[idx][\"input_ids\"]),\n            \"attention_mask\": torch.tensor(self.examples[idx][\"attention_mask\"]),\n            \"labels\": torch.tensor(self.examples[idx][\"labels\"])\n        }\n\ntrain_dataset = MemoryEfficientDataset(train_processed)\nval_dataset = MemoryEfficientDataset(val_processed)\n\n# Free memory\ndel train_processed, val_processed\nfree_memory()\n\n# Load the model\nprint(\"Loading model...\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n\n# Define training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./distractor_generation_model\",\n    overwrite_output_dir=True,\n    num_train_epochs=NUM_EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=1,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    logging_dir=\"./logs\",\n    logging_steps=50,\n    learning_rate=LEARNING_RATE,\n    weight_decay=0.01,\n    warmup_ratio=0.05,\n    predict_with_generate=False,\n    fp16=MIXED_PRECISION == \"fp16\",\n    optim=\"adamw_torch\",\n    report_to=\"none\",\n    disable_tqdm=False,\n)\n\n# Set up the trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True),\n)\n\n# Train the model\nprint(\"Training model...\")\ntrainer.train()\n\n# Save the model\nprint(\"Saving model...\")\nmodel.save_pretrained(\"./distractor_generation_model_final\")\ntokenizer.save_pretrained(\"./distractor_generation_model_final\")\n\n# Test on a few examples\nprint(\"Testing on some examples...\")\nmodel.eval()\ntest_examples = [\n    val_dataset[i] for i in range(min(3, len(val_dataset)))\n]\n\nfor example in test_examples:\n    input_ids = example[\"input_ids\"].unsqueeze(0).to(model.device)\n    attention_mask = example[\"attention_mask\"].unsqueeze(0).to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            max_length=MAX_TARGET_LENGTH,\n            num_beams=2,\n            early_stopping=True\n        )\n    \n    input_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n    predicted_distractors = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    print(f\"Input: {input_text}\")\n    print(f\"Predicted Distractors: {predicted_distractors}\")\n    print(\"=\" * 50)\n\nprint(\"Training complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T12:38:37.689231Z","iopub.execute_input":"2025-03-03T12:38:37.689584Z","iopub.status.idle":"2025-03-03T16:18:33.099170Z","shell.execute_reply.started":"2025-03-03T12:38:37.689552Z","shell.execute_reply":"2025-03-03T16:18:33.098435Z"}},"outputs":[{"name":"stdout","text":"Loading dataset...\nExtracting answers and distractors...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/166000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0765f37a8f444e3483cd7732376421e3"}},"metadata":{}},{"name":"stdout","text":"Total processed examples: 165420\nTraining examples: 148878\nValidation examples: 2000\nProcessing training data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/291 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8589cf0466a4c578497b58900218451"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Processing validation data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ea994678ebb4bb986df8fbd93191add"}},"metadata":{}},{"name":"stdout","text":"Loading model...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-3-709bbdbd72bc>:277: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"},{"name":"stdout","text":"Training model...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='13956' max='13956' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [13956/13956 3:37:14, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.008100</td>\n      <td>0.003985</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.006100</td>\n      <td>0.002732</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nThere were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n","output_type":"stream"},{"name":"stdout","text":"Saving model...\nTesting on some examples...\nInput: Context: Why might individuals who report eating chocolate more often be more likely to be thin (not overweight)? A. Because overweight individuals are more likely to be on a diet, and therefore eat chocolate less frequently B. Because individuals who eat chocolate more often also exhibit more self control and actually eat less total chocolate C. Because overweight individuals are less likely to honestly report eating foods that are perceived as â€œunhealthyâ€ D. Because chocolate contains certain nutrients that make a person fuller and therefore less likely to eat as many calories of other foods Answer:A Question: Why might individuals who report eating chocolate more often be more likely to be thin (not overweight)? Correct Answer: Because overweight individuals are more likely to be on a diet, and therefore eat chocolate less frequently\nPredicted Distractors: Because individuals who eat chocolate more often also exhibit more self control and actually eat less total chocolate | Because overweight individuals are less likely to honestly report eating foods that are perceived as â€œunhealthyâ€ | Because chocolate contains certain nutrients that make a person fuller and therefore less likely to eat as many calories of other foods\n==================================================\nInput: Context: Explanation:Flash memory is of two types â€“ NAND and NOR. 6. Which is the cheapest memory device in terms of costs/bit ? A. Semiconductor memory B. Magnetic disks C. Compact disks D. Magnetic tapes Answer:A Question: Explanation:Flash memory is of two types â€“ NAND and NOR. 6. Which is the cheapest memory device in terms of costs/bit ? Correct Answer: Semiconductor memory\nPredicted Distractors: Magnetic disks | Compact disks | Magnetic tapes\n==================================================\nInput: Context: Which port is used for CLI Secure shell access? A. Port 23 B. Port 25 C. Port 22 D. Port 443 Answer:C Question: Which port is used for CLI Secure shell access? Correct Answer: Port 22\nPredicted Distractors: Port 23 | Port 25 | Port 443\n==================================================\nTraining complete!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Add these imports at the top of your script\nfrom huggingface_hub import login, HfApi\n\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nHF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n\nprint(\"Logging in to Hugging Face Hub...\")\nlogin(token=HF_TOKEN)\n\n# Define your Hugging Face repository name\n# Format: \"username/repository-name\"\nHF_REPO_ID = \"aayeshanakarmi/distractor-generation-redstone-flant5small-2\"  # Replace with your desired repo name\n\n# Save the model to the Hub\nprint(f\"Uploading model to Hugging Face Hub as {HF_REPO_ID}...\")\nmodel.push_to_hub(HF_REPO_ID, use_auth_token=HF_TOKEN)\ntokenizer.push_to_hub(HF_REPO_ID, use_auth_token=HF_TOKEN)\n\n\nprint(f\"Model successfully uploaded to Hugging Face Hub: https://huggingface.co/{HF_REPO_ID}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T16:36:09.624146Z","iopub.execute_input":"2025-03-03T16:36:09.624502Z","iopub.status.idle":"2025-03-03T16:36:22.854002Z","shell.execute_reply.started":"2025-03-03T16:36:09.624472Z","shell.execute_reply":"2025-03-03T16:36:22.853190Z"}},"outputs":[{"name":"stdout","text":"Logging in to Hugging Face Hub...\nUploading model to Hugging Face Hub as aayeshanakarmi/distractor-generation-redstone-flant5small-2...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:894: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f158d8b23eda48b3b47b2e34af1de20f"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:894: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f63031b6d6f942fe851222b34a398db2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03c20b09a95e4f57b46aff0a8ae677e4"}},"metadata":{}},{"name":"stdout","text":"Model successfully uploaded to Hugging Face Hub: https://huggingface.co/aayeshanakarmi/distractor-generation-redstone-flant5small-2\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Answer Generation Model ","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n    DataCollatorForSeq2Seq\n)\nimport torch\nfrom torch.utils.data import Dataset\nimport random\nimport gc\nfrom tqdm.auto import tqdm\nimport re\n\n# Set random seeds for reproducibility\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\ntorch.cuda.empty_cache()\n\n# Define constants\nMODEL_NAME = \"google/flan-t5-small\"\nMAX_INPUT_LENGTH = 384\nMAX_TARGET_LENGTH = 48\nBATCH_SIZE = 4\nGRADIENT_ACCUMULATION_STEPS = 4\nLEARNING_RATE = 3e-4\nNUM_EPOCHS = 3\nSAMPLE_RATIO = 0.1\nMIXED_PRECISION = \"fp16\"\n\n# Function to free memory\ndef free_memory():\n    gc.collect()\n    torch.cuda.empty_cache()\n\n# Enhanced function to extract actual answer content (not just A, B, C, D)\ndef extract_actual_answer(row):\n    correct_answer = row['answer'].replace(\"Answer:\", \"\").strip()\n    choices = row['choices']\n    \n    # If answer is just a single letter (A, B, C, D)\n    if re.match(r'^[A-D]$', correct_answer):\n        letter = correct_answer\n        for choice in choices:\n            # Look for the choice that starts with this letter\n            if choice.startswith(letter + \".\") or choice.startswith(letter + \" \") or choice.startswith(letter + \":\"):\n                # Extract everything after the letter and separator\n                content = re.sub(r'^[A-D][.\\s:]+', '', choice).strip()\n                return content\n    \n    # If answer starts with letter followed by period/colon/space (e.g., \"A. text\", \"A: text\", \"A text\")\n    match = re.match(r'^([A-D])[.\\s:]+(.+)$', correct_answer)\n    if match:\n        # Extract the content part after the letter\n        content = match.group(2).strip()\n        if content:\n            return content\n        \n        # If no content after the letter in the answer, find it in choices\n        letter = match.group(1)\n        for choice in choices:\n            if choice.startswith(letter + \".\") or choice.startswith(letter + \" \") or choice.startswith(letter + \":\"):\n                content = re.sub(r'^[A-D][.\\s:]+', '', choice).strip()\n                return content\n    \n    # Handle case where the answer might be the full text that matches one of the choices\n    for choice in choices:\n        # Extract the content part of the choice (removing any leading A., B., etc.)\n        choice_content = re.sub(r'^[A-D][.\\s:]+', '', choice).strip()\n        # If the answer matches this content exactly, return it\n        if correct_answer == choice_content:\n            return correct_answer\n    \n    # If we couldn't match it to a specific choice or extract a letter,\n    # just return the original answer as a fallback\n    return correct_answer\n\n# Load the dataset in streaming mode to reduce memory usage\nprint(\"Loading dataset...\")\ndataset = load_dataset(\"zjsd/RedStone-QA-mcq\", split=\"train\", streaming=True)\n\n# Convert to regular dataset for sampling\ndataset = dataset.take(int(1.66e6 * SAMPLE_RATIO * 1.2))\ndataset = list(dataset)\nrandom.shuffle(dataset)\ndataset = dataset[:int(1.66e6 * SAMPLE_RATIO)]\n\n# Process dataset to extract actual answers\nprint(\"Extracting actual answers...\")\nprocessed_dataset = []\nfor row in tqdm(dataset):\n    actual_answer = extract_actual_answer(row)\n    if actual_answer.strip():  # Only keep examples with non-empty answers\n        row['actual_answer'] = actual_answer\n        processed_dataset.append(row)\n\n# Free memory\ndel dataset\nfree_memory()\n\n# Split into train and validation\ntrain_val_split = 0.9\ntrain_size = int(len(processed_dataset) * train_val_split)\n\n# Create train and validation datasets\ntrain_dataset = processed_dataset[:train_size]\nval_dataset = processed_dataset[train_size:train_size + min(2000, len(processed_dataset) - train_size)]\n\nprint(f\"Total examples with actual answers: {len(processed_dataset)}\")\nprint(f\"Training examples: {len(train_dataset)}\")\nprint(f\"Validation examples: {len(val_dataset)}\")\n\n# Free memory\ndel processed_dataset\nfree_memory()\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n\n# Define a processing function for batch tokenization\ndef preprocess_function(examples):\n    batch_size = len(examples)\n    input_texts = []\n    target_texts = []\n    \n    for i in range(batch_size):\n        context = examples[i]['text']\n        question = examples[i]['question']\n        answer = examples[i]['actual_answer']\n        \n        # Format the input (context + question)\n        input_text = f\"Context: {context} Question: {question}\"\n        input_texts.append(input_text)\n        target_texts.append(answer)\n    \n    # Tokenize inputs\n    model_inputs = tokenizer(\n        input_texts,\n        max_length=MAX_INPUT_LENGTH,\n        padding=\"max_length\",\n        truncation=True\n    )\n    \n    # Tokenize targets\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            target_texts,\n            max_length=MAX_TARGET_LENGTH,\n            padding=\"max_length\",\n            truncation=True\n        )\n    \n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    \n    # Replace padding token id with -100 so it's ignored in the loss\n    for i in range(len(model_inputs[\"labels\"])):\n        model_inputs[\"labels\"][i] = [\n            -100 if token == tokenizer.pad_token_id else token \n            for token in model_inputs[\"labels\"][i]\n        ]\n    \n    return model_inputs\n\n# Process data in batches to save memory\nprint(\"Processing training data...\")\nbatch_size = 512\ntrain_processed = []\nfor i in tqdm(range(0, len(train_dataset), batch_size)):\n    batch = train_dataset[i:i+batch_size]\n    processed_batch = preprocess_function(batch)\n    for j in range(len(batch)):\n        train_processed.append({\n            \"input_ids\": processed_batch[\"input_ids\"][j],\n            \"attention_mask\": processed_batch[\"attention_mask\"][j],\n            \"labels\": processed_batch[\"labels\"][j]\n        })\n\nprint(\"Processing validation data...\")\nval_processed = []\nfor i in tqdm(range(0, len(val_dataset), batch_size)):\n    batch = val_dataset[i:i+batch_size]\n    processed_batch = preprocess_function(batch)\n    for j in range(len(batch)):\n        val_processed.append({\n            \"input_ids\": processed_batch[\"input_ids\"][j],\n            \"attention_mask\": processed_batch[\"attention_mask\"][j],\n            \"labels\": processed_batch[\"labels\"][j]\n        })\n\n# Free memory\ndel train_dataset, val_dataset\nfree_memory()\n\n# Create PyTorch datasets\nclass MemoryEfficientDataset(Dataset):\n    def __init__(self, examples):\n        self.examples = examples\n    \n    def __len__(self):\n        return len(self.examples)\n    \n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": torch.tensor(self.examples[idx][\"input_ids\"]),\n            \"attention_mask\": torch.tensor(self.examples[idx][\"attention_mask\"]),\n            \"labels\": torch.tensor(self.examples[idx][\"labels\"])\n        }\n\ntrain_dataset = MemoryEfficientDataset(train_processed)\nval_dataset = MemoryEfficientDataset(val_processed)\n\n# Free memory\ndel train_processed, val_processed\nfree_memory()\n\n# Load the model\nprint(\"Loading model...\")\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n\n# Define training arguments\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./answer_generation_model\",\n    overwrite_output_dir=True,\n    num_train_epochs=NUM_EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=1,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    logging_dir=\"./logs\",\n    logging_steps=50,\n    learning_rate=LEARNING_RATE,\n    weight_decay=0.01,\n    warmup_ratio=0.05,\n    predict_with_generate=False,\n    fp16=MIXED_PRECISION == \"fp16\",\n    optim=\"adamw_torch\",\n    report_to=\"none\",\n    disable_tqdm=False,\n)\n\n# Set up the trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True),\n)\n\n# Train the model\nprint(\"Training model...\")\ntrainer.train()\n\n# Save the model\nprint(\"Saving model...\")\nmodel.save_pretrained(\"./answer_generation_model_final\")\ntokenizer.save_pretrained(\"./answer_generation_model_final\")\n\n# Test on a few examples\nprint(\"Testing on some examples...\")\nmodel.eval()\ntest_examples = [\n    val_dataset[i] for i in range(min(3, len(val_dataset)))\n]\n\nfor example in test_examples:\n    input_ids = example[\"input_ids\"].unsqueeze(0).to(model.device)\n    attention_mask = example[\"attention_mask\"].unsqueeze(0).to(model.device)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            max_length=MAX_TARGET_LENGTH,\n            num_beams=2,\n            early_stopping=True\n        )\n    \n    input_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n    predicted_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    \n    print(f\"Input: {input_text}\")\n    print(f\"Predicted Answer: {predicted_answer}\")\n    print(\"=\" * 50)\n\nprint(\"Training complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-03T17:02:21.275523Z","iopub.execute_input":"2025-03-03T17:02:21.275872Z","execution_failed":"2025-03-03T21:20:11.562Z"}},"outputs":[{"name":"stdout","text":"Loading dataset...\nExtracting actual answers...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/166000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd082447388e4815b57568a1ef5fef66"}},"metadata":{}},{"name":"stdout","text":"Total examples with actual answers: 165884\nTraining examples: 149295\nValidation examples: 2000\nProcessing training data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/292 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93849c7aa67c4fbdb3e515ad861b606b"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3953: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Processing validation data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ad123f785fd4ea98f1acf7ebb6c47dc"}},"metadata":{}},{"name":"stdout","text":"Loading model...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-6-afd1c075a049>:250: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"},{"name":"stdout","text":"Training model...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='11576' max='13995' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [11576/13995 2:46:03 < 34:42, 1.16 it/s, Epoch 2.48/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.014300</td>\n      <td>0.011770</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.009300</td>\n      <td>0.013052</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}